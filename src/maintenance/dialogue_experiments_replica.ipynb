{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the label classification method presented in \"CLASSIFICATION MODELS FOR RST DISCOURSE PARSING OF TEXTS IN RUSSIAN\"\n",
    "http://www.dialog-21.ru/media/4595/chistovaevplusetal-076.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get code for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r rurst2019\n",
    "mkdir rurst2019\n",
    "cd rurst2019\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/meaningfulwords_v3.py\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/language_features.py\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/features_processor.py\n",
    "\n",
    "# some external modules are structurally the same but have other paths\n",
    "sed -i \"s|utils/tf_idf_pipeline.save|models/tf_idf/pipeline.pkl|g\" features_processor.py  # tf-idf pipeline\n",
    "sed -i \"s|models_w2v/model2_tokenized|models/w2v/segmentator/model2_tokenized|g\" features_processor.py  # w2v model\n",
    "\n",
    "# also some fixes of the feature extractor\n",
    "sed -i \"s|'common_root_fpos',|\\n|g\" features_processor.py\n",
    "sed -i \"s|'common_root_att',|\\n|g\" features_processor.py\n",
    "sed -i \"s|'common_root'|\\n|g\" features_processor.py\n",
    "sed -i \"s|/ len(row))|/ (len(row) + 1e-8))|g\" features_processor.py\n",
    "sed -i \"s|'tokens_x', 'tokens_y',|\\n|g\" features_processor.py\n",
    "#sed -i \"s|return [self.annotations['tokens'][i].text for i in range(begin, end)]|result = [self.annotations['tokens'][i].text for i in range(begin, end)]\\n        if result:\\n            return result\\n        return ['_']|g\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract features \n",
    "Same way as in ``1_data_extraction.ipynb`` but with another interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rurst2019.features_processor import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_gold, read_annotation\n",
    "\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    # print(file)\n",
    "    table = read_gold(file.replace('.json', ''))#pd.read_json(file)\n",
    "    table = table[table.snippet_x.map(len) > 0]\n",
    "    table = table[table.snippet_y.map(len) > 0]\n",
    "    annot = read_annotation(file.replace('.json', ''))#pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    features = features_processor(table, \n",
    "                                  annot)\n",
    "    features.to_pickle(file.replace('.json', '.gold.pkl.oldf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import rcParams\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from catboost import CatBoostClassifier\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['font.sans-serif'] = 'Arial'\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "random_state = 41\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in train:\n",
    "    train_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "\n",
    "for file in dev:\n",
    "    dev_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "    \n",
    "for file in test:\n",
    "    test_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "dev_samples = pd.concat(dev_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'category_id'\n",
    "MAX_LEN = 100\n",
    "\n",
    "train_samples[TARGET] = train_samples[TARGET].replace(['antithesis_r',], 'contrast_m')\n",
    "train_samples[TARGET] = train_samples[TARGET].replace(['cause_r', 'effect_r'], 'cause-effect_r')\n",
    "train_samples[TARGET] = train_samples[TARGET].replace(['conclusion_r',], 'restatement_m')\n",
    "train_samples[TARGET] = train_samples[TARGET].replace(['evaluation_r'], 'interpretation-evaluation_r')\n",
    "train_samples[TARGET] = train_samples[TARGET].replace(['motivation_r',], 'condition_r')\n",
    "train_samples['relation'] = train_samples[TARGET].map(lambda row: row[:-1]) + train_samples['order']\n",
    "train_samples['relation'] = train_samples['relation'].replace(['restatement_SN', 'restatement_NS'], 'restatement_NN')\n",
    "train_samples['relation'] = train_samples['relation'].replace(['contrast_SN', 'contrast_NS'], 'contrast_NN')\n",
    "train_samples['relation'] = train_samples['relation'].replace(['solutionhood_NS', 'preparation_NS'], 'elaboration_NS')\n",
    "train_samples['relation'] = train_samples['relation'].replace(['concession_SN', 'evaluation_SN', \n",
    "                                                               'elaboration_SN', 'evidence_SN'], 'preparation_SN')\n",
    "train_samples = train_samples[train_samples.tokens_x.map(len) < MAX_LEN]\n",
    "train_samples = train_samples[train_samples.tokens_y.map(len) < MAX_LEN]\n",
    "\n",
    "dev_samples[TARGET] = dev_samples[TARGET].replace(['antithesis_r',], 'contrast_m')\n",
    "dev_samples[TARGET] = dev_samples[TARGET].replace(['cause_r', 'effect_r'], 'cause-effect_r')\n",
    "dev_samples[TARGET] = dev_samples[TARGET].replace(['conclusion_r',], 'restatement_m')\n",
    "dev_samples[TARGET] = dev_samples[TARGET].replace(['evaluation_r'], 'interpretation-evaluation_r')\n",
    "dev_samples[TARGET] = dev_samples[TARGET].replace(['motivation_r',], 'condition_r')\n",
    "dev_samples['relation'] = dev_samples[TARGET].map(lambda row: row[:-1]) + dev_samples['order']\n",
    "dev_samples['relation'] = dev_samples['relation'].replace(['restatement_SN', 'restatement_NS'], 'restatement_NN')\n",
    "dev_samples['relation'] = dev_samples['relation'].replace(['contrast_SN', 'contrast_NS'], 'contrast_NN')\n",
    "dev_samples['relation'] = dev_samples['relation'].replace(['solutionhood_NS', 'preparation_NS'], 'elaboration_NS')\n",
    "dev_samples['relation'] = dev_samples['relation'].replace(['concession_SN', 'evaluation_SN', \n",
    "                                                           'elaboration_SN', 'evidence_SN'], 'preparation_SN')\n",
    "dev_samples = dev_samples[dev_samples.tokens_x.map(len) < MAX_LEN]\n",
    "dev_samples = dev_samples[dev_samples.tokens_y.map(len) < MAX_LEN]\n",
    "\n",
    "test_samples[TARGET] = test_samples[TARGET].replace(['antithesis_r',], 'contrast_m')\n",
    "test_samples[TARGET] = test_samples[TARGET].replace(['cause_r', 'effect_r'], 'cause-effect_r')\n",
    "test_samples[TARGET] = test_samples[TARGET].replace(['conclusion_r',], 'restatement_m')\n",
    "test_samples[TARGET] = test_samples[TARGET].replace(['evaluation_r'], 'interpretation-evaluation_r')\n",
    "test_samples[TARGET] = test_samples[TARGET].replace(['motivation_r',], 'condition_r')\n",
    "test_samples['relation'] = test_samples[TARGET].map(lambda row: row[:-1]) + test_samples['order']\n",
    "test_samples['relation'] = test_samples['relation'].replace(['restatement_SN', 'restatement_NS'], 'restatement_NN')\n",
    "test_samples['relation'] = test_samples['relation'].replace(['contrast_SN', 'contrast_NS'], 'contrast_NN')\n",
    "test_samples['relation'] = test_samples['relation'].replace(['solutionhood_NS', 'preparation_NS'], 'elaboration_NS')\n",
    "test_samples['relation'] = test_samples['relation'].replace(['concession_SN', 'evaluation_SN', \n",
    "                                                             'elaboration_SN', 'evidence_SN'], 'preparation_SN')\n",
    "test_samples = test_samples[test_samples.tokens_x.map(len) < MAX_LEN]\n",
    "test_samples = test_samples[test_samples.tokens_y.map(len) < MAX_LEN]\n",
    "\n",
    "TARGET = 'relation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = np.array([3634., 3194., 1235., 819., 742., 725., 690., 593., 546., \n",
    "                   540., 507., 487., 393., 388., 280., 271., 258., 174.,\n",
    "                   159., 130., 107., 105., 91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.head(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['snippet_x', 'snippet_y', 'order', 'filename', 'tokens_x', 'tokens_y']\n",
    "y_train, X_train = train_samples[TARGET].to_frame(), train_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples[TARGET].to_frame(), dev_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_train)\n",
    "X_train = pd.DataFrame(X_scaled_np, index=X_train.index)#, columns=X.columns)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)#, columns=X.columns)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)#, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "y_train = lab_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "logreg = LogisticRegression(random_state=random_state,\n",
    "                            solver='lbfgs',\n",
    "                            n_jobs=8,\n",
    "                            C=0.002,\n",
    "                            multi_class='multinomial',\n",
    "                            class_weight='balanced')\n",
    "\n",
    "eval_dataset = Pool(data=X_dev,\n",
    "                    label=y_dev)\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.1,\n",
    "    custom_loss=['F1'],\n",
    "    random_seed=random_state,\n",
    "    verbose=0,\n",
    "    loss_function='MultiClass',\n",
    "    #task_type='GPU',\n",
    "    class_weights=counts / counts[-1]\n",
    ")\n",
    "\n",
    "fs_catboost = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LogisticRegression(solver='saga', penalty='l1', C=1., n_jobs=-1))),\n",
    "  ('classification', catboost)\n",
    "])\n",
    "\n",
    "logreg = LogisticRegression(random_state=random_state,\n",
    "                            solver='lbfgs',\n",
    "                            n_jobs=-1,\n",
    "                            C=0.002,\n",
    "                            multi_class='multinomial',\n",
    "                            class_weight='balanced')\n",
    "\n",
    "fs_catboost_plus_logreg = VotingClassifier([('fs_catboost', fs_catboost), ('logreg', logreg)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_catboost_plus_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = lab_encoder.inverse_transform(fs_catboost_plus_logreg.predict(X_dev))\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_dev.values, predicted, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_dev.values, predicted, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_dev.values, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, predicted, digits=4))\n",
    "print('macro precision: %.2f'%(metrics.precision_score(y_dev, predicted, average='macro')*100.))\n",
    "print('macro recall: %.2f'%(metrics.recall_score(y_dev, predicted, average='macro')*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = lab_encoder.inverse_transform(fs_catboost_plus_logreg.predict(X_test))\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_test.values, predicted, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test.values, predicted, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_test.values, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted, digits=4))\n",
    "print('macro precision: %.2f'%(metrics.precision_score(y_test, predicted, average='macro')*100.))\n",
    "print('macro recall: %.2f'%(metrics.recall_score(y_test, predicted, average='macro')*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fs_catboost_plus_logreg, open('models/dialog_model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
