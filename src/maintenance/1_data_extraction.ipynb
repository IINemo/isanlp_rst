{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS3 parsing \n",
    "output:\n",
    " - file.edus  # text file with edus from .rs3 - each line contains one edu\n",
    " - file.json  # json file with du-pairs from gold trees. keys: ['snippet_x', 'snippet_y', 'category_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/blogs/blogs_rs3/* > rst_blogs_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/news1/news1_rs3/* > rst_news1_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/news2/news2_rs3/* > rst_news2_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/sci_comp/sci_comp_rs3/* > rst_scicomp_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/sci_ling/sci_ling_rs3/* > rst_sciling_parsing.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat rst_sciling_parsing.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! wc -l data/*.edus | grep 'total'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - file.annot.pkl  # morphology, syntax, semantics to use with isanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install git+https://github.com/IINemo/isanlp.git@discourse\n",
    "pip install git+https://github.com/tchewik/isanlp_srl_framebank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = ''\n",
    "host3 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "ppl = PipelineCommon([(ProcessorRemote(host, 4333, 'default'),\n",
    "                       ['text'],\n",
    "                       {'sentences' : 'sentences', \n",
    "                        'tokens' : 'tokens',\n",
    "                        'postag' : 'postag',\n",
    "                        'lemma' : 'lemma'}),\n",
    "                      (ConverterMystemToUd(), \n",
    "                        ['postag'],\n",
    "                        {'morph' : 'morph',\n",
    "                         'postag': 'postag'}),\n",
    "                      (ProcessorRemote(host, 5336, '0'), \n",
    "                        ['tokens', 'sentences'], \n",
    "                        {'syntax_dep_tree' : 'syntax_dep_tree',\n",
    "                         'postag' : 'ud_postag'}),\n",
    "#                       (ProcessorRemote(host3, 4336, 'default'),\n",
    "#                         ['tokens', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "#                         {'srl' : 'srl'})\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_gold, read_annotation, prepare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full/sci_comp/sci_comp_txt/',\n",
    "                'corpus/RuRsTreebank_full/sci_ling/sci_ling_txt/',\n",
    "                'corpus/RuRsTreebank_full/blogs/blogs_txt/blogs_46',\n",
    "                'corpus/RuRsTreebank_full/news1/news1_txt/',\n",
    "                'corpus/RuRsTreebank_full/news2/news2_txt/'\n",
    "]\n",
    "\n",
    "for path in directories:\n",
    "    print('analyze path:', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        text = prepare_text(open(file, 'r').read())\n",
    "        annot = ppl(text)\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        pickle.dump(annot, open(os.path.join('data', filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - models/tf_idf/pipeline.pkl  # is used in default feature extraction\n",
    " - file.gold.pkl  # dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from utils.file_reading import read_annotation\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "! mkdir models\n",
    "! mkdir models/tf_idf\n",
    "\n",
    "corpus = []\n",
    "for file in glob.glob(\"%s*.json\" % IN_PATH):\n",
    "    tokens = read_annotation(file.replace('.json', ''))['tokens']\n",
    "    corpus.append(list(map(lambda token: token.text.lower(), tokens)))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), tokenizer=dummy, preprocessor=dummy)\n",
    "\n",
    "svd = TruncatedSVD(n_components=25,\n",
    "                   tol=0.0,\n",
    "                   n_iter=7,\n",
    "                   random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('svd', svd)\n",
    "])\n",
    "\n",
    "pipeline.fit(corpus)\n",
    "pickle.dump(pipeline, open('models/tf_idf/pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -c \"import nltk; nltk.download('stopwords')\"\n",
    "pip install dostoevsky\n",
    "dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "    \n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_gold, read_annotation\n",
    "\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    table = read_gold(file.replace('.json', ''))#pd.read_json(file)\n",
    "    table = table[table.snippet_x.map(len) > 0]\n",
    "    annot = read_annotation(file.replace('.json', ''))#pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    features = features_processor(table, \n",
    "                                  annot['text'], annot['tokens'], \n",
    "                                  annot['sentences'], annot['lemma'], \n",
    "                                  annot['morph'], annot['postag'], \n",
    "                                  annot['syntax_dep_tree'])\n",
    "    features.to_pickle(file.replace('.json', '.gold.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDPipe assigns different postags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "avail_pairs = []\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    for sent in annot['ud_postag']:\n",
    "        avail_pairs.append('_'.join(sent[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(avail_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "avail_pairs = []\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    for sent in annot['srl']:\n",
    "        for pred in sent:\n",
    "            for event in pred:\n",
    "                print(event)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
