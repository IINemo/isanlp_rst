{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS3 parsing \n",
    "output:\n",
    " - file.edus  # text file with edus from .rs3 - each line contains one edu\n",
    " - file.json  # json file with du-pairs from gold trees. keys: ['snippet_x', 'snippet_y', 'category_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python utils/parse_rs3.py corpus/news_texts/news_rs3/* > rst_news_parsing.log\n",
    "! python utils/parse_rs3.py corpus/science_texts/compscience/compscience_rs3/* > rst_comp_parsing.log\n",
    "! python utils/parse_rs3.py corpus/science_texts/linguistics/linguistics_rs3/* > rst_ling_parsing.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/blogs/blogs_rs3/* > rst_blogs_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/news1/news1_rs3/* > rst_news1_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/news2/news2_rs3/* > rst_news2_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/sci_comp/sci_comp_rs3/* > rst_scicomp_parsing.log\n",
    "! python utils/parse_rs3.py corpus/RuRsTreebank_full/sci_ling/sci_ling_rs3/* > rst_sciling_parsing.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! wc -l data/*.edus | grep 'total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_pickle(filename + '.gold.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_json(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.annot.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - file.annot.pkl  # morphology, syntax, semantics to use with isanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install git+https://github.com/IINemo/isanlp.git@dev\n",
    "pip install git+https://github.com/tchewik/isanlp_srl_framebank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = ''\n",
    "host3 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "ppl = PipelineCommon([(ProcessorRemote(host, 4333, 'default'),\n",
    "                       ['text'],\n",
    "                       {'sentences' : 'sentences', \n",
    "                        'tokens' : 'tokens',\n",
    "                        'postag' : 'mystem_postags',\n",
    "                        'lemma' : 'lemma'}),\n",
    "                      (ConverterMystemToUd(), \n",
    "                        ['mystem_postags'],\n",
    "                        {'morph' : 'morph',\n",
    "                         'postag': 'postag'}),\n",
    "                      (ProcessorRemote(host, 5336, '0'), \n",
    "                        ['tokens', 'sentences'], \n",
    "                        {'syntax_dep_tree' : 'syntax_dep_tree',\n",
    "                         'postag' : 'ud_postag'}),\n",
    "                      (ProcessorRemote(host3, 4336, 'default'),\n",
    "                        ['tokens', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "                        {'srl' : 'srl'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "directories = ['corpus/news_texts/news_txt/',\n",
    "               'corpus/science_texts/coвmpscience/compscience_txt/',\n",
    "               'corpus/science_texts/linguistics/linguistics_txt/']\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full/sci_comp/sci_comp_txt/',\n",
    "                'corpus/RuRsTreebank_full/sci_ling/sci_ling_txt/',\n",
    "                'corpus/RuRsTreebank_full/blogs/blogs_txt/',\n",
    "                'corpus/RuRsTreebank_full/news1/news1_txt/',\n",
    "                'corpus/RuRsTreebank_full/news2/news2_txt/']\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = text.replace('  \\n', '#####')\n",
    "    text = text.replace(' \\n', '#####')\n",
    "    text = text + '#####'\n",
    "    text = text.replace('#####', '\\n')\n",
    "    text_html_map = {\n",
    "        '\\n': r' ',\n",
    "        '&gt;': r'>',\n",
    "        '&lt;': r'<',\n",
    "        '&amp;': r'&',\n",
    "        '&quot;': r'\"',\n",
    "        '&ndash;': r'–',\n",
    "        '##### ': r'',\n",
    "        '\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "        '   ': r' ',\n",
    "        '  ': r' ',\n",
    "        '——': r'-',\n",
    "        '—': r'-',\n",
    "        '/': r'',\n",
    "        '\\^': r'',\n",
    "        '^': r'',\n",
    "        '±': r'+',\n",
    "        'y': r'у',\n",
    "        'xc': r'хс',\n",
    "        'x': r'х'\n",
    "    }\n",
    "    for key in text_html_map.keys():\n",
    "        text = text.replace(key, text_html_map[key])\n",
    "    return text    \n",
    "\n",
    "for path in directories:\n",
    "    print('>>', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        text = prepare_text(open(file, 'r').read())\n",
    "        annot = ppl(text)\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        pickle.dump(annot, open(os.path.join('data', filename), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - models/tf_idf/pipeline.pkl  # is used in default feature extraction\n",
    " - file.gold.pkl  # dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "! mkdir models\n",
    "! mkdir models/tf_idf\n",
    "\n",
    "corpus = []\n",
    "for file in glob.glob(\"%s*.json\" % IN_PATH):\n",
    "    table = pd.read_json(file)\n",
    "    #annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    #for sentence in annot['sentences']:\n",
    "    #    corpus.append(' '.join([token.text for token in annot['tokens'][sentence.begin:sentence.end]]))\n",
    "    for snippet in table.snippet_x.values:\n",
    "        corpus.append(' '.join(nltk.tokenize.casual_tokenize(snippet)))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "#tf_idf_vectorizer = TfidfVectorizer(sublinear_tf=False, norm='l2', analyzer='word',\n",
    "#                                    ngram_range=(2, 3), use_idf=1, smooth_idf=1)\n",
    "\n",
    "svd = TruncatedSVD(n_components=25,\n",
    "                   tol=0.0,\n",
    "                   n_iter=7,\n",
    "                   random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('svd', svd)\n",
    "])\n",
    "\n",
    "pipeline.fit(corpus)\n",
    "pickle.dump(pipeline, open('models/tf_idf/pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -c \"import nltk; nltk.download('stopwords')\"\n",
    "pip install dostoevsky\n",
    "dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    table = pd.read_json(file)\n",
    "    table = table[table.snippet_x.map(len) > 0]\n",
    "    annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    features = features_processor(table, \n",
    "                                  annot['text'], annot['tokens'], \n",
    "                                  annot['sentences'], annot['lemma'], \n",
    "                                  annot['morph'], annot['postag'], \n",
    "                                  annot['syntax_dep_tree'])\n",
    "    features.to_pickle(file.replace('.json', '.gold.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sm_x_positive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(annot['srl'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot['ud_postag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDPipe assigns different postags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "avail_pairs = []\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    for sent in annot['ud_postag']:\n",
    "        avail_pairs.append('_'.join(sent[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(avail_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "avail_pairs = []\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    annot = pickle.load(open(file.replace('.json', '.annot.pkl'), 'rb'))\n",
    "    for sent in annot['srl']:\n",
    "        for pred in sent:\n",
    "            for event in pred:\n",
    "                print(event)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
