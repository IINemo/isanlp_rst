{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building\n",
    "\n",
    "1. Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "2. Train models, save the best one.\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``models/structure_predictor/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_negative(filename):\n",
    "    return pd.read_json(filename + '.json.neg')\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.annot.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_merge_gr(scores):\n",
    "    return scores.index(True)\n",
    "\n",
    "def create_training_set_greedy(edus, gold):    \n",
    "    def in_gold(pair):\n",
    "        tmp = gold[(gold['snippet_x'] == pair[0].strip()) & (gold['snippet_y'] == pair[1].strip())]\n",
    "        return len(tmp) > 0\n",
    "\n",
    "    def make_samples(nodes, scores):\n",
    "        res = []\n",
    "        for node in nodes:\n",
    "            res.append((node[0], node[1], in_gold(node)))\n",
    "        return res\n",
    "\n",
    "    pairs = [(edus[i], edus[i+1]) for i in range(len(edus) - 1)]\n",
    "    nodes = edus\n",
    "    scores = [in_gold(pair) for pair in pairs] \n",
    "    training_set = make_samples(pairs, scores)\n",
    "    result = []\n",
    "    \n",
    "    print('Start')\n",
    "    counter = 0\n",
    "    \n",
    "    while len(edus) > counter:\n",
    "        while True in scores:\n",
    "            # select two nodes to merge\n",
    "            j = to_merge_gr(scores)  # position of the left node\n",
    "            new_du = nodes[j] + ' ' + nodes[j+1]\n",
    "            nodes = nodes[:j] + [new_du] + nodes[j+2:]\n",
    "            counter += 1\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                new_score = in_gold((nodes[j], nodes[j+1]))\n",
    "                training_set.append((nodes[j], nodes[j+1], in_gold((nodes[j], nodes[j+1]))))\n",
    "                scores = [new_score] + scores[j+2:]\n",
    "\n",
    "            elif j+1 < len(nodes):\n",
    "                new_score_left = in_gold((nodes[j-1], nodes[j]))\n",
    "                new_score_right = in_gold((nodes[j], nodes[j+1]))\n",
    "\n",
    "                training_set += [\n",
    "                    (nodes[j-1], nodes[j], in_gold((nodes[j-1], nodes[j]))),\n",
    "                    (nodes[j], nodes[j+1], in_gold((nodes[j], nodes[j+1])))\n",
    "                ]\n",
    "\n",
    "                scores = scores[:j-1] + [new_score_left, new_score_right] + scores[j+2:]\n",
    "\n",
    "            else:\n",
    "                new_score = in_gold((nodes[j-1], nodes[j]))\n",
    "                training_set.append((nodes[j-1], nodes[j], in_gold((nodes[j-1], nodes[j]))))\n",
    "                scores = scores[:j-1] + [new_score]\n",
    "        # print(nodes, scores)\n",
    "        return\n",
    "\n",
    "    return list(set(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_snippet_ids(snippet, edus):\n",
    "    return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "\n",
    "\n",
    "def check_snippet_pair_in_dataset(dataset, snippet_left, snippet_right):\n",
    "    return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "            or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "\n",
    "\n",
    "def extract_negative_samples_for_snippet(gold, edus, snippet):\n",
    "    training_set = []\n",
    "    \n",
    "    snippet_ids = extract_snippet_ids(snippet, edus)\n",
    "    \n",
    "    if not snippet_ids:\n",
    "        return []\n",
    "        \n",
    "    if snippet_ids[0] > 0:\n",
    "        if not check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "            training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "    if snippet_ids[-1] < len(edus) - 1:\n",
    "        if not check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "            training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "def create_training_set(edus, gold):\n",
    "    training_set = []\n",
    "    \n",
    "    snippet_cache = []\n",
    "    for num, e in enumerate(gold.index):\n",
    "        snippet_x = gold.loc[e, 'snippet_x']\n",
    "        cache_x = extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "        snippet_y = gold.loc[e, 'snippet_y']\n",
    "        cache_y = extract_snippet_ids(snippet_y, edus)\n",
    "                    \n",
    "        if cache_x and cache_y:\n",
    "            snippet_cache.append((cache_x, snippet_x))\n",
    "            snippet_cache.append((cache_y, snippet_y))\n",
    "            \n",
    "#             if cache_x[0] < cache_y[0]:\n",
    "#                 training_set.append((snippet_x, snippet_y, True))\n",
    "#             else:\n",
    "#                 training_set.append((snippet_y, snippet_x, True))\n",
    "        \n",
    "    for i in range(len(edus) - 1):\n",
    "        if not check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "            training_set.append((edus[i], edus[i+1], False))\n",
    "    \n",
    "    for i in gold.index:\n",
    "        training_set += extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "        training_set += extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "    \n",
    "    for i in range(len(snippet_cache)):\n",
    "        for j in range(i, len(snippet_cache)):\n",
    "            cache_i, snippet_i = snippet_cache[i]\n",
    "            cache_j, snippet_j = snippet_cache[j]\n",
    "            \n",
    "            if cache_i[-1] + 1 == cache_j[0]:\n",
    "                if not check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                    training_set.append((snippet_i, snippet_j, False))\n",
    "            \n",
    "            if cache_j[-1] + 1 == cache_i[0]:\n",
    "                if not check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                    training_set.append((snippet_j, snippet_i, False))\n",
    "    \n",
    "    return list(set(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('./data/*.json'), key=lambda s: int(os.path.basename(s)[5]))\n",
    "test = files[::5]\n",
    "train = [file for file in files if not file in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.json')):\n",
    "    filename = filename.replace('.json', '')\n",
    "    df = read_gold(filename)\n",
    "    edus = read_edus(filename)\n",
    "        \n",
    "    new_set = create_training_set(edus, df)\n",
    "    result = []\n",
    "    for item in new_set:\n",
    "        result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "    tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "    \n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    def place_locations(row):\n",
    "        row['loc_x'] = annot['text'].find(row.snippet_x)\n",
    "        row['loc_y'] = annot['text'][row['loc_x']:].find(row.snippet_y)\n",
    "        return row\n",
    "\n",
    "    tmp = tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    tmp.to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = 'models/structure_predictor'\n",
    "! mkdir $model_path\n",
    "\n",
    "drop_columns = ['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y']\n",
    "pickle.dump(drop_columns, open(os.path.join(model_path, 'drop_columns.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'data/news_16'\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "annot = read_annotation(filename)\n",
    "negatives = read_negative(filename)\n",
    "\n",
    "%time result = features_processor(negatives, \\\n",
    "                            annot['text'],\\\n",
    "                            annot['tokens'],\\\n",
    "                            annot['sentences'],\\\n",
    "                            annot['lemma'],\\\n",
    "                            annot['morph'],\\\n",
    "                            annot['postag'],\\\n",
    "                            annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"rst_pairs/*.json.neg\"):\n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename)\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    try:\n",
    "        result = features_processor(df, \\\n",
    "                                   annotation['text'],\\\n",
    "                                   annotation['tokens'],\\\n",
    "                                   annotation['sentences'],\\\n",
    "                                   annotation['lemma'],\\\n",
    "                                   annotation['morph'],\\\n",
    "                                   annotation['postag'],\\\n",
    "                                   annotation['syntax_dep_tree'])\n",
    "\n",
    "        result.to_pickle(filename + '.neg.features')\n",
    "    except IndexError:\n",
    "        print('INDEX ERROR ::: FILENAME :::', filename)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As well as from gold examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"rst_pairs/*.json\"):\n",
    "    filename = filename.replace('.json', '')\n",
    "    \n",
    "    df = read_gold(filename)\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    try:\n",
    "        %time result = features_processor(df, \\\n",
    "                                   annotation['text'],\\\n",
    "                                   annotation['tokens'],\\\n",
    "                                   annotation['sentences'],\\\n",
    "                                   annotation['lemma'],\\\n",
    "                                   annotation['morph'],\\\n",
    "                                   annotation['postag'],\\\n",
    "                                   annotation['syntax_dep_tree'])\n",
    "\n",
    "        result.to_pickle(filename + '.gold.features')\n",
    "    except IndexError:\n",
    "        print('INDEX ERROR ::: FILENAME :::', filename)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls rst_pairs/*.gold.features | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls rst_pairs/*.neg.features | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "files = sorted(glob.glob('./rst_pairs/*.edus'), key=lambda s: int(os.path.basename(s)[5]))\n",
    "test = files[::5]\n",
    "train = [file for file in files if not file in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('news in train:', len([file for file in train if 'news' in file]) / len(train))\n",
    "print('ling in train:', len([file for file in train if 'ling' in file]) / len(train))\n",
    "print('comp in train:', len([file for file in train if 'comp' in file]) / len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('news in test:', len([file for file in test if 'news' in file]) / len(test))\n",
    "print('ling in test:', len([file for file in test if 'ling' in file]) / len(test))\n",
    "print('comp in test:', len([file for file in test if 'comp' in file]) / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_samples = []\n",
    "\n",
    "for file in train:\n",
    "    train_samples.append(pd.read_pickle(file.replace('.edus', '.gold.features')))\n",
    "    try:\n",
    "        train_samples.append(pd.read_pickle(file.replace('.edus', '.neg.features')))\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "train_samples.relation = train_samples.relation.fillna(True)\n",
    "train_samples['genre'] = train_samples.filename.map(lambda row: row.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "y_train, X_train = train_samples[TARGET].to_frame(), train_samples.drop(TARGET, axis=1).drop(columns=['category_id', 'snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y', 'filename', 'order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_feature_mask = X_train.dtypes==object\n",
    "categorical_cols = X_train.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_ohe = ohe.fit_transform(X_train[categorical_cols].values)\n",
    "X_ohe = pd.DataFrame(X_ohe, X_train.index, columns=ohe.get_feature_names(categorical_cols))\n",
    "\n",
    "X_train = X_train.join(\n",
    "   pd.DataFrame(X_ohe, X_train.index).add_prefix('cat_'), how='right'\n",
    ").drop(columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(categorical_cols, open(os.path.join(model_path, 'categorical_cols.pkl'), 'wb'))\n",
    "pickle.dump(le, open(os.path.join(model_path, 'label_encoder.pkl'), 'wb'))\n",
    "pickle.dump(ohe, open(os.path.join(model_path, 'one_hot_encoder.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "\n",
    "for file in test:\n",
    "    test_samples.append(pd.read_pickle(file.replace('.edus', '.gold.features')))\n",
    "    try:\n",
    "        test_samples.append(pd.read_pickle(file.replace('.edus', '.neg.features')))\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_samples.relation = test_samples.relation.fillna(True)\n",
    "test_samples['genre'] = test_samples.filename.map(lambda row: row.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(columns=['category_id', 'snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y', 'filename', 'order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = pickle.load(open('binary_classifier_models/categorical_cols.pkl', 'rb'))\n",
    "le = pickle.load(open('binary_classifier_models/label_encoder.pkl', 'rb'))\n",
    "ohe = pickle.load(open('binary_classifier_models/one_hot_encoder.pkl', 'rb'))\n",
    "\n",
    "X_test[categorical_cols] = X_test[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "X_ohe = ohe.transform(X_test[categorical_cols].values)\n",
    "X_ohe = pd.DataFrame(X_ohe, X_test.index, columns=ohe.get_feature_names(categorical_cols))\n",
    "\n",
    "X_test = X_test.join(\n",
    "   pd.DataFrame(X_ohe, X_test.index).add_prefix('cat_'), how='right'\n",
    ").drop(columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "std_scaler = MinMaxScaler().fit(X_train.values)\n",
    "\n",
    "X_train = pd.DataFrame(std_scaler.transform(X_train.values), index=X_train.index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(std_scaler.transform(X_test.values), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "scaler_path = 'binary_classifier_models/scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(std_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', class_weight='balanced', C=0.0005, n_jobs=4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)  \n",
    "print('weighted f1: ', metrics.f1_score(y_test, predicted, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test, predicted, average='macro'))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(os.path.join(model_path, 'model.pkl'), 'wb'))\n",
    "pickle.dump(std_scaler, open(os.path_join(model_path, 'scaler.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 300,\n",
    "    'max_depth' : 5,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 300,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "model = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 300,\n",
    "    'max_depth' : 5,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 300,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "classifier = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "feature_selector = SelectFromModel(LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1'))\n",
    "model = Pipeline([('feature_selector', feature_selector), \n",
    "                   ('classifier', classifier)])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import lightgbm.sklearn as lgb\n",
    "\n",
    "random_state = 41\n",
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 600,\n",
    "    'max_depth' : 6,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 600,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "classifier = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "feature_selector = SelectFromModel(LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1'))\n",
    "model_single = Pipeline([('feature_selector', feature_selector), \n",
    "                   ('classifier', classifier)])\n",
    "\n",
    "model = BaggingClassifier(base_estimator=model_single, \n",
    "                          n_estimators=3, \n",
    "                          max_samples=1.0, \n",
    "                          max_features=0.8, \n",
    "                          bootstrap=True, \n",
    "                          random_state=random_state)\n",
    "\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))  # here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fi = np.array(classifier.feature_importances_)\n",
    "sorted_idx = np.argsort(fi)\n",
    "print(np.count_nonzero(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 150)\n",
    "#start, finish = 0, 2000\n",
    "dd = pd.DataFrame({'Feature': np.array(X_test.keys())[sorted_idx], 'Importance': fi[sorted_idx][::-1]})\n",
    "dd = dd[dd['Importance'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.Feature.str[-2:] != '_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'predictor_relation_presence_classifier.pkl'\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "model = CatBoostClassifier(one_hot_max_size=5,\n",
    "                           learning_rate=0.5,\n",
    "                           iterations=200,\n",
    "                           class_weights=[0.35, 1.],\n",
    "                           depth=3,\n",
    "                           #task_type=\"GPU\"\n",
    "                          )\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train.astype(float),\n",
    "          eval_set=Pool(X_test, y_test.astype(float)),\n",
    "          verbose=False,\n",
    "          plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
