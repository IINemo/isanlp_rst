{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building\n",
    "\n",
    "1. Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "2. Train models, save the best one.\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``models/structure_predictor/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text[row['loc_x']+len(row.snippet_x):].find(row.snippet_y)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor):\n",
    "        self.features_processor = features_processor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "        self.genre = None\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        return self.label_predictor.predict(features)\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, corpus):\n",
    "        RSTTreePredictor.__init__(self, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return [args[0].text, args[1].text]\n",
    "    \n",
    "    def initialize_features(self, *args):\n",
    "        return [(args[0][i].text, args[0][i+1].text) for i in range(len(args[0]) - 1)]\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        # print('>> features =', features)\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return ((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                    or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                        axis=0) != 0)\n",
    "\n",
    "        left_snippet, right_snippet = features\n",
    "        return float(_check_snippet_pair_in_dataset(left_snippet, right_snippet))\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        label = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].category_id.values\n",
    "        if label.size == 0:\n",
    "            return 'no_relation'\n",
    "        \n",
    "        return label[0]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        nuclearity = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].order.values\n",
    "        if nuclearity.size == 0:\n",
    "            return '_'\n",
    "        \n",
    "        return nuclearity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            gap_counter = 0\n",
    "            #while len(left.text + right.text) < len(self.text):\n",
    "            #    self.text = left.text + ' ' * gap_counter + right.text\n",
    "            #    gap_counter += 1\n",
    "            self.start = left.start\n",
    "            self.end = right.end\n",
    "        \n",
    "        # (1) for gold tree parsing\n",
    "        \"\"\"\n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "        \"\"\"\n",
    "        # (2) ??\n",
    "        \n",
    "        if self.left:\n",
    "            self.text = ' '.join([self.left.text, self.right.text])\n",
    "        else:\n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "    \n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "        \n",
    "        negative_nodes = []\n",
    "        \n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus        \n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes)\n",
    "        scores = list(map(self.tree_predictor.predict_pair_proba, features))\n",
    "        relations = list(map(self.tree_predictor.predict_label, features))\n",
    "        nuclearities = list(map(self.tree_predictor.predict_nuclearity, features))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            \n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features[j]),\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "            \n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1])\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = [predicted] + scores[j + 2:]\n",
    "                features = [features_right] + features[j + 2:]\n",
    "                \n",
    "                if predicted == 0:\n",
    "                    relation = self.tree_predictor.predict_label(features_right)\n",
    "                    if relation == 'relation':\n",
    "                        negative_nodes.append(\n",
    "                            DiscourseUnit(\n",
    "                                id=None,\n",
    "                                left=nodes[j],\n",
    "                                right=nodes[j + 1],\n",
    "                                relation=relation,\n",
    "                                nuclearity=self.tree_predictor.predict_nuclearity(features_right),\n",
    "                                proba=predicted,\n",
    "                                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j])\n",
    "                predicted_left = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                if predicted_left == 0:\n",
    "                    relation = self.tree_predictor.predict_label(features_left)\n",
    "                    if relation == 'relation':\n",
    "                        negative_nodes.append(\n",
    "                            DiscourseUnit(\n",
    "                                id=None,\n",
    "                                left=nodes[j - 1],\n",
    "                                right=nodes[j],\n",
    "                                relation=relation,\n",
    "                                nuclearity=self.tree_predictor.predict_nuclearity(features_left),\n",
    "                                proba=predicted_left,\n",
    "                                text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1])\n",
    "                predicted_right = self.tree_predictor.predict_pair_proba(features_right)\n",
    "                if predicted_right == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation=self.tree_predictor.predict_label(features_right),\n",
    "                            nuclearity=self.tree_predictor.predict_nuclearity(features_right),\n",
    "                            proba=predicted_right,\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "                scores = scores[:j - 1] + [predicted_left] + [predicted_right] + scores[j + 2:]\n",
    "                features = features[:j - 1] + [features_left] + [features_right] + features[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j])\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                if predicted == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation=self.tree_predictor.predict_label(features_left),\n",
    "                            nuclearity=self.tree_predictor.predict_nuclearity(features_left),\n",
    "                            proba=predicted,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                    ))\n",
    "                    \n",
    "                scores = scores[:j - 1] + [predicted]\n",
    "                features = features[:j - 1] + [features_left]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return negative_nodes\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "\n",
    "gen = RandomNegativeGenerator()\n",
    "#gen = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.json')):\n",
    "    filename = filename.replace('.json', '')\n",
    "    df = read_gold(filename)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    if gen.__name__() == 'RandomNegativeGenerator':\n",
    "        tmp = gen(edus, df, annot['text'])\n",
    "    \n",
    "    elif gen.__name__() == 'GreedyNegativeGenerator':\n",
    "        _edus = []\n",
    "        last_end = 0\n",
    "        for max_id in range(len(edus)):\n",
    "            start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "            end = start + len(edus[max_id])\n",
    "            temp = DiscourseUnit(\n",
    "                    id=max_id,\n",
    "                    left=None,\n",
    "                    right=None,\n",
    "                    relation='edu',\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    orig_text=annot['text'],\n",
    "                    proba=1.\n",
    "                )\n",
    "            _edus.append(temp)\n",
    "            last_end = end\n",
    "\n",
    "        tmp = gen(_edus, df, annot['text'])\n",
    "        tmp = pd.DataFrame(extr_pairs_forest(tmp), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "        tmp = tmp[tmp.category_id == 'no_relation']\n",
    "    \n",
    "    tmp.to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "\n",
    "gen = RandomNegativeGenerator()\n",
    "#gen = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/news1_16*.json')):\n",
    "    filename = filename.replace('.json', '')\n",
    "    df = read_gold(filename)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    if gen.__name__() == 'RandomNegativeGenerator':\n",
    "        tmp = gen(edus, df, annot['text'])\n",
    "    \n",
    "    elif gen.__name__() == 'GreedyNegativeGenerator':\n",
    "        _edus = []\n",
    "        last_end = 0\n",
    "        for max_id in range(len(edus)):\n",
    "            start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "            end = start + len(edus[max_id])\n",
    "            temp = DiscourseUnit(\n",
    "                    id=max_id,\n",
    "                    left=None,\n",
    "                    right=None,\n",
    "                    relation='edu',\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    orig_text=annot['text'],\n",
    "                    proba=1.\n",
    "                )\n",
    "            _edus.append(temp)\n",
    "            last_end = end\n",
    "\n",
    "        tmp = gen(_edus, df, annot['text'])\n",
    "        tmp = pd.DataFrame(extr_pairs_forest(tmp), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "        tmp = tmp[tmp.category_id == 'no_relation']\n",
    "    \n",
    "    tmp.to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = 'models/structure_predictor'\n",
    "! mkdir $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'data/news1_16'\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "annot = read_annotation(filename)\n",
    "negatives = read_negative(filename)\n",
    "negatives = negatives.drop(columns=['loc_y'])\n",
    "\n",
    "%time result = features_processor(negatives, \\\n",
    "                            annot['text'],\\\n",
    "                            annot['tokens'],\\\n",
    "                            annot['sentences'],\\\n",
    "                            annot['lemma'],\\\n",
    "                            annot['morph'],\\\n",
    "                            annot['postag'],\\\n",
    "                            annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm data/news2_6.json.neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in tqdm(glob.glob(\"data/*.json.neg\")):    \n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename).drop(columns=['loc_y'])\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    try:\n",
    "        result = features_processor(df, \\\n",
    "                                   annotation['text'],\\\n",
    "                                   annotation['tokens'],\\\n",
    "                                   annotation['sentences'],\\\n",
    "                                   annotation['lemma'],\\\n",
    "                                   annotation['morph'],\\\n",
    "                                   annotation['postag'],\\\n",
    "                                   annotation['syntax_dep_tree'])\n",
    "\n",
    "        result.to_pickle(filename + '.neg.features')\n",
    "    except IndexError:\n",
    "        print('INDEX ERROR ::: FILENAME :::', filename)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_data\n",
    "\n",
    "train, test = split_data('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "train_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = [c for c in train_samples.columns if len(set(train_samples[c])) == 1]\n",
    "to_drop = ['snippet_x', 'snippet_y', 'category_id', 'snippet_x_tmp', 'snippet_y_tmp', 'filename', 'order', 'postags_x', 'postags_y']\n",
    "train_samples = train_samples.drop(columns=constants)\n",
    "pickle.dump(constants+to_drop, open(os.path.join(model_path, 'drop_columns.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "y_train, X_train = train_samples[TARGET].to_frame(), train_samples.drop(TARGET, axis=1).drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_feature_mask = X_train.dtypes==object\n",
    "categorical_cols = X_train.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_ohe = ohe.fit_transform(X_train[categorical_cols].values)\n",
    "X_ohe = pd.DataFrame(X_ohe, X_train.index, columns=ohe.get_feature_names(categorical_cols))\n",
    "\n",
    "X_train = X_train.join(\n",
    "   pd.DataFrame(X_ohe, X_train.index).add_prefix('cat_'), how='right'\n",
    ").drop(columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle.dump(categorical_cols, open(os.path.join(model_path, 'categorical_cols.pkl'), 'wb'))\n",
    "pickle.dump(le, open(os.path.join(model_path, 'label_encoder.pkl'), 'wb'))\n",
    "pickle.dump(ohe, open(os.path.join(model_path, 'one_hot_encoder.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "test_samples = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(columns=to_drop+['category_id']+constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "std_scaler = MinMaxScaler().fit(X_train.values)\n",
    "\n",
    "X_train = pd.DataFrame(std_scaler.transform(X_train.values), index=X_train.index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(std_scaler.transform(X_test.values), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "scaler_path = os.path.join(model_path, 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(std_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', C=0.0005, n_jobs=4, class_weight='balanced', random_state=random_state)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "print('pr:', metrics.precision_score(y_test, predicted))\n",
    "print('re:', metrics.recall_score(y_test, predicted))\n",
    "print('f1:', metrics.f1_score(y_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(random_state=random_state, C=0.01, class_weight='balanced')\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "model = svc\n",
    "predicted = model.predict(X_test)\n",
    "print('pr:', metrics.precision_score(y_test, predicted))\n",
    "print('re:', metrics.recall_score(y_test, predicted))\n",
    "print('f1:', metrics.f1_score(y_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svc, open(os.path.join(model_path, 'model.pkl'), 'wb'))\n",
    "pickle.dump(std_scaler, open(os.path.join(model_path, 'scaler.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 300,\n",
    "    'max_depth' : 5,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 300,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "model = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 300,\n",
    "    'max_depth' : 5,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 300,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "classifier = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "feature_selector = SelectFromModel(LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1'))\n",
    "model = Pipeline([('feature_selector', feature_selector), \n",
    "                   ('classifier', classifier)])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import lightgbm.sklearn as lgb\n",
    "\n",
    "random_state = 41\n",
    "lgbm_param_bin =  {\n",
    "    'tree_learner': 'feature',\n",
    "    'task': 'train',\n",
    "    'random_state': random_state,\n",
    "    'metric': 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'boosting_type': 'dart',\n",
    "    'application': 'binary',\n",
    "    'num_iterations': 600,\n",
    "    'max_depth' : 6,\n",
    "    'is_unbalance' : True,\n",
    "    'n_estimators' : 600,\n",
    "    'colsample_bytree' : 0.8\n",
    "}\n",
    "classifier = lgb.LGBMClassifier(**lgbm_param_bin)\n",
    "feature_selector = SelectFromModel(LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1'))\n",
    "model_single = Pipeline([('feature_selector', feature_selector), \n",
    "                   ('classifier', classifier)])\n",
    "\n",
    "model = BaggingClassifier(base_estimator=model_single, \n",
    "                          n_estimators=3, \n",
    "                          max_samples=1.0, \n",
    "                          max_features=0.8, \n",
    "                          bootstrap=True, \n",
    "                          random_state=random_state)\n",
    "\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))  # here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fi = np.array(classifier.feature_importances_)\n",
    "sorted_idx = np.argsort(fi)\n",
    "print(np.count_nonzero(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 150)\n",
    "#start, finish = 0, 2000\n",
    "dd = pd.DataFrame({'Feature': np.array(X_test.keys())[sorted_idx], 'Importance': fi[sorted_idx][::-1]})\n",
    "dd = dd[dd['Importance'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.Feature.str[-2:] != '_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'predictor_relation_presence_classifier.pkl'\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "model = CatBoostClassifier(one_hot_max_size=5,\n",
    "                           learning_rate=.03,\n",
    "                           iterations=25000,\n",
    "                           scale_pos_weight=3.,\n",
    "                           depth=2,\n",
    "                           score_function='SolarL2',\n",
    "                           random_state=random_state,\n",
    "                           task_type='GPU',\n",
    "                           devices='0',\n",
    "                           #task_type=\"GPU\"\n",
    "                          )\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train.astype(float),\n",
    "          eval_set=Pool(X_test, y_test.astype(float)),\n",
    "          verbose=False,\n",
    "          plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print('pr:', metrics.precision_score(y_test, predicted))\n",
    "print('re:', metrics.recall_score(y_test, predicted))\n",
    "print('f1:', metrics.f1_score(y_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[predicted == 0.].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[predicted == 1.].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print('pr:', metrics.precision_score(y_test, predicted))\n",
    "print('re:', metrics.recall_score(y_test, predicted))\n",
    "print('f1:', metrics.f1_score(y_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(os.path.join(model_path, 'model.pkl'), 'wb'))\n",
    "pickle.dump(std_scaler, open(os.path.join(model_path, 'scaler.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
