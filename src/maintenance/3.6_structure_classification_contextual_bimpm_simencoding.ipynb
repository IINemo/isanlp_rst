{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building\n",
    "\n",
    "1. prepare train/test sets\n",
    "2. generate config file for bimpm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_sequence(sequence):\n",
    "    symbol_map = {\n",
    "        'x': 'х',\n",
    "        'X': 'X',\n",
    "        'y': 'у',\n",
    "        '—': '-',\n",
    "        '“': '«',\n",
    "        '‘': '«',\n",
    "        '”': '»',\n",
    "        '’': '»',\n",
    "        '😆': '😄',\n",
    "        '😊': '😄',\n",
    "        '😑': '😄',\n",
    "        '😔': '😄',\n",
    "        '😉': '😄',\n",
    "        '❗': '😄',\n",
    "        '🤔': '😄',\n",
    "        '😅': '😄',\n",
    "        '⚓': '😄',\n",
    "        'ε': 'α',\n",
    "        'ζ': 'α',\n",
    "        'η': 'α',\n",
    "        'μ': 'α',\n",
    "        'δ': 'α',\n",
    "        'λ': 'α',\n",
    "        'ν': 'α',\n",
    "        'β': 'α',\n",
    "        'γ': 'α',\n",
    "        'と': '尋',\n",
    "        'の': '尋',\n",
    "        '神': '尋',\n",
    "        '隠': '尋',\n",
    "        'し': '尋',\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for token in sequence.split():\n",
    "\n",
    "        for key, value in symbol_map.items():\n",
    "            token = token.replace(key, value)\n",
    "\n",
    "        for keyword in ['www', 'http']:\n",
    "            if keyword in token:\n",
    "                token = '_html_'\n",
    "\n",
    "        result.append(token)\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def _prepare_left_context(sequence, _len=3):\n",
    "    return ' '.join(_prepare_sequence(sequence).split()[-_len:])\n",
    "\n",
    "def _prepare_right_context(sequence, _len=3):\n",
    "    return ' '.join(_prepare_sequence(sequence).split()[:_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.'):\n",
    "        row.left_context += row.snippet_x[0]\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    if row.right_context[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_y += row.right_context[0]\n",
    "        row.right_context = row.right_context[1:].strip()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_lstm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf5_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf5_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf5_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate train/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "MAX_DOCS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_context(position:int, number_of_tokens:int, tokens:list):\n",
    "    result = [token.text for token in tokens[position-number_of_tokens: position]]\n",
    "    if len(result) < number_of_tokens:\n",
    "        result = ['_END_'] * (number_of_tokens - len(result)) + result\n",
    "    return ' '.join(result)\n",
    "\n",
    "def get_right_context(position:int, number_of_tokens:int, tokens:list):\n",
    "    result = [token.text for token in tokens[position: position+number_of_tokens]]\n",
    "    if len(result) < number_of_tokens:\n",
    "        result = result + ['_END_'] * (number_of_tokens - len(result))\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "random_state = 45\n",
    "train_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    tokens = read_annotation(file.replace('.edus', ''))['tokens']\n",
    "    edus = read_edus(file.replace('.edus', ''))\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['left_context'] = gold.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    gold['right_context'] = gold.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'left_context', 'right_context']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['left_context'] = negative.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    negative['right_context'] = negative.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'left_context', 'right_context']]])\n",
    "    sample = sample.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')    \n",
    "    train_samples.append(sample)\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "train_samples['snippet_x'] = train_samples.snippet_x.map(_prepare_sequence)\n",
    "train_samples['snippet_y'] = train_samples.snippet_y.map(_prepare_sequence)\n",
    "train_samples['left_context'] = train_samples.left_context.map(_prepare_left_context)\n",
    "train_samples['right_context'] = train_samples.right_context.map(_prepare_right_context)\n",
    "train_samples = train_samples[train_samples.snippet_x.map(len) > 0]\n",
    "train_samples = train_samples[train_samples.snippet_y.map(len) > 0]\n",
    "train_samples = train_samples.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], \n",
    "                                                                                        keep='last') \n",
    "train_samples = train_samples.sample(frac=1, random_state=random_state).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.right_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_samples[train_samples.right_context.str.contains('END')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'left_context', 'right_context', 'same_sentence', 'index']].to_csv(TRAIN_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_state = 45\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    tokens = read_annotation(file.replace('.edus', ''))['tokens']\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['left_context'] = gold.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    gold['right_context'] = gold.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'left_context', 'right_context']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['left_context'] = negative.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    negative['right_context'] = negative.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'left_context', 'right_context']]])\n",
    "    dev_samples.append(sample)\n",
    "\n",
    "dev_samples = pd.concat(dev_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "dev_samples['snippet_x'] = dev_samples.snippet_x.map(_prepare_sequence)\n",
    "dev_samples['snippet_y'] = dev_samples.snippet_y.map(_prepare_sequence)\n",
    "dev_samples['left_context'] = dev_samples.left_context.map(_prepare_left_context)\n",
    "dev_samples['right_context'] = dev_samples.right_context.map(_prepare_right_context)\n",
    "dev_samples = dev_samples[dev_samples.snippet_x.map(len) > 0]\n",
    "dev_samples = dev_samples[dev_samples.snippet_y.map(len) > 0]\n",
    "dev_samples = dev_samples.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last') \n",
    "dev_samples = dev_samples.sample(frac=1, random_state=random_state).reset_index(level=0)\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'left_context', 'right_context', 'same_sentence', 'index']].to_csv(DEV_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_samples[dev_samples.snippet_y.str.contains(\"несмотря на\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_samples[dev_samples.snippet_y.str.contains(\"поэтому\")].head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 45\n",
    "test_samples = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    tokens = read_annotation(file.replace('.edus', ''))['tokens']\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['left_context'] = gold.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    gold['right_context'] = gold.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'left_context', 'right_context']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['left_context'] = negative.token_begin_x.map(lambda row: get_left_context(row, 4, tokens))\n",
    "    negative['right_context'] = negative.token_end_y.map(lambda row: get_right_context(row, 4, tokens))\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', \n",
    "                                          'same_sentence', 'left_context', 'right_context']]])\n",
    "    test_samples.append(sample)\n",
    "\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_samples['snippet_x'] = test_samples.snippet_x.map(_prepare_sequence)\n",
    "test_samples['snippet_y'] = test_samples.snippet_y.map(_prepare_sequence)\n",
    "test_samples['left_context'] = test_samples.left_context.map(_prepare_left_context)\n",
    "test_samples['right_context'] = test_samples.right_context.map(_prepare_right_context)\n",
    "test_samples = test_samples[test_samples.snippet_x.map(len) > 0]\n",
    "test_samples = test_samples[test_samples.snippet_y.map(len) > 0]\n",
    "test_samples = test_samples.sort_values(['relation'], \n",
    "                                        ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last') \n",
    "test_samples = test_samples.sample(frac=1, random_state=random_state).reset_index(level=0)\n",
    "\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', \n",
    "              'left_context', 'right_context', \n",
    "              'same_sentence', 'index']].to_csv(TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize model with adding inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -r models/customization_package2\n",
    "! mkdir models/customization_package2\n",
    "! touch models/customization_package2/__init__.py\n",
    "! mkdir models/customization_package2/tokenizers\n",
    "! mkdir models/customization_package2/dataset_readers\n",
    "! mkdir models/customization_package2/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/dataset_readers/__init__.py\n",
    "\n",
    "try:\n",
    "    from customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from customization_package2.dataset_readers.contextual_reader import ContextualReader\n",
    "except ModuleNotFoundError:\n",
    "    from models.customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.customization_package2.dataset_readers.contextual_reader import ContextualReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/dataset_readers/contextual_reader.py\n",
    "\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "\n",
    "try:\n",
    "    from customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "except ModuleNotFoundError:\n",
    "    from models.customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"contextual_reader\")\n",
    "class ContextualReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional\n",
    "        Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens.\n",
    "        Defaults to `WhitespaceTokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
    "        Indexers used to define input token representations. Defaults to `{\"tokens\":\n",
    "        SingleIdTokenIndexer()}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: Tokenizer = None, token_indexers: Dict[str, TokenIndexer] = None, lazy: bool = True) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(cached_path(file_path), \"r\", encoding='utf8') as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 7:\n",
    "                    yield self.text_to_instance(premise=row[1], hypothesis=row[2], \n",
    "                                                left_context=row[3], right_context=row[4],\n",
    "                                                label=row[0], same_sentence=row[5])\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(\n",
    "        self,  # type: ignore\n",
    "        premise: str,\n",
    "        hypothesis: str,\n",
    "        left_context: str,\n",
    "        right_context: str,\n",
    "        label: str,\n",
    "        same_sentence: str,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_premise = self._tokenizer.tokenize(premise)\n",
    "        tokenized_hypothesis = self._tokenizer.tokenize(hypothesis)\n",
    "        tokenized_left_context = self._tokenizer.tokenize(left_context)\n",
    "        tokenized_right_context = self._tokenizer.tokenize(right_context)\n",
    "        tokenized_input = tokenized_left_context + [] + tokenized_premise + [\n",
    "            ] + tokenized_hypothesis + [] + tokenized_right_context\n",
    "        #tokenized_input = self._tokenizer.tokenize(' '.join([left_context, premise, hypothesis, right_context]))\n",
    "        fields[\"joined_input\"] = TextField(tokenized_input, self._token_indexers)\n",
    "        fields[\"premise\"] = TextField(tokenized_premise, self._token_indexers)\n",
    "        fields[\"hypothesis\"] = TextField(tokenized_hypothesis, self._token_indexers)\n",
    "        fields[\"left_context\"] = TextField(tokenized_left_context, self._token_indexers)\n",
    "        fields[\"right_context\"] = TextField(tokenized_right_context, self._token_indexers)\n",
    "        additional_features = list(map(list, zip(*same_sentence)))\n",
    "        fields[\"metadata\"] = ArrayField(np.array(additional_features))\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/model/__init__.py\n",
    "\n",
    "try:\n",
    "    from customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from customization_package2.model.contextual_bimpm import ContextualBiMpm\n",
    "    from customization_package2.model.contextual_bimpm_predictor import ContextualBiMpmPredictor\n",
    "except ModuleNotFoundError:\n",
    "    from models.customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.customization_package2.model.contextual_bimpm import ContextualBiMpm\n",
    "    from models.customization_package2.model.contextual_bimpm_predictor import ContextualBiMpmPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/tokenizers/whitespace_tokenizer.py\n",
    "\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "from overrides import overrides\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "@Tokenizer.register(\"simple\")\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/model/contextual_bimpm.py\n",
    "\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "from overrides import overrides\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.nn.util import get_final_encoder_states\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@Model.register(\"contextual_bimpm_cnn\")\n",
    "class ContextualBiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 encoder_context: Seq2SeqEncoder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 dropout: float = 0.1,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(ContextualBiMpm, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.encoder_context = encoder_context\n",
    "        \n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "        matching_dim = self.matcher_word.get_output_dim() + \\\n",
    "                       self.matcher_forward1.get_output_dim() + self.matcher_backward1.get_output_dim() + \\\n",
    "                       self.matcher_forward2.get_output_dim() + self.matcher_backward2.get_output_dim()\n",
    "                        \n",
    "        #matching_dim *= 3  # contextual matches on both sides\n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                               \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "        \n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy(),\n",
    "                        \"f1\": F1Measure(1)}\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor([0.1, 1]))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                left_context: Dict[str, torch.LongTensor],\n",
    "                right_context: Dict[str, torch.LongTensor],\n",
    "                metadata: List[Dict[str, torch.FloatTensor]],\n",
    "                joined_input: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor=None,# pylint:disable=unused-argument\n",
    "               ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "        mask_left_context = util.get_text_field_mask(left_context)\n",
    "        mask_right_context = util.get_text_field_mask(right_context)\n",
    "\n",
    "#         print()\n",
    "#         print(\"size(0) of joined input:\", joined_input[\"elmo\"].size(0))\n",
    "#         print(\"size(1) of joined input:\", joined_input[\"elmo\"].size(1))\n",
    "        \n",
    "        # embedding and encoding\n",
    "        _left_context: Dict[str, torch.LongTensor] = {}\n",
    "        for key in joined_input.keys():\n",
    "            _left_context[key] = joined_input[key][:,0:left_context[key].size(1)]\n",
    "            \n",
    "        embedded_left_context = self.dropout(self.text_field_embedder(_left_context))\n",
    "        encoded_left_context = self.dropout(\n",
    "            self.encoder_context(embedded_left_context, mask_left_context))\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        _premise: Dict[str, torch.LongTensor] = {}\n",
    "        for key in joined_input.keys():\n",
    "            _premise[key] = joined_input[key][:,left_context[key].size(1):left_context[key].size(1) + premise[key].size(1)]\n",
    "        \n",
    "        embedded_premise = self.dropout(self.text_field_embedder(_premise))\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        _hypothesis: Dict[str, torch.LongTensor] = {}\n",
    "        for key in joined_input.keys():\n",
    "            _hypothesis[key] = joined_input[key][:,joined_input[key].size(1) - right_context[key].size(\n",
    "                1) - hypothesis[key].size(1):joined_input[key].size(1) - right_context[key].size(1)]\n",
    "        \n",
    "        embedded_hypothesis = self.dropout(self.text_field_embedder(_hypothesis))\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "        \n",
    "        _right_context: Dict[str, torch.LongTensor] = {}\n",
    "        for key in joined_input.keys():\n",
    "            _right_context[key] = joined_input[key][:,joined_input[key].size(1) - right_context[key].size(1):]\n",
    "\n",
    "        embedded_right_context = self.dropout(self.text_field_embedder(_right_context))\n",
    "        encoded_right_context = self.dropout(\n",
    "            self.encoder_context(embedded_right_context, mask_right_context))\n",
    "        \n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, \n",
    "                            embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        batch_size, _ = aggregated_premise.size()\n",
    "        encoded_meta = self.dropout(metadata.float().view(batch_size, -1))\n",
    "        encoded_left_context = encoded_left_context.view(batch_size, -1)\n",
    "        encoded_right_context = encoded_right_context.view(batch_size, -1)\n",
    "        \n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(torch.cat([aggregated_premise, aggregated_hypothesis, \n",
    "                                                        encoded_meta, encoded_left_context, encoded_right_context], dim=-1))\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts indices to string labels, and adds a ``\"label\"`` key to the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"probs\"].cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"f1\": self.metrics[\"f1\"].get_metric(reset=reset)[2],\n",
    "            \"accuracy\": self.metrics[\"accuracy\"].get_metric(reset=reset)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package2/model/contextual_bimpm_predictor.py\n",
    "\n",
    "\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.predictors.decomposable_attention import DecomposableAttentionPredictor\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "from overrides import overrides\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "try:\n",
    "    from customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from customization_package2.dataset_readers.contextual_reader import ContextualReader\n",
    "except ModuleNotFoundError:\n",
    "    from models.customization_package2.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.customization_package2.dataset_readers.contextual_reader import ContextualReader\n",
    "\n",
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"contextual_bimpm_predictor\")\n",
    "class ContextualBiMpmPredictor(DecomposableAttentionPredictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "    def predict(self, premise: str, hypothesis: str, left_context: str, right_context: str, metadata: str) -> JsonDict:\n",
    "        return self.predict_json({\"premise\": premise, \"hypothesis\": hypothesis, \n",
    "                                  \"left_context\": left_context, \"right_context\": right_context,\n",
    "                                  \"metadata\": metadata})\n",
    "    \n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like `{\"premise\": \"...\", \"hypothesis\": \"...\", \"metadata\": \"...\", \n",
    "                                       \"right_context\": \"...\", \"left_context\": \"...\"}`.\n",
    "        \"\"\"\n",
    "        premise_text = json_dict[\"premise\"]\n",
    "        hypothesis_text = json_dict[\"hypothesis\"]\n",
    "        left_context_text = json_dict[\"left_context\"]\n",
    "        right_context_text = json_dict[\"right_context\"]\n",
    "        same_sentence = json_dict[\"metadata\"]\n",
    "        return self._dataset_reader.text_to_instance(premise_text, hypothesis_text, \n",
    "                                                     left_context=left_context_text, \n",
    "                                                     right_context=right_context_text, \n",
    "                                                     label=None, same_sentence=same_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -64 models/structure_predictor_lstm/structure_cf5_train.tsv > models/structure_predictor_lstm/structure_cf5_train_s.tsv\n",
    "! head -64 models/structure_predictor_lstm/structure_cf5_dev.tsv > models/structure_predictor_lstm/structure_cf5_dev_s.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat models/structure_predictor_lstm/structure_cf5_train_s.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config7_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "local NUM_EPOCHS = 50;\n",
    "local LR = 1e-3;\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"contextual_reader\",\n",
    "    \"lazy\": false,\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_lstm/structure_cf5_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_lstm/structure_cf5_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"contextual_bimpm_cnn\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.1\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "              },\n",
    "              \"dropout\": 0.2\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+100,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder_context\": {\n",
    "      \"type\": \"gru\",\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 10,\n",
    "      \"num_layers\": 1,\n",
    "      \"bidirectional\": true,\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 200+200+1+60+60,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"relu\", \"linear\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 2,\n",
    "        \"padding_noise\": 0,\n",
    "        \"sorting_keys\": [\n",
    "            [\n",
    "                \"premise\",\n",
    "                \"num_tokens\"\n",
    "            ],\n",
    "            [\n",
    "                \"hypothesis\",\n",
    "                \"num_tokens\"\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "    \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 7,\n",
    "    \"cuda_device\": 1,\n",
    "    \"grad_norm\": 10.0,\n",
    "    \"validation_metric\": \"+f1\",\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": LR\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/train_structure_predictor7.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor7.sh elmo result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf5_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf5_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_lstm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_lstm/${RESULT_DIR}/ structure_predictor_lstm/config7_${METHOD}.json \\\n",
    "   --include-package customization_package2\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_lstm/${RESULT_DIR}/predictions_dev.json \\\n",
    "    structure_predictor_lstm/${RESULT_DIR}/model.tar.gz structure_predictor_lstm/${DEV_FILE_PATH} \\\n",
    "    --include-package customization_package2 \\\n",
    "    --predictor contextual_bimpm_predictor\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_lstm/${RESULT_DIR}/predictions_test.json \\\n",
    "    structure_predictor_lstm/${RESULT_DIR}/model.tar.gz structure_predictor_lstm/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package2 \\\n",
    "    --predictor contextual_bimpm_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'result_74'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
