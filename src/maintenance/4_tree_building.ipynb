{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            self.start = left.start\n",
    "            self.end = right.end+1\n",
    "        \n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(tree):\n",
    "    def _(n):\n",
    "        if n.relation:\n",
    "            value = (n.relation, \"%.2f\"%(n.proba))\n",
    "        else:\n",
    "            value = n.text\n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    return printBTree(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor):\n",
    "        self.features_processor = features_processor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "        self.genre = None\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        return self.label_predictor.predict(features)\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, corpus):\n",
    "        RSTTreePredictor.__init__(self, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return [args[0].text, args[1].text]\n",
    "    \n",
    "    def initialize_features(self, *args):\n",
    "        return [(args[0][i].text, args[0][i+1].text) for i in range(len(args[0]) - 1)]\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        # print('>> features =', features)\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return ((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                    or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                        axis=0) != 0)\n",
    "\n",
    "        left_snippet, right_snippet = features\n",
    "        return float(_check_snippet_pair_in_dataset(left_snippet, right_snippet))\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        label = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].category_id.values\n",
    "        if label.size == 0:\n",
    "            return 'relation'\n",
    "        \n",
    "        return label[0]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        nuclearity = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].order.values\n",
    "        if nuclearity.size == 0:\n",
    "            return '_'\n",
    "        \n",
    "        return nuclearity[0]\n",
    "\n",
    "\n",
    "class CustomTreePredictor(RSTTreePredictor):\n",
    "    \"\"\"\n",
    "    Contains trained classifiers and feature processors needed for tree prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor=None):\n",
    "        RSTTreePredictor.__init__(self, features_processor, relation_predictor, label_predictor)\n",
    "\n",
    "    def extract_features(self, left_node: DiscourseUnit, right_node: DiscourseUnit,\n",
    "                         annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                         annot_syntax_dep_tree):\n",
    "        pair = pd.DataFrame({\n",
    "            'snippet_x': [left_node.text.strip()],\n",
    "            'snippet_y': [right_node.text.strip()],\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_processor(pair, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features.values.tolist()\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "        \n",
    "    def initialize_features(self, nodes, \n",
    "                            annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, \n",
    "                            annot_lemma, annot_syntax_dep_tree):\n",
    "        pairs = pd.DataFrame({\n",
    "            'snippet_x': [node.text.strip() for node in nodes[:-1]],\n",
    "            'snippet_y': [node.text.strip() for node in nodes[1:]]\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            features = self.features_processor(pairs, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features.values.tolist()\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        if type(features[0]) != float:\n",
    "            return self.relation_predictor.predict_proba(features)[0][1]\n",
    "        else:\n",
    "            return self.relation_predictor.predict_proba([features])[0][1]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        # ToDO:\n",
    "        return 'unavail'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "\n",
    "class GreedyRSTParser:\n",
    "    def __init__(self, tree_predictor, forest_threshold=0.05):\n",
    "        \"\"\"\n",
    "        :param RSTTreePredictor tree_predictor:\n",
    "        :param float forest_threshold: minimum relation probability to append the pair into the tree\n",
    "        \"\"\"\n",
    "        self.tree_predictor = tree_predictor\n",
    "        self.forest_threshold = forest_threshold\n",
    "\n",
    "    def __call__(self, edus, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                 annot_syntax_dep_tree, genre=None):\n",
    "        \"\"\"\n",
    "        :param list edus: DiscourseUnit\n",
    "        :param str annot_text: original text\n",
    "        :param list annot_tokens: isanlp.annotation.Token\n",
    "        :param list annot_sentences: isanlp.annotation.Sentence\n",
    "        :param list annot_postag: lists of str for each sentence\n",
    "        :param annot_lemma: lists of str for each sentence\n",
    "        :param annot_syntax_dep_tree: list of isanlp.annotation.WordSynt for each sentence\n",
    "        :return: list of DiscourseUnit containing each extracted tree\n",
    "        \"\"\"\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        self.tree_predictor.genre = genre\n",
    "\n",
    "        nodes = edus\n",
    "\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes, annot_text, annot_tokens,\n",
    "                                                 annot_sentences,\n",
    "                                                 annot_postag, annot_morph, annot_lemma,\n",
    "                                                 annot_syntax_dep_tree)\n",
    "     \n",
    "        scores = list(map(self.tree_predictor.predict_pair_proba, features))\n",
    "        relations = list(map(self.tree_predictor.predict_label, features))\n",
    "        nuclearities = list(map(self.tree_predictor.predict_nuclearity, features))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            \n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features[j]),\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                      annot_text, annot_tokens, \n",
    "                                                                      annot_sentences, annot_postag,\n",
    "                                                                      annot_morph, annot_lemma, \n",
    "                                                                      annot_syntax_dep_tree)\n",
    "                _scores = [self.tree_predictor.predict_pair_proba(features_right)]\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = [features_right] + features[j + 2:]\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]], \n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_postag, annot_morph, annot_lemma,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "                _scores = list(map(self.tree_predictor.predict_pair_proba, _features))\n",
    "                features = features[:j - 1] + _features + features[j + 2:]\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                     annot_text, annot_tokens, \n",
    "                                                                     annot_sentences, annot_postag,\n",
    "                                                                     annot_morph, annot_lemma, \n",
    "                                                                     annot_syntax_dep_tree)\n",
    "                _scores = [self.tree_predictor.predict_pair_proba(features_left)]\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = features[:j - 1] + [features_left]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_embeddings(embedder, X, maxlen=100):\n",
    "    X_ = [text[:text.rfind('_')] for text in X.split()]\n",
    "    result = np.zeros((embedder.vector_size, maxlen))\n",
    "\n",
    "    for i in range(min(len(X_), maxlen)):\n",
    "        try:\n",
    "            result[i] = embedder[X_[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    DROP_COLUMNS = ['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y']\n",
    "\n",
    "    def __init__(self, processor, scaler=None, categorical_cols=None, one_hot_encoder=None, label_encoder=None):\n",
    "        self.processor = processor\n",
    "        self.scaler = scaler\n",
    "        self._categorical_cols = categorical_cols\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __call__(self, df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree):\n",
    "        X = self.processor(df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "        X = X.drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self._categorical_cols:\n",
    "            if self.label_encoder:\n",
    "                X[self._categorical_cols] = X[self._categorical_cols].apply(lambda col: self.label_encoder.fit_transform(col))\n",
    "\n",
    "            if self.one_hot_encoder:\n",
    "                X_ohe = self.one_hot_encoder.transform(X[self._categorical_cols].values)\n",
    "                X_ohe = pd.DataFrame(X_ohe, X.index, columns=self.one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                X = X.join(\n",
    "                    pd.DataFrame(X_ohe, X.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols).drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self.scaler:\n",
    "            return pd.DataFrame(self.scaler.transform(X.values), index=X.index, columns=X.columns)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold tree parsing example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "def extr_pairs_forest(forest):\n",
    "    pp = []\n",
    "    for tree in forest:\n",
    "        pp += extr_pairs(tree)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.38114754098360654,\tin test: 0.3114754098360656\n",
      "ling in train: 0.1680327868852459,\tin test: 0.14754098360655737\n",
      "comp in train: 0.13114754098360656,\tin test: 0.22950819672131148\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_data\n",
    "\n",
    "train, test = split_data('data/', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_gold, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4544ca7ac6dd4ae5af8e7ed090920b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2.73 s, sys: 4 ms, total: 2.73 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import read_edus, read_gold, read_annotation\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(test):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                    annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Новость о том, что в России будут создавать фи...</td>\n",
       "      <td>Это совсем не смешно, друзья.</td>\n",
       "      <td>evaluation_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Новость о том, что в России будут создавать фи...</td>\n",
       "      <td>но, увы, ей не является.</td>\n",
       "      <td>contrast_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Новость о том, что в России будут создавать фи...</td>\n",
       "      <td>кажется шуткой,</td>\n",
       "      <td>same-unit_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Новость о том, что в России будут создавать фи...</td>\n",
       "      <td>( https:takiedela.runews20190722ik-pri-predpri...</td>\n",
       "      <td>attribution_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Использовать дешевый (почти бесплатный) труд з...</td>\n",
       "      <td>И эта мера, как обычно, лишь заплатка на очере...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Использовать дешевый (почти бесплатный) труд з...</td>\n",
       "      <td>это, вероятно последняя надежда для нашей экон...</td>\n",
       "      <td>evaluation_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>это, вероятно последняя надежда для нашей экон...</td>\n",
       "      <td>не способных выдерживать хоть сколько-нибудь с...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>И эта мера, как обычно, лишь заплатка на очере...</td>\n",
       "      <td>Налоги снижать нельзя, облегчать условия работ...</td>\n",
       "      <td>evidence_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>И эта мера, как обычно, лишь заплатка на очере...</td>\n",
       "      <td>призванной сдержать растерзанную экономику Рос...</td>\n",
       "      <td>purpose_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Налоги снижать нельзя, облегчать условия работ...</td>\n",
       "      <td>Заставить заключенных работать.</td>\n",
       "      <td>solutionhood_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Налоги снижать нельзя, облегчать условия работ...</td>\n",
       "      <td>значит что можно?</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Налоги снижать нельзя, облегчать условия работ...</td>\n",
       "      <td>снижать риски для предпринимателей нельзя,</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Налоги снижать нельзя,</td>\n",
       "      <td>облегчать условия работы нельзя,</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Мы же понимаем, что как бы это не подавалось в...</td>\n",
       "      <td>Мы же понимаем, что заключенные не будут получ...</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Мы же понимаем, что как бы это не подавалось в...</td>\n",
       "      <td>в реальности это будет не добровольная работа,...</td>\n",
       "      <td>contrast_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Вспомните, как в прошлом году начальника тюрьм...</td>\n",
       "      <td>за то, что у него на зоне процветал рабский тр...</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Вспомните, как в прошлом году начальника тюрьм...</td>\n",
       "      <td>отстранили от должности</td>\n",
       "      <td>same-unit_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Вспомните, как в прошлом году начальника тюрьмы</td>\n",
       "      <td>в которой сидела Nadуa Tolokonnikova​</td>\n",
       "      <td>background_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>за то, что у него на зоне процветал рабский труд.</td>\n",
       "      <td>Заключался он в том, что \"осужденные женщины з...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Заключался он в том, что \"осужденные женщины з...</td>\n",
       "      <td>(https:www.fontanka.ru20181224115).</td>\n",
       "      <td>attribution_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>То есть, понимаете, это уже есть.</td>\n",
       "      <td>И будет только хуже.</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>И дело даже не в том, что мы его строим заново,</td>\n",
       "      <td>не в том, что за политические высказывания и п...</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>не в том, что</td>\n",
       "      <td>за политические высказывания и политическую де...</td>\n",
       "      <td>same-unit_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>за политические высказывания и политическую де...</td>\n",
       "      <td>люди отправляются в тюрьму.</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Дело в том, что все молчат:</td>\n",
       "      <td>либо боятся, либо плевать хотели, думают, что ...</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>либо боятся, либо плевать хотели,</td>\n",
       "      <td>думают, что их это не касается.</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>либо боятся,</td>\n",
       "      <td>либо плевать хотели,</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Просто ответьте себе на вопрос - могут ли в се...</td>\n",
       "      <td>Конечно могут.</td>\n",
       "      <td>solutionhood_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Иногда.</td>\n",
       "      <td>Мы знаем единичные случаи, когда людей получае...</td>\n",
       "      <td>evidence_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Мы знаем единичные случаи, когда людей получае...</td>\n",
       "      <td>И всегда эти победы связаны исключительно с да...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>И всегда эти победы связаны исключительно с да...</td>\n",
       "      <td>потому, что на государственные институты вроде...</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>И всегда эти победы связаны</td>\n",
       "      <td>исключительно с давлением общества,</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>В итоге получается, что в любой момент любого ...</td>\n",
       "      <td>Поэтому самое страшное в этой новости именно т...</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>В итоге получается, что в любой момент любого ...</td>\n",
       "      <td>и вряд ли кто-либо будет за него бороться.</td>\n",
       "      <td>contrast_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Поэтому самое страшное в этой новости именно т...</td>\n",
       "      <td>Мы балансируем над пропастью, но между нами и ...</td>\n",
       "      <td>evaluation_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Мы балансируем над пропастью,</td>\n",
       "      <td>но между нами и обрывом нет ничего, ни одной п...</td>\n",
       "      <td>contrast_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Единственное, что может нам помочь - это сдела...</td>\n",
       "      <td>Что это за шаг?</td>\n",
       "      <td>preparation_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Единственное, что может нам помочь - это сдела...</td>\n",
       "      <td>Шаг к тому, чтобы сделать невозможным повторен...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Шаг к тому,</td>\n",
       "      <td>чтобы сделать невозможным повторение трагическ...</td>\n",
       "      <td>purpose_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>пока мы не почувствуем свою персональную ответ...</td>\n",
       "      <td>за то, что происходило, происходит и будет про...</td>\n",
       "      <td>cause_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>за то, что происходило, происходит</td>\n",
       "      <td>и будет происходить в нашей стране.</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>за то, что происходило,</td>\n",
       "      <td>происходит</td>\n",
       "      <td>relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Пока мы не поймем,</td>\n",
       "      <td>как мы участвуем, участвовали или будем участв...</td>\n",
       "      <td>elaboration_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>как мы участвуем, участвовали</td>\n",
       "      <td>или будем участвовать в этом процессе.</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>как мы участвуем,</td>\n",
       "      <td>участвовали</td>\n",
       "      <td>joint_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Пока участие не станет общественной нормой,</td>\n",
       "      <td>а безучастность не перестанет ей быть.</td>\n",
       "      <td>contrast_m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            snippet_x  \\\n",
       "0   Новость о том, что в России будут создавать фи...   \n",
       "1   Новость о том, что в России будут создавать фи...   \n",
       "2   Новость о том, что в России будут создавать фи...   \n",
       "3   Новость о том, что в России будут создавать фи...   \n",
       "4   Использовать дешевый (почти бесплатный) труд з...   \n",
       "5   Использовать дешевый (почти бесплатный) труд з...   \n",
       "6   это, вероятно последняя надежда для нашей экон...   \n",
       "7   И эта мера, как обычно, лишь заплатка на очере...   \n",
       "8   И эта мера, как обычно, лишь заплатка на очере...   \n",
       "9   Налоги снижать нельзя, облегчать условия работ...   \n",
       "10  Налоги снижать нельзя, облегчать условия работ...   \n",
       "11  Налоги снижать нельзя, облегчать условия работ...   \n",
       "12                             Налоги снижать нельзя,   \n",
       "13  Мы же понимаем, что как бы это не подавалось в...   \n",
       "14  Мы же понимаем, что как бы это не подавалось в...   \n",
       "15  Вспомните, как в прошлом году начальника тюрьм...   \n",
       "16  Вспомните, как в прошлом году начальника тюрьм...   \n",
       "17    Вспомните, как в прошлом году начальника тюрьмы   \n",
       "18  за то, что у него на зоне процветал рабский труд.   \n",
       "19  Заключался он в том, что \"осужденные женщины з...   \n",
       "20                  То есть, понимаете, это уже есть.   \n",
       "21    И дело даже не в том, что мы его строим заново,   \n",
       "22                                      не в том, что   \n",
       "23  за политические высказывания и политическую де...   \n",
       "24                        Дело в том, что все молчат:   \n",
       "25                  либо боятся, либо плевать хотели,   \n",
       "26                                       либо боятся,   \n",
       "27  Просто ответьте себе на вопрос - могут ли в се...   \n",
       "28                                            Иногда.   \n",
       "29  Мы знаем единичные случаи, когда людей получае...   \n",
       "30  И всегда эти победы связаны исключительно с да...   \n",
       "31                        И всегда эти победы связаны   \n",
       "32  В итоге получается, что в любой момент любого ...   \n",
       "33  В итоге получается, что в любой момент любого ...   \n",
       "34  Поэтому самое страшное в этой новости именно т...   \n",
       "35                      Мы балансируем над пропастью,   \n",
       "36  Единственное, что может нам помочь - это сдела...   \n",
       "37  Единственное, что может нам помочь - это сдела...   \n",
       "38                                        Шаг к тому,   \n",
       "39  пока мы не почувствуем свою персональную ответ...   \n",
       "40                 за то, что происходило, происходит   \n",
       "41                            за то, что происходило,   \n",
       "42                                 Пока мы не поймем,   \n",
       "43                      как мы участвуем, участвовали   \n",
       "44                                  как мы участвуем,   \n",
       "45        Пока участие не станет общественной нормой,   \n",
       "\n",
       "                                            snippet_y     category_id  \n",
       "0                       Это совсем не смешно, друзья.    evaluation_r  \n",
       "1                            но, увы, ей не является.      contrast_m  \n",
       "2                                     кажется шуткой,     same-unit_m  \n",
       "3   ( https:takiedela.runews20190722ik-pri-predpri...   attribution_r  \n",
       "4   И эта мера, как обычно, лишь заплатка на очере...   elaboration_r  \n",
       "5   это, вероятно последняя надежда для нашей экон...    evaluation_r  \n",
       "6   не способных выдерживать хоть сколько-нибудь с...   elaboration_r  \n",
       "7   Налоги снижать нельзя, облегчать условия работ...      evidence_r  \n",
       "8   призванной сдержать растерзанную экономику Рос...       purpose_r  \n",
       "9                     Заставить заключенных работать.  solutionhood_r  \n",
       "10                                  значит что можно?         cause_r  \n",
       "11         снижать риски для предпринимателей нельзя,         joint_m  \n",
       "12                   облегчать условия работы нельзя,         joint_m  \n",
       "13  Мы же понимаем, что заключенные не будут получ...         joint_m  \n",
       "14  в реальности это будет не добровольная работа,...      contrast_m  \n",
       "15  за то, что у него на зоне процветал рабский тр...         cause_r  \n",
       "16                            отстранили от должности     same-unit_m  \n",
       "17              в которой сидела Nadуa Tolokonnikova​    background_r  \n",
       "18  Заключался он в том, что \"осужденные женщины з...   elaboration_r  \n",
       "19                (https:www.fontanka.ru20181224115).   attribution_r  \n",
       "20                               И будет только хуже.         joint_m  \n",
       "21  не в том, что за политические высказывания и п...         joint_m  \n",
       "22  за политические высказывания и политическую де...     same-unit_m  \n",
       "23                        люди отправляются в тюрьму.         cause_r  \n",
       "24  либо боятся, либо плевать хотели, думают, что ...         cause_r  \n",
       "25                    думают, что их это не касается.         joint_m  \n",
       "26                               либо плевать хотели,         joint_m  \n",
       "27                                     Конечно могут.  solutionhood_r  \n",
       "28  Мы знаем единичные случаи, когда людей получае...      evidence_r  \n",
       "29  И всегда эти победы связаны исключительно с да...   elaboration_r  \n",
       "30  потому, что на государственные институты вроде...         cause_r  \n",
       "31                исключительно с давлением общества,         cause_r  \n",
       "32  Поэтому самое страшное в этой новости именно т...         cause_r  \n",
       "33         и вряд ли кто-либо будет за него бороться.      contrast_m  \n",
       "34  Мы балансируем над пропастью, но между нами и ...    evaluation_r  \n",
       "35  но между нами и обрывом нет ничего, ни одной п...      contrast_m  \n",
       "36                                    Что это за шаг?   preparation_r  \n",
       "37  Шаг к тому, чтобы сделать невозможным повторен...   elaboration_r  \n",
       "38  чтобы сделать невозможным повторение трагическ...       purpose_r  \n",
       "39  за то, что происходило, происходит и будет про...         cause_r  \n",
       "40                и будет происходить в нашей стране.         joint_m  \n",
       "41                                         происходит        relation  \n",
       "42  как мы участвуем, участвовали или будем участв...   elaboration_r  \n",
       "43             или будем участвовать в этом процессе.         joint_m  \n",
       "44                                        участвовали         joint_m  \n",
       "45             а безучастность не перестанет ей быть.      contrast_m  "
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval\n",
    "\n",
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/sci.ling_10</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/blogs_80</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/blogs_60</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/news1_40</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/blogs_90</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  true_pos  all_parsed  all_gold\n",
       "0  data/sci.ling_10        61          61        76\n",
       "1     data/blogs_80       126         126       174\n",
       "2     data/blogs_60        43          43        48\n",
       "3     data/news1_40        21          21        26\n",
       "4     data/blogs_90        46          46        66"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#filename = 'rst_pairs/news_55'\n",
    "#filename = 'rst_pairs/news_13'\n",
    "filename = 'data/sci.comp_26'\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "annot = read_annotation(filename)\n",
    "\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Gold tree construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.696969696969697\n",
      "Precision: 1.0\n",
      "F1: 0.8214285714285715\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>66</td>\n",
       "      <td>0.69697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_pos  all_parsed  all_gold   recall  precision        f1\n",
       "0        46          46        66  0.69697        1.0  0.821429"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "fnames = []\n",
    "\n",
    "c_true_pos, c_all_parsed, c_all_gold = metric_parseval(parsed_pairs, gold)\n",
    "true_pos.append(c_true_pos)\n",
    "all_parsed.append(c_all_parsed)\n",
    "all_gold.append(c_all_gold)\n",
    "\n",
    "recall = sum(true_pos) / sum(all_gold)\n",
    "print('Recall: ', recall)\n",
    "\n",
    "precision = sum(true_pos) / sum(all_parsed)\n",
    "print('Precision:', precision)\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print('F1:', f1)\n",
    "    \n",
    "aa = pd.DataFrame({'true_pos': true_pos, 'all_parsed': all_parsed, 'all_gold': all_gold})\n",
    "aa['recall'] = aa.true_pos / aa.all_gold\n",
    "aa['precision'] = aa.true_pos / aa.all_parsed\n",
    "aa['f1'] = aa.recall * aa.precision * 2 / (aa.precision + aa.recall)\n",
    "\n",
    "aa.sort_values('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "binary_classifier_model_path = 'models/structure_predictor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 s, sys: 394 ms, total: 36.8 s\n",
      "Wall time: 36.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "scaler = pickle.load(open(os.path.join(binary_classifier_model_path, 'scaler.pkl'), 'rb'))\n",
    "#categorical_cols = pickle.load(open(binary_classifier_model_path + 'categorical_cols.pkl', 'rb'))\n",
    "#ohe = pickle.load(open(binary_classifier_model_path + 'one_hot_encoder.pkl', 'rb'))\n",
    "#le = pickle.load(open(binary_classifier_model_path + 'label_encoder.pkl', 'rb'))\n",
    "binary_classifier = pickle.load(open(os.path.join(binary_classifier_model_path, 'model.pkl'), 'rb'))\n",
    "features_extractor = FeaturesExtractor(features_processor, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = CustomTreePredictor(features_extractor, binary_classifier, label_predictor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(predictor, forest_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eead6c2d38d847daa23baa6983c2ab8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-891-f1a6e602de6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mannot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mannot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'syntax_dep_tree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 genre=filename.split('_')[0])\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-886-c9483b5a6f67>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, edus, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree, genre)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                  \u001b[0mannot_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                  \u001b[0mannot_postag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_morph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_lemma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                  annot_syntax_dep_tree)\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;31m#_scores = [self.tree_predictor.predict_pair_proba([_feature]) for _feature in _features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0m_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_pair_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-885-b336e9f5711e>\u001b[0m in \u001b[0;36minitialize_features\u001b[0;34m(self, nodes, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                                \u001b[0mannot_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannot_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannot_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                                \u001b[0mannot_postag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannot_postag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_morph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannot_morph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                                annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-887-678c58b7300d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_postag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_morph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_syntax_dep_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_postag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_morph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_syntax_dep_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDROP_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/isanlp_rst/src/maintenance/utils/features_processor_default.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, df_, annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag, annot_syntax_dep_tree)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loc_x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnippet_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannot_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'loc_y'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loc_y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnippet_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnippet_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_begin_x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3450\u001b[0m         \u001b[0;31m# value exception to occur first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3452\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[0;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[1;32m   3243\u001b[0m             \u001b[0;31m# the copy weakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3247\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import extr_pairs_forest\n",
    "\n",
    "cache = {}\n",
    "broken_files = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus) - 1):\n",
    "        start = annot['text'].find(edus[max_id], last_end)\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "        \n",
    "    parsed = parser(_edus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['postag'], \n",
    "                annot['morph'], \n",
    "                annot['lemma'], \n",
    "                annot['syntax_dep_tree'], \n",
    "                genre=filename.split('_')[0])\n",
    "\n",
    "#     try:\n",
    "#         parsed = parser(_edus, \n",
    "#                         annot['text'], \n",
    "#                         annot['tokens'], \n",
    "#                         annot['sentences'], \n",
    "#                         annot['postag'], \n",
    "#                         annot['morph'], \n",
    "#                         annot['lemma'], \n",
    "#                         annot['syntax_dep_tree'], \n",
    "#                         genre=filename.split('_')[0])\n",
    "#     except:\n",
    "#         broken_files.append(filename)\n",
    "#         continue\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval\n",
    "\n",
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_golds(filename):\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "    annot = read_annotation(filename)\n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = annot['text'].find(edus[max_id], last_end)\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "    \n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    #parsed = parser(_edus)\n",
    "    \n",
    "    try:\n",
    "        parsed = parser(_edus, \n",
    "                        annot['text'], \n",
    "                        annot['tokens'], \n",
    "                        annot['sentences'], \n",
    "                        annot['postag'], \n",
    "                        annot['morph'], \n",
    "                        annot['lemma'], \n",
    "                        annot['syntax_dep_tree'], \n",
    "                        genre=filename.split('_')[0])\n",
    "    except:\n",
    "        broken_files.append(filename)\n",
    "        continue\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    return (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(glob.glob('data/*.edus')):\n",
    "    filename = file.replace('.edus', '')\n",
    "    cache[filename] = parse_golds(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('comp')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('ling')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('news')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('blogs')].F1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad file analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/news2_17'\n",
    "\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "annot = read_annotation(filename)\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = annot['text'].find(edus[max_id], last_end)\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            #text=edus[max_id],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end\n",
    "\n",
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "#parsed = parser(_edus)\n",
    "\n",
    "parsed = parser(_edus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['postag'], \n",
    "                annot['morph'], \n",
    "                annot['lemma'], \n",
    "                annot['syntax_dep_tree'], \n",
    "                genre=filename.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in _edus:\n",
    "    print(vars(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for tree in parsed:\n",
    "    if tree.relation != 'edu':\n",
    "        print(vars(tree))\n",
    "        counter += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parsed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval, extr_pairs, extr_pairs_forest, _check_snippet_pair_in_dataset, _not_parsed_as_in_gold\n",
    "\n",
    "parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "print(parsed_pairs.shape, gold.shape)\n",
    "errors = _not_parsed_as_in_gold(parsed_pairs, gold)\n",
    "\n",
    "def find_edu_number(edus, error):\n",
    "    for i, edu in enumerate(edus):\n",
    "        if error[2].find(edu) > -1:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors.iloc[3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(find_edu_number(edus, errors.iloc[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
