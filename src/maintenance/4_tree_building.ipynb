{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            self.start = left.start\n",
    "            self.end = right.end+1\n",
    "        \n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(tree):\n",
    "    def _(n):\n",
    "        if n.relation:\n",
    "            value = (n.relation, \"%.2f\"%(n.proba))\n",
    "        else:\n",
    "            value = n.text\n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    return printBTree(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor):\n",
    "        self.features_processor = features_processor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "        self.genre = None\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        return self.label_predictor.predict(features)\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, corpus):\n",
    "        RSTTreePredictor.__init__(self, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return [args[0].text, args[1].text]\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return ((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                    or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                        axis=0) != 0)\n",
    "\n",
    "        left_snippet, right_snippet = features\n",
    "        return float(_check_snippet_pair_in_dataset(left_snippet, right_snippet))\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        label = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].category_id.values\n",
    "        if label.size == 0:\n",
    "            return 'relation'\n",
    "        \n",
    "        return label[0]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        nuclearity = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].order.values\n",
    "        if nuclearity.size == 0:\n",
    "            return '_'\n",
    "        \n",
    "        return nuclearity[0]\n",
    "\n",
    "\n",
    "class CustomTreePredictor(RSTTreePredictor):\n",
    "    \"\"\"\n",
    "    Contains trained classifiers and feature processors needed for tree prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor=None):\n",
    "        RSTTreePredictor.__init__(self, features_processor, relation_predictor, label_predictor)\n",
    "\n",
    "    def extract_features(self, left_node: DiscourseUnit, right_node: DiscourseUnit,\n",
    "                         annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                         annot_syntax_dep_tree):\n",
    "        pair = pd.DataFrame({\n",
    "            'snippet_x': [left_node.text.strip()],\n",
    "            'snippet_y': [right_node.text.strip()],\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_processor(pair, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        return self.relation_predictor.predict_proba(features)[0][1]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        # ToDO:\n",
    "        return 'unavail'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "\n",
    "class GreedyRSTParser:\n",
    "    def __init__(self, tree_predictor, forest_threshold=0.05):\n",
    "        \"\"\"\n",
    "        :param RSTTreePredictor tree_predictor:\n",
    "        :param float forest_threshold: minimum relation probability to append the pair into the tree\n",
    "        \"\"\"\n",
    "        self.tree_predictor = tree_predictor\n",
    "        self.forest_threshold = forest_threshold\n",
    "\n",
    "    def __call__(self, edus, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                 annot_syntax_dep_tree, genre=None):\n",
    "        \"\"\"\n",
    "        :param list edus: DiscourseUnit\n",
    "        :param str annot_text: original text\n",
    "        :param list annot_tokens: isanlp.annotation.Token\n",
    "        :param list annot_sentences: isanlp.annotation.Sentence\n",
    "        :param list annot_postag: lists of str for each sentence\n",
    "        :param annot_lemma: lists of str for each sentence\n",
    "        :param annot_syntax_dep_tree: list of isanlp.annotation.WordSynt for each sentence\n",
    "        :return: list of DiscourseUnit containing each extracted tree\n",
    "        \"\"\"\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        self.tree_predictor.genre = genre\n",
    "\n",
    "        nodes = edus\n",
    "        \n",
    "#         for edu in nodes:\n",
    "#             print(edu, file=sys.stderr)\n",
    "        \n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = [\n",
    "            self.tree_predictor.extract_features(nodes[i], nodes[i + 1], annot_text, annot_tokens,\n",
    "                                                 annot_sentences,\n",
    "                                                 annot_postag, annot_morph, annot_lemma,\n",
    "                                                 annot_syntax_dep_tree)\n",
    "            for i in range(len(nodes) - 1)]\n",
    "\n",
    "        scores = [self.tree_predictor.predict_pair_proba(features[i]) for i in range(len(nodes) - 1)]\n",
    "        relations = [self.tree_predictor.predict_label(features[i]) for i in range(len(nodes) - 1)]\n",
    "        nuclearities = [self.tree_predictor.predict_nuclearity(features[i]) for i in range(len(nodes) - 1)]\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            \n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features[j]),\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                #orig_text=annot_text\n",
    "                #text=nodes[j].text + nodes[j + 1].text  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "            \n",
    "#             print(temp, file=sys.stderr)\n",
    "            \n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                annot_text, annot_tokens, \n",
    "                                       \n",
    "                                                                      annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = [predicted] + scores[j + 2:]\n",
    "                features = [features_right] + features[j + 2:]\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j], \n",
    "                                                                     annot_text, annot_tokens,\n",
    "                                                                     annot_sentences, annot_postag, \n",
    "                                                                     annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_left = self.tree_predictor.predict_pair_proba(features_left)\n",
    "\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1], \n",
    "                                                                      annot_text, annot_tokens,\n",
    "                                                                      annot_sentences, annot_postag, \n",
    "                                                                      annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_right = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = scores[:j - 1] + [predicted_left] + [predicted_right] + scores[j + 2:]\n",
    "                features = features[:j - 1] + [features_left] + [features_right] + features[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                annot_text, annot_tokens, \n",
    "                                                                annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                scores = scores[:j - 1] + [predicted]\n",
    "                features = features[:j - 1] + [features_left]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_embeddings(embedder, X, maxlen=100):\n",
    "    X_ = [text[:text.rfind('_')] for text in X.split()]\n",
    "    result = np.zeros((embedder.vector_size, maxlen))\n",
    "\n",
    "    for i in range(min(len(X_), maxlen)):\n",
    "        try:\n",
    "            result[i] = embedder[X_[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    DROP_COLUMNS = ['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y']\n",
    "\n",
    "    def __init__(self, processor, scaler=None, categorical_cols=None, one_hot_encoder=None, label_encoder=None):\n",
    "        self.processor = processor\n",
    "        self.scaler = scaler\n",
    "        self._categorical_cols = categorical_cols\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __call__(self, df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree):\n",
    "        X = self.processor(df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "        X = X.drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self._categorical_cols:\n",
    "            if self.label_encoder:\n",
    "                X[self._categorical_cols] = X[self._categorical_cols].apply(lambda col: self.label_encoder.fit_transform(col))\n",
    "\n",
    "            if self.one_hot_encoder:\n",
    "                X_ohe = self.one_hot_encoder.transform(X[self._categorical_cols].values)\n",
    "                X_ohe = pd.DataFrame(X_ohe, X.index, columns=self.one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                X = X.join(\n",
    "                    pd.DataFrame(X_ohe, X.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols).drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self.scaler:\n",
    "            return pd.DataFrame(self.scaler.transform(X.values), index=X.index, columns=X.columns)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold tree parsing example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "def extr_pairs_forest(forest):\n",
    "    pp = []\n",
    "    for tree in forest:\n",
    "        pp += extr_pairs(tree)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_data\n",
    "\n",
    "train, test = split_data('data/', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_gold, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import read_edus, read_gold, read_annotation\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(test[:3]):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                    annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#filename = 'rst_pairs/news_55'\n",
    "#filename = 'rst_pairs/news_13'\n",
    "filename = 'data/sci.comp_26'\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "annot = read_annotation(filename)\n",
    "\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Gold tree construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "fnames = []\n",
    "\n",
    "c_true_pos, c_all_parsed, c_all_gold = metric_parseval(parsed_pairs, gold)\n",
    "true_pos.append(c_true_pos)\n",
    "all_parsed.append(c_all_parsed)\n",
    "all_gold.append(c_all_gold)\n",
    "\n",
    "recall = sum(true_pos) / sum(all_gold)\n",
    "print('Recall: ', recall)\n",
    "\n",
    "precision = sum(true_pos) / sum(all_parsed)\n",
    "print('Precision:', precision)\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print('F1:', f1)\n",
    "    \n",
    "aa = pd.DataFrame({'true_pos': true_pos, 'all_parsed': all_parsed, 'all_gold': all_gold})\n",
    "aa['recall'] = aa.true_pos / aa.all_gold\n",
    "aa['precision'] = aa.true_pos / aa.all_parsed\n",
    "aa['f1'] = aa.recall * aa.precision * 2 / (aa.precision + aa.recall)\n",
    "\n",
    "aa.sort_values('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "binary_classifier_model_path = 'models/structure_predictor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "scaler = pickle.load(open(os.path.join(binary_classifier_model_path, 'scaler.pkl'), 'rb'))\n",
    "#categorical_cols = pickle.load(open(binary_classifier_model_path + 'categorical_cols.pkl', 'rb'))\n",
    "#ohe = pickle.load(open(binary_classifier_model_path + 'one_hot_encoder.pkl', 'rb'))\n",
    "#le = pickle.load(open(binary_classifier_model_path + 'label_encoder.pkl', 'rb'))\n",
    "binary_classifier = pickle.load(open(os.path.join(binary_classifier_model_path, 'model.pkl'), 'rb'))\n",
    "features_extractor = FeaturesExtractor(features_processor, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = CustomTreePredictor(features_extractor, binary_classifier, label_predictor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(predictor, forest_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import extr_pairs_forest\n",
    "\n",
    "cache = {}\n",
    "broken_files = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus) - 1):\n",
    "        start = annot['text'].find(edus[max_id], last_end)\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    try:\n",
    "        parsed = parser(_edus, \n",
    "                        annot['text'], \n",
    "                        annot['tokens'], \n",
    "                        annot['sentences'], \n",
    "                        annot['postag'], \n",
    "                        annot['morph'], \n",
    "                        annot['lemma'], \n",
    "                        annot['syntax_dep_tree'], \n",
    "                        genre=filename.split('_')[0])\n",
    "    except:\n",
    "        broken_files.append(filename)\n",
    "        continue\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval\n",
    "\n",
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_golds(file):\n",
    "    filename = file.replace('.edus', '')\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "    annot = read_annotation(filename)\n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = annot['text'].find(edus[max_id], last_end)\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "    \n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    #parsed = parser(_edus)\n",
    "    \n",
    "    try:\n",
    "        parsed = parser(_edus, \n",
    "                        annot['text'], \n",
    "                        annot['tokens'], \n",
    "                        annot['sentences'], \n",
    "                        annot['postag'], \n",
    "                        annot['morph'], \n",
    "                        annot['lemma'], \n",
    "                        annot['syntax_dep_tree'], \n",
    "                        genre=filename.split('_')[0])\n",
    "    except:\n",
    "        broken_files.append(filename)\n",
    "        continue\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    return (filename, parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(glob.glob('data/*.edus')):\n",
    "    filename = file.replace('.edus', '')\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "    annot = read_annotation(filename)\n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = annot['text'].find(edus[max_id], last_end)\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "    \n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    #parsed = parser(_edus)\n",
    "    \n",
    "    try:\n",
    "        parsed = parser(_edus, \n",
    "                        annot['text'], \n",
    "                        annot['tokens'], \n",
    "                        annot['sentences'], \n",
    "                        annot['postag'], \n",
    "                        annot['morph'], \n",
    "                        annot['lemma'], \n",
    "                        annot['syntax_dep_tree'], \n",
    "                        genre=filename.split('_')[0])\n",
    "    except:\n",
    "        broken_files.append(filename)\n",
    "        continue\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)\n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('comp')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('ling')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('news')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('blogs')].F1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad file analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/news2_17'\n",
    "\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "annot = read_annotation(filename)\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = annot['text'].find(edus[max_id], last_end)\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            #text=edus[max_id],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end\n",
    "\n",
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "#parsed = parser(_edus)\n",
    "\n",
    "parsed = parser(_edus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['postag'], \n",
    "                annot['morph'], \n",
    "                annot['lemma'], \n",
    "                annot['syntax_dep_tree'], \n",
    "                genre=filename.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in _edus:\n",
    "    print(vars(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for tree in parsed:\n",
    "    if tree.relation != 'edu':\n",
    "        print(vars(tree))\n",
    "        counter += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parsed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval, extr_pairs, extr_pairs_forest, _check_snippet_pair_in_dataset, _not_parsed_as_in_gold\n",
    "\n",
    "parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "print(parsed_pairs.shape, gold.shape)\n",
    "errors = _not_parsed_as_in_gold(parsed_pairs, gold)\n",
    "\n",
    "def find_edu_number(edus, error):\n",
    "    for i, edu in enumerate(edus):\n",
    "        if error[2].find(edu) > -1:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors.iloc[3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(find_edu_number(edus, errors.iloc[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
