{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            #gap_counter = 0\n",
    "            #while len(left.text + right.text) < len(self.text):\n",
    "            #    self.text = left.text + ' ' * gap_counter + right.text\n",
    "            #    gap_counter += 1\n",
    "            self.start = left.start\n",
    "            self.end = right.end\n",
    "        \n",
    "        \"\"\"\n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "        \"\"\"\n",
    "        if self.left:\n",
    "            self.text = ' '.join([self.left.text, self.right.text])\n",
    "        else:\n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(tree):\n",
    "    def _(n):\n",
    "        if n.relation:\n",
    "            value = (n.relation, \"%.2f\"%(n.proba))\n",
    "        else:\n",
    "            value = n.text\n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    return printBTree(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    def __init__(self, features_extractor=None, relation_predictor=None, label_predictor=None):\n",
    "        self.features_extractor = features_extractor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "        self.genre = None\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        return self.label_predictor.predict(features)\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, corpus):\n",
    "        RSTTreePredictor.__init__(self, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return [args[0].text, args[1].text]\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return ((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                    or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                        axis=0) != 0)\n",
    "\n",
    "        left_snippet, right_snippet = features\n",
    "        return float(_check_snippet_pair_in_dataset(left_snippet, right_snippet))\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        #print('features:', features)\n",
    "        left_snippet, right_snippet = features\n",
    "        in_left = self.corpus[self.corpus.snippet_x == left_snippet]\n",
    "        return in_left[in_left.snippet_y == right_snippet].category_id.values\n",
    "        \n",
    "        #if not self.label_predictor:\n",
    "        #    return 'relation'\n",
    "\n",
    "\n",
    "class CustomTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, features_extractor, relation_predictor, label_predictor=None):\n",
    "        RSTTreePredictor.__init__(self, features_extractor, relation_predictor, label_predictor)\n",
    "\n",
    "    def extract_features(self, left_node: DiscourseUnit, right_node: DiscourseUnit,\n",
    "                         annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                         annot_syntax_dep_tree):\n",
    "        pair = pd.DataFrame({\n",
    "            'snippet_x': [left_node.text.strip()],\n",
    "            'snippet_y': [right_node.text.strip()],\n",
    "            #'genre': self.genre\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_extractor(pair, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        return self.relation_predictor.predict_proba(features)[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "\n",
    "class GreedyRSTParser:\n",
    "    def __init__(self, tree_predictor, forest_threshold=0.05):\n",
    "        \"\"\"\n",
    "        :param RSTTreePredictor tree_predictor:\n",
    "        :param float forest_threshold: minimum relation probability to append the pair into the tree\n",
    "        \"\"\"\n",
    "        self.tree_predictor = tree_predictor\n",
    "        self.forest_threshold = forest_threshold\n",
    "\n",
    "    def __call__(self, edus, annot_text=None, annot_tokens=None, annot_sentences=None, annot_postag=None, \n",
    "                 annot_morph=None, annot_lemma=None,\n",
    "                 annot_syntax_dep_tree=None, genre=None):\n",
    "        \"\"\"\n",
    "        :param list edus: DiscourseUnit\n",
    "        :param str annot_text: original text\n",
    "        :param list annot_tokens: isanlp.annotation.Token\n",
    "        :param list annot_sentences: isanlp.annotation.Sentence\n",
    "        :param list annot_postag: lists of str for each sentence\n",
    "        :param annot_lemma: lists of str for each sentence\n",
    "        :param annot_syntax_dep_tree: list of isanlp.annotation.WordSynt for each sentence\n",
    "        :return: list of DiscourseUnit containing each extracted tree\n",
    "        \"\"\"\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        self.tree_predictor.genre = genre\n",
    "\n",
    "        nodes = edus\n",
    "        \n",
    "        for edu in nodes:\n",
    "            print(edu, file=sys.stderr)\n",
    "        \n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        # ToDO: pass these pairs in the extractor at once, then just take the values of each row\n",
    "        # maybe also parallelize a couple of feature extractors\n",
    "        features = [\n",
    "            self.tree_predictor.extract_features(nodes[i], nodes[i + 1], annot_text, annot_tokens,\n",
    "                                                 annot_sentences,\n",
    "                                                 annot_postag, annot_morph, annot_lemma,\n",
    "                                                 annot_syntax_dep_tree)\n",
    "            for i in range(len(nodes) - 1)]\n",
    "\n",
    "        scores = [self.tree_predictor.predict_pair_proba(features[i]) for i in range(len(nodes) - 1)]\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            relation = self.tree_predictor.predict_label(features[j])\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                orig_text=annot_text,\n",
    "                relation=relation,\n",
    "                proba=scores[j],\n",
    "                #text=nodes[j].text + nodes[j + 1].text  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "            \n",
    "            print(temp, file=sys.stderr)\n",
    "            \n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                annot_text, annot_tokens, annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features)\n",
    "\n",
    "                scores = [predicted] + scores[j + 2:]\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j], annot_text, annot_tokens,\n",
    "                                                                     annot_sentences, annot_postag, annot_morph,\n",
    "                                                                     annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_left = self.tree_predictor.predict_pair_proba(features_left)\n",
    "\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1], annot_text, annot_tokens,\n",
    "                                                                      annot_sentences, annot_postag, annot_morph,\n",
    "                                                                      annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_right = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = scores[:j - 1] + [predicted_left] + [predicted_right] + scores[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                annot_text, annot_tokens, annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features)\n",
    "                scores = scores[:j - 1] + [predicted]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                orig_text=annot_text,\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_embeddings(embedder, X, maxlen=100):\n",
    "    X_ = [text[:text.rfind('_')] for text in X.split()]\n",
    "    result = np.zeros((embedder.vector_size, maxlen))\n",
    "\n",
    "    for i in range(min(len(X_), maxlen)):\n",
    "        try:\n",
    "            result[i] = embedder[X_[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    DROP_COLUMNS = ['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y']\n",
    "\n",
    "    def __init__(self, processor, scaler=None, categorical_cols=None, one_hot_encoder=None, label_encoder=None):\n",
    "        self.processor = processor\n",
    "        self.scaler = scaler\n",
    "        self._categorical_cols = categorical_cols\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __call__(self, df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree):\n",
    "        X = self.processor(df, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "        X = X.drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self._categorical_cols:\n",
    "            if self.label_encoder:\n",
    "                X[self._categorical_cols] = X[self._categorical_cols].apply(lambda col: self.label_encoder.fit_transform(col))\n",
    "\n",
    "            if self.one_hot_encoder:\n",
    "                X_ohe = self.one_hot_encoder.transform(X[self._categorical_cols].values)\n",
    "                X_ohe = pd.DataFrame(X_ohe, X.index, columns=self.one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                X = X.join(\n",
    "                    pd.DataFrame(X_ohe, X.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols).drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self.scaler:\n",
    "            return pd.DataFrame(self.scaler.transform(X.values), index=X.index, columns=X.columns)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold tree parsing example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.nlp')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "files = sorted(glob.glob('rst_pairs/*.edus'), key=lambda s: int(os.path.basename(s)[5]))\n",
    "test = files[::5]\n",
    "train = [file for file in files if not file in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "def extr_pairs_forest(forest):\n",
    "    pp = []\n",
    "    for tree in forest:\n",
    "        pp += extr_pairs(tree)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(test[:3]):\n",
    "    filename = ''.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus) - 1):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    parsed = parser(_edus)\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#filename = 'rst_pairs/news_55'\n",
    "#filename = 'rst_pairs/news_13'\n",
    "filename = 'rst_pairs/comp_26'\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "annot = read_annotation(filename)\n",
    "\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed = parser(_edus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_parseval(parsed_pairs, gold):\n",
    "    parsed_strings = []\n",
    "    for i in parsed_pairs.index:\n",
    "        parsed_strings.append(parsed_pairs.loc[i, 'snippet_x'] + ' ' + parsed_pairs.loc[i, 'snippet_y'])\n",
    "    parsed_strings = set(parsed_strings)\n",
    "    \n",
    "    gold_strings_1 = []\n",
    "    for i in gold.index:\n",
    "        gold_strings_1.append(gold.loc[i, 'snippet_x'] + ' ' + gold.loc[i, 'snippet_y'])\n",
    "    gold_strings_1 = set(gold_strings_1)\n",
    "    \n",
    "    gold_strings_2 = []\n",
    "    for i in gold.index:\n",
    "        gold_strings_2.append(gold.loc[i, 'snippet_y'] + ' ' + gold.loc[i, 'snippet_x'])\n",
    "    gold_strings_2 = set(gold_strings_2)\n",
    "    \n",
    "    true_pos = len(gold_strings_1 & parsed_strings) + len(gold_strings_2 & parsed_strings)\n",
    "    all_parsed = len(parsed_strings)\n",
    "    all_gold = len(gold_strings_1)\n",
    "    \n",
    "    return true_pos, all_parsed, all_gold\n",
    "    \n",
    "\n",
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "def extr_pairs_forest(forest):\n",
    "    pp = []\n",
    "    for tree in forest:\n",
    "        pp += extr_pairs(tree)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "    left_snippet = left_snippet.strip()\n",
    "    right_snippet = right_snippet.strip()\n",
    "    return ((((gold.snippet_x == left_snippet) & (gold.snippet_y == right_snippet)).sum(axis=0) != 0) \n",
    "            or ((gold.snippet_y == left_snippet) & (gold.snippet_x == right_snippet)).sum(axis=0) != 0)\n",
    "\n",
    "def _not_parsed_as_in_gold(parsed_pairs: pd.DataFrame, gold: pd.DataFrame):\n",
    "    tmp = pd.merge(gold, parsed_pairs, on=['snippet_x', 'snippet_y'], how='left', suffixes=('_gold', '_parsed'))\n",
    "    return tmp[pd.isnull(tmp.category_id_parsed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pairs.shape, gold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = _not_parsed_as_in_gold(parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edu_number(edus, error):\n",
    "    for i, edu in enumerate(edus):\n",
    "        if error[2].find(edu) > -1:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "fnames = []\n",
    "\n",
    "c_true_pos, c_all_parsed, c_all_gold = metric_parseval(parsed_pairs, gold)\n",
    "true_pos.append(c_true_pos)\n",
    "all_parsed.append(c_all_parsed)\n",
    "all_gold.append(c_all_gold)\n",
    "\n",
    "recall = sum(true_pos) / sum(all_gold)\n",
    "print('Recall: ', recall)\n",
    "\n",
    "precision = sum(true_pos) / sum(all_parsed)\n",
    "print('Precision:', precision)\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print('F1:', f1)\n",
    "    \n",
    "aa = pd.DataFrame({'true_pos': true_pos, 'all_parsed': all_parsed, 'all_gold': all_gold})\n",
    "aa['recall'] = aa.true_pos / aa.all_gold\n",
    "aa['precision'] = aa.true_pos / aa.all_parsed\n",
    "aa['f1'] = aa.recall * aa.precision * 2 / (aa.precision + aa.recall)\n",
    "\n",
    "aa.sort_values('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor\n",
    "\n",
    "binary_classifier_model_path = 'structure_predictor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "scaler = pickle.load(open(binary_classifier_model_path + 'scaler.pkl', 'rb'))\n",
    "#categorical_cols = pickle.load(open(binary_classifier_model_path + 'categorical_cols.pkl', 'rb'))\n",
    "#ohe = pickle.load(open(binary_classifier_model_path + 'one_hot_encoder.pkl', 'rb'))\n",
    "#le = pickle.load(open(binary_classifier_model_path + 'label_encoder.pkl', 'rb'))\n",
    "binary_classifier = pickle.load(open(binary_classifier_model_path + 'model.pkl', 'rb'))\n",
    "\n",
    "features_extractor = FeaturesExtractor(features_processor, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = CustomTreePredictor(features_extractor, binary_classifier, label_predictor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GreedyRSTParser(predictor, forest_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(test[:1]):\n",
    "    filename = ''.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus) - 1):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parsed = parser(_edus, \n",
    "                    annot['text'], \n",
    "                    annot['tokens'], \n",
    "                    annot['sentences'], \n",
    "                    annot['postag'], \n",
    "                    annot['morph'], \n",
    "                    annot['lemma'], \n",
    "                    annot['syntax_dep_tree'], \n",
    "                    genre=filename.split('_')[0])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cache = {}\n",
    "for file in tqdm(glob.glob('rst_pairs/*.edus')):\n",
    "    filename = file.replace('.edus', '')\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "    annot = read_annotation(filename)\n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "                #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "    \n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    parsed = parser(_edus)\n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('comp')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('ling')].F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['filename'].str.contains('news')].F1.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
