{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 1. Negative samples generation\n",
    "\n",
    "Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``data_structure/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "from utils.file_reading import *\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_rst.src.isanlp_rst.rst_tree_predictor import RSTTreePredictor, GoldTreePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text[row['loc_x']+len(row.snippet_x):].find(row.snippet_y)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "    \n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "        \n",
    "        negative_nodes = []\n",
    "        \n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus        \n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes)\n",
    "        scores = list(map(self.tree_predictor.predict_pair_proba, features))\n",
    "        relations = list(map(self.tree_predictor.predict_label, features))\n",
    "        nuclearities = list(map(self.tree_predictor.predict_nuclearity, features))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            \n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features[j]),\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "            \n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1])\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = [predicted] + scores[j + 2:]\n",
    "                features = [features_right] + features[j + 2:]\n",
    "                \n",
    "                if predicted == 0:\n",
    "                    relation = self.tree_predictor.predict_label(features_right)\n",
    "                    if relation == 'relation':\n",
    "                        negative_nodes.append(\n",
    "                            DiscourseUnit(\n",
    "                                id=None,\n",
    "                                left=nodes[j],\n",
    "                                right=nodes[j + 1],\n",
    "                                relation=relation,\n",
    "                                nuclearity=self.tree_predictor.predict_nuclearity(features_right),\n",
    "                                proba=predicted,\n",
    "                                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j])\n",
    "                predicted_left = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                if predicted_left == 0:\n",
    "                    relation = self.tree_predictor.predict_label(features_left)\n",
    "                    if relation == 'relation':\n",
    "                        negative_nodes.append(\n",
    "                            DiscourseUnit(\n",
    "                                id=None,\n",
    "                                left=nodes[j - 1],\n",
    "                                right=nodes[j],\n",
    "                                relation=relation,\n",
    "                                nuclearity=self.tree_predictor.predict_nuclearity(features_left),\n",
    "                                proba=predicted_left,\n",
    "                                text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1])\n",
    "                predicted_right = self.tree_predictor.predict_pair_proba(features_right)\n",
    "                if predicted_right == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation=self.tree_predictor.predict_label(features_right),\n",
    "                            nuclearity=self.tree_predictor.predict_nuclearity(features_right),\n",
    "                            proba=predicted_right,\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "                scores = scores[:j - 1] + [predicted_left] + [predicted_right] + scores[j + 2:]\n",
    "                features = features[:j - 1] + [features_left] + [features_right] + features[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j])\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                if predicted == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation=self.tree_predictor.predict_label(features_left),\n",
    "                            nuclearity=self.tree_predictor.predict_nuclearity(features_left),\n",
    "                            proba=predicted,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                    ))\n",
    "                    \n",
    "                scores = scores[:j - 1] + [predicted]\n",
    "                features = features[:j - 1] + [features_left]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return negative_nodes\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RandomNegativeGenerator()\n",
    "#gen = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.json')):\n",
    "    filename = filename.replace('.json', '')\n",
    "    df = read_gold(filename, features=True)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    if gen.__name__() == 'RandomNegativeGenerator':\n",
    "        tmp = gen(edus, df, annot['text'])\n",
    "    \n",
    "    elif gen.__name__() == 'GreedyNegativeGenerator':\n",
    "        _edus = []\n",
    "        last_end = 0\n",
    "        for max_id in range(len(edus)):\n",
    "            start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "            end = start + len(edus[max_id])\n",
    "            temp = DiscourseUnit(\n",
    "                    id=max_id,\n",
    "                    left=None,\n",
    "                    right=None,\n",
    "                    relation='edu',\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    orig_text=annot['text'],\n",
    "                    proba=1.\n",
    "                )\n",
    "            _edus.append(temp)\n",
    "            last_end = end\n",
    "\n",
    "        tmp = gen(_edus, df, annot['text'])\n",
    "        tmp = pd.DataFrame(extr_pairs_forest(tmp), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "        tmp = tmp[tmp.category_id == 'no_relation']\n",
    "    \n",
    "    tmp.to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "for filename in tqdm(glob.glob(\"data/*.json.neg\")):    \n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename).drop(columns=['loc_y'])\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    \n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    result = features_processor(df, \\\n",
    "                                   annotation['text'],\\\n",
    "                                   annotation['tokens'],\\\n",
    "                                   annotation['sentences'],\\\n",
    "                                   annotation['lemma'],\\\n",
    "                                   annotation['morph'],\\\n",
    "                                   annotation['postag'],\\\n",
    "                                   annotation['syntax_dep_tree'])\n",
    "    \n",
    "    result = result[result.is_broken == False]\n",
    "    \n",
    "    result = result[result.tokens_x.map(len) < MAX_LEN]\n",
    "    result = result[result.tokens_y.map(len) < MAX_LEN]\n",
    "    \n",
    "    result.to_pickle(filename + '.neg.features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "random_state = 45\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    dev_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    dev_samples.append(negative)\n",
    "    \n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples)\n",
    "dev_samples = pd.concat(dev_samples)\n",
    "test_samples = pd.concat(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.prepare_sequence import _prepare_sequence\n",
    "\n",
    "\n",
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row\n",
    "\n",
    "def prepare_data(data, max_len=100):\n",
    "\n",
    "    data = data[data.tokens_x.map(len) < max_len]\n",
    "    data = data[data.tokens_y.map(len) < max_len]\n",
    "    \n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data.apply(correct_samples, axis=1)\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "    \n",
    "    data['snippet_x'] = data.snippet_x.map(_prepare_sequence)\n",
    "    data['snippet_y'] = data.snippet_y.map(_prepare_sequence)\n",
    "    \n",
    "    data = data.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)\n",
    "\n",
    "OUT_PATH = 'data_structure'\n",
    "! mkdir $OUT_PATH\n",
    "train_samples.to_pickle(os.path.join(OUT_PATH, 'train_samples.pkl'))\n",
    "dev_samples.to_pickle(os.path.join(OUT_PATH, 'dev_samples.pkl'))\n",
    "test_samples.to_pickle(os.path.join(OUT_PATH, 'test_samples.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
