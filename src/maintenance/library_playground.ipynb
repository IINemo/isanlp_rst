{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER0 = ''\n",
    "SERVER1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.processor_syntaxnet_remote import ProcessorSyntaxNetRemote\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "address_morph = (SERVER0, 4333)\n",
    "address_syntax = (SERVER0, 4343)\n",
    "address_rst = (SERVER1, 3490)\n",
    "\n",
    "ppl = PipelineCommon([(ProcessorRemote(address_morph[0], address_morph[1], 'default'),\n",
    "                 ['text'],\n",
    "                 {'tokens': 'tokens',\n",
    "                  'sentences': 'sentences',\n",
    "                  'postag': 'mystem_postag',\n",
    "                  'lemma': 'lemma'}),\n",
    "                (ProcessorSyntaxNetRemote(address_syntax[0], address_syntax[1]),\n",
    "                 ['tokens', 'sentences'],\n",
    "                 {'syntax_dep_tree': 'syntax_dep_tree'}),\n",
    "                (ConverterMystemToUd(),\n",
    "                 ['mystem_postag'],\n",
    "                 {'morph': 'morph',\n",
    "                  'postag': 'postag'}),\n",
    "                (ProcessorRemote(address_rst[0], address_rst[1], 'default'),\n",
    "                 ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "                 {'rst': 'rst'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ppl('Внутри 22-й Московской международной книжной выставки-ярмарки, проходившей в начале сентября 2009 года на ВВЦ, работала внутренняя выставка - «Книгабайт», посвящённая электронному книгоизданию.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = ppl('Как сообщили «Ведомости», правительство внесло в Госдуму пакет поправок в законодательство по помощи регионам в связи с потерями, которые они понесут из-за кризиса. Общая стоимость помощи - около 100 млрд рублей. Однако налоговые доходы регионов, по данным Минфина, могут снизиться в 2009 году на 700-800 млрд рублей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res['rst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    \n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['rst'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['rst'][0].left.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res['rst'][0].right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extr_pairs(res['rst'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_pickle(filename + '.gold.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_json(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.annot.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/news1_62'\n",
    "edus = read_edus(filename)\n",
    "gold = read_json(filename)\n",
    "annot = read_annotation(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annot['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            gap_counter = 0\n",
    "            #while len(left.text + right.text) < len(self.text):\n",
    "            #    self.text = left.text + ' ' * gap_counter + right.text\n",
    "            #    gap_counter += 1\n",
    "            self.start = left.start\n",
    "            self.end = right.end\n",
    "        \n",
    "        # (1) for gold tree parsing\n",
    "        \"\"\"\n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "        \"\"\"\n",
    "        # (2) ??\n",
    "        \n",
    "        if self.left:\n",
    "            self.text = ' '.join([self.left.text, self.right.text])\n",
    "        else:\n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor):\n",
    "        self.features_processor = features_processor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "        self.genre = None\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        return self.label_predictor.predict(features)\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, corpus):\n",
    "        RSTTreePredictor.__init__(self, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return [args[0].text, args[1].text]\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return ((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                    or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                        axis=0) != 0)\n",
    "\n",
    "        left_snippet, right_snippet = features\n",
    "        return float(_check_snippet_pair_in_dataset(left_snippet, right_snippet))\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        label = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].category_id.values\n",
    "        if label.size == 0:\n",
    "            return 'relation'\n",
    "        \n",
    "        return label[0]\n",
    "    \n",
    "    def predict_nuclearity(self, features):\n",
    "        left_snippet, right_snippet = features\n",
    "        nuclearity = self.corpus[((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].order.values\n",
    "        if nuclearity.size == 0:\n",
    "            return '_'\n",
    "        \n",
    "        return nuclearity[0]\n",
    "\n",
    "\n",
    "class CustomTreePredictor(RSTTreePredictor):\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor=None):\n",
    "        RSTTreePredictor.__init__(self, features_processor, relation_predictor, label_predictor)\n",
    "\n",
    "    def extract_features(self, left_node: DiscourseUnit, right_node: DiscourseUnit,\n",
    "                         annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                         annot_syntax_dep_tree):\n",
    "        pair = pd.DataFrame({\n",
    "            'snippet_x': [left_node.text.strip()],\n",
    "            'snippet_y': [right_node.text.strip()],\n",
    "            #'genre': self.genre\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_processor(pair, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        return self.relation_predictor.predict_proba(features)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "\n",
    "class GreedyRSTParser:\n",
    "    def __init__(self, tree_predictor, forest_threshold=0.05):\n",
    "        \"\"\"\n",
    "        :param RSTTreePredictor tree_predictor:\n",
    "        :param float forest_threshold: minimum relation probability to append the pair into the tree\n",
    "        \"\"\"\n",
    "        self.tree_predictor = tree_predictor\n",
    "        self.forest_threshold = forest_threshold\n",
    "\n",
    "    def __call__(self, edus, annot_text, annot_tokens, annot_sentences, annot_postag, annot_morph, annot_lemma,\n",
    "                 annot_syntax_dep_tree, genre=None):\n",
    "        \"\"\"\n",
    "        :param list edus: DiscourseUnit\n",
    "        :param str annot_text: original text\n",
    "        :param list annot_tokens: isanlp.annotation.Token\n",
    "        :param list annot_sentences: isanlp.annotation.Sentence\n",
    "        :param list annot_postag: lists of str for each sentence\n",
    "        :param annot_lemma: lists of str for each sentence\n",
    "        :param annot_syntax_dep_tree: list of isanlp.annotation.WordSynt for each sentence\n",
    "        :return: list of DiscourseUnit containing each extracted tree\n",
    "        \"\"\"\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        self.tree_predictor.genre = genre\n",
    "\n",
    "        nodes = edus\n",
    "        \n",
    "        for edu in nodes:\n",
    "            print(edu, file=sys.stderr)\n",
    "        \n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = [\n",
    "            self.tree_predictor.extract_features(nodes[i], nodes[i + 1], annot_text, annot_tokens,\n",
    "                                                 annot_sentences,\n",
    "                                                 annot_postag, annot_morph, annot_lemma,\n",
    "                                                 annot_syntax_dep_tree)\n",
    "            for i in range(len(nodes) - 1)]\n",
    "\n",
    "        scores = [self.tree_predictor.predict_pair_proba(features[i]) for i in range(len(nodes) - 1)]\n",
    "        relations = [self.tree_predictor.predict_label(features[i]) for i in range(len(nodes) - 1)]\n",
    "        nuclearities = [self.tree_predictor.predict_nuclearity(features[i]) for i in range(len(nodes) - 1)]\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "            \n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features[j]),\n",
    "                proba=scores[j],\n",
    "                text=nodes[j].text + nodes[j + 1].text  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "            )\n",
    "            \n",
    "            print(temp, file=sys.stderr)\n",
    "            \n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                annot_text, annot_tokens, \n",
    "                                                                annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = [predicted] + scores[j + 2:]\n",
    "                features = [features_right] + features[j + 2:]\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j], \n",
    "                                                                     annot_text, annot_tokens,\n",
    "                                                                     annot_sentences, annot_postag, \n",
    "                                                                     annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_left = self.tree_predictor.predict_pair_proba(features_left)\n",
    "\n",
    "                features_right = self.tree_predictor.extract_features(nodes[j], nodes[j + 1], \n",
    "                                                                      annot_text, annot_tokens,\n",
    "                                                                      annot_sentences, annot_postag, \n",
    "                                                                      annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted_right = self.tree_predictor.predict_pair_proba(features_right)\n",
    "\n",
    "                scores = scores[:j - 1] + [predicted_left] + [predicted_right] + scores[j + 2:]\n",
    "                features = features[:j - 1] + [features_left] + [features_right] + features[j + 2:]\n",
    "\n",
    "            else:\n",
    "                features_left = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                annot_text, annot_tokens, \n",
    "                                                                annot_sentences, annot_postag,\n",
    "                                                                annot_morph, annot_lemma, annot_syntax_dep_tree)\n",
    "                predicted = self.tree_predictor.predict_pair_proba(features_left)\n",
    "                scores = scores[:j - 1] + [predicted]\n",
    "                features = features[:j - 1] + [features_left]\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end\n",
    "\n",
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as fn\n",
    "\n",
    "\n",
    "def printBTree(node, nodeInfo=None, inverted=False, isTop=True):\n",
    "    # node value string and sub nodes\n",
    "    info = nodeInfo(node)\n",
    "    \n",
    "    if info:\n",
    "        stringValue, leftNode, rightNode = info\n",
    "\n",
    "        stringValueWidth  = len(stringValue)\n",
    "\n",
    "        # recurse to sub nodes to obtain line blocks on left and right\n",
    "        leftTextBlock     = [] if not leftNode else printBTree(leftNode, nodeInfo, inverted, False)\n",
    "        rightTextBlock    = [] if not rightNode else printBTree(rightNode, nodeInfo, inverted, False)\n",
    "\n",
    "        # count common and maximum number of sub node lines\n",
    "        commonLines       = min(len(leftTextBlock),len(rightTextBlock))\n",
    "        subLevelLines     = max(len(rightTextBlock),len(leftTextBlock))\n",
    "\n",
    "        # extend lines on shallower side to get same number of lines on both sides\n",
    "        leftSubLines      = leftTextBlock  + [\"\"] *  (subLevelLines - len(leftTextBlock))\n",
    "        rightSubLines     = rightTextBlock + [\"\"] *  (subLevelLines - len(rightTextBlock))\n",
    "\n",
    "        # compute location of value or link bar for all left and right sub nodes\n",
    "        #   * left node's value ends at line's width\n",
    "        #   * right node's value starts after initial spaces\n",
    "        leftLineWidths    = [ len(line) for line in leftSubLines  ]                            \n",
    "        rightLineIndents  = [ len(line)-len(line.lstrip(\" \")) for line in rightSubLines ]\n",
    "\n",
    "        # top line value locations, will be used to determine position of current node & link bars\n",
    "        firstLeftWidth    = (leftLineWidths   + [0])[0]  \n",
    "        firstRightIndent  = (rightLineIndents + [0])[0] \n",
    "\n",
    "        # width of sub node link under node value (i.e. with slashes if any)\n",
    "        # aims to center link bars under the value if value is wide enough\n",
    "        # \n",
    "        # ValueLine:    v     vv    vvvvvv   vvvvv\n",
    "        # LinkLine:    / \\   /  \\    /  \\     / \\ \n",
    "        #\n",
    "        linkSpacing       = min(stringValueWidth, 2 - stringValueWidth % 2)\n",
    "        leftLinkBar       = 1 if leftNode  else 0\n",
    "        rightLinkBar      = 1 if rightNode else 0\n",
    "        minLinkWidth      = leftLinkBar + linkSpacing + rightLinkBar\n",
    "        valueOffset       = (stringValueWidth - linkSpacing) // 2\n",
    "\n",
    "        # find optimal position for right side top node\n",
    "        #   * must allow room for link bars above and between left and right top nodes\n",
    "        #   * must not overlap lower level nodes on any given line (allow gap of minSpacing)\n",
    "        #   * can be offset to the left if lower subNodes of right node \n",
    "        #     have no overlap with subNodes of left node                                                                                                                                 \n",
    "        minSpacing        = 2\n",
    "        rightNodePosition = fn.reduce(lambda r,i: max(r,i[0] + minSpacing + firstRightIndent - i[1]), \\\n",
    "                                     zip(leftLineWidths,rightLineIndents[0:commonLines]), \\\n",
    "                                     firstLeftWidth + minLinkWidth)\n",
    "\n",
    "        # extend basic link bars (slashes) with underlines to reach left and right\n",
    "        # top nodes.  \n",
    "        #\n",
    "        #        vvvvv\n",
    "        #       __/ \\__\n",
    "        #      L       R\n",
    "        #\n",
    "        linkExtraWidth    = max(0, rightNodePosition - firstLeftWidth - minLinkWidth )\n",
    "        rightLinkExtra    = linkExtraWidth // 2\n",
    "        leftLinkExtra     = linkExtraWidth - rightLinkExtra\n",
    "\n",
    "        # build value line taking into account left indent and link bar extension (on left side)\n",
    "        valueIndent       = max(0, firstLeftWidth + leftLinkExtra + leftLinkBar - valueOffset)\n",
    "        valueLine         = \" \" * max(0,valueIndent) + stringValue\n",
    "        slash             = \"\\\\\" if inverted else  \"/\"\n",
    "        backslash         = \"/\" if inverted else  \"\\\\\"\n",
    "        uLine             = \"¯\" if inverted else  \"_\"\n",
    "\n",
    "        # build left side of link line\n",
    "        leftLink          = \"\" if not leftNode else ( \" \" * firstLeftWidth + uLine * leftLinkExtra + slash)\n",
    "\n",
    "        # build right side of link line (includes blank spaces under top node value) \n",
    "        rightLinkOffset   = linkSpacing + valueOffset * (1 - leftLinkBar)                      \n",
    "        rightLink         = \"\" if not rightNode else ( \" \" * rightLinkOffset + backslash + uLine * rightLinkExtra )\n",
    "\n",
    "        # full link line (will be empty if there are no sub nodes)                                                                                                    \n",
    "        linkLine          = leftLink + rightLink\n",
    "\n",
    "        # will need to offset left side lines if right side sub nodes extend beyond left margin\n",
    "        # can happen if left subtree is shorter (in height) than right side subtree                                                \n",
    "        leftIndentWidth   = max(0,firstRightIndent - rightNodePosition) \n",
    "        leftIndent        = \" \" * leftIndentWidth\n",
    "        indentedLeftLines = [ (leftIndent if line else \"\") + line for line in leftSubLines ]\n",
    "\n",
    "        # compute distance between left and right sublines based on their value position\n",
    "        # can be negative if leading spaces need to be removed from right side\n",
    "        mergeOffsets      = [ len(line) for line in indentedLeftLines ]\n",
    "        mergeOffsets      = [ leftIndentWidth + rightNodePosition - firstRightIndent - w for w in mergeOffsets ]\n",
    "        mergeOffsets      = [ p if rightSubLines[i] else 0 for i,p in enumerate(mergeOffsets) ]\n",
    "\n",
    "        # combine left and right lines using computed offsets\n",
    "        #   * indented left sub lines\n",
    "        #   * spaces between left and right lines\n",
    "        #   * right sub line with extra leading blanks removed.\n",
    "        mergedSubLines    = zip(range(len(mergeOffsets)), mergeOffsets, indentedLeftLines)\n",
    "        mergedSubLines    = [ (i,p,line + (\" \" * max(0,p)) )       for i,p,line in mergedSubLines ]\n",
    "        mergedSubLines    = [ line + rightSubLines[i][max(0,-p):]  for i,p,line in mergedSubLines ]                        \n",
    "\n",
    "        # Assemble final result combining\n",
    "        #  * node value string\n",
    "        #  * link line (if any)\n",
    "        #  * merged lines from left and right sub trees (if any)\n",
    "        treeLines = [leftIndent + valueLine] + ( [] if not linkLine else [leftIndent + linkLine] ) + mergedSubLines\n",
    "\n",
    "        # invert final result if requested\n",
    "        treeLines = reversed(treeLines) if inverted and isTop else treeLines\n",
    "\n",
    "        # return intermediate tree lines or print final result\n",
    "        if isTop : return \"\\n\".join(treeLines)\n",
    "        else     : return treeLines        \n",
    "\n",
    "def print_rst_tree(tree, file):\n",
    "    def _(n):\n",
    "        if n.relation != 'elementary':\n",
    "            value = (n.relation, n.proba)\n",
    "        else:\n",
    "            value = n.text\n",
    "        \n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    lines = printBTree(tree, _)\n",
    "    file.write(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rst_tree(parsed[2], open('tmp.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_list(tree):        \n",
    "    rr = []\n",
    "    \n",
    "    if tree.relation not in rr:\n",
    "        rr = [tree.relation]\n",
    "\n",
    "    if tree.left:\n",
    "        rr += extract_relations_list(tree.left)\n",
    "        rr += extract_relations_list(tree.right)\n",
    "\n",
    "    rr = list(set(rr))\n",
    "    rr.sort()\n",
    "    \n",
    "    return rr\n",
    "\n",
    "def extract_segments(tree):\n",
    "    span_counter = tree.id + 1\n",
    "    edus = []\n",
    "    spans = []\n",
    "\n",
    "    if tree and tree.left and tree.left.relation == 'edu':\n",
    "        temp_edu = ((tree.left.id, tree.right.id, tree.relation, tree.left.text))  # id, parent, relation, text            \n",
    "        edus.append(temp_edu)\n",
    "        \n",
    "    if tree and tree.right and tree.right.relation == 'edu':\n",
    "        edus.append((tree.right.id, tree.left.id, tree.relation, tree.right.text))\n",
    "\n",
    "    if tree.left:\n",
    "        edus += extract_segments(tree.left)\n",
    "        \n",
    "    if tree.right:\n",
    "        edus += extract_segments(tree.right)\n",
    "\n",
    "\n",
    "    return sorted(edus, key=lambda x: x[0])\n",
    "\n",
    "def extract_groups(tree):\n",
    "    groups = []\n",
    "        \n",
    "    if tree and tree.left and tree.left.relation != 'edu':\n",
    "        groups.append((tree.left.id, tree.right.id, tree.relation, tree.text))  # id, parent, relation, text\n",
    "        \n",
    "    if tree and tree.right and tree.right.relation != 'edu':\n",
    "        groups.append((tree.right.id, tree.left.id, tree.relation, tree.text))\n",
    "    \n",
    "    else:\n",
    "        if tree.left:\n",
    "            groups += extract_groups(tree.left)\n",
    "        if tree.right:\n",
    "            groups += extract_groups(tree.right)\n",
    "\n",
    "    return groups\n",
    "    \n",
    "def export_rs3(tree):    \n",
    "    def make_header():\n",
    "        def wrap_relations(relations_list):\n",
    "            res = '\\t\\t<relations>\\n'\n",
    "\n",
    "            for relation in relations_list:\n",
    "                res += f'\\t\\t\\t<rel name=\"{relation}\" type=\"rst\" />\\n'\n",
    "\n",
    "            res += '\\t\\t</relations>\\n'\n",
    "            return res\n",
    "    \n",
    "        res = '\\t<header>\\n' +\\\n",
    "                wrap_relations(extract_relations_list(tree)) +\\\n",
    "              '\\t</header>\\n'\n",
    "    \n",
    "        return res\n",
    "    \n",
    "    def make_body():\n",
    "        def wrap_segments(segments):\n",
    "            res = ''\n",
    "            \n",
    "            for segment in segments:\n",
    "                res += f'\\t\\t<segment id=\"{segment[0]}\" parent=\"{segment[1]}\" relname=\"{segment[2]}\">'\n",
    "                res += segment[3]\n",
    "                res += '</segment>\\n'\n",
    "            \n",
    "            return res\n",
    "        \n",
    "        def wrap_groups(groups):\n",
    "            res = ''\n",
    "            \n",
    "            for group in groups:\n",
    "                res += f'\\t\\t<group id=\"{group[0]}\" type=\"span\" parent=\"{group[1]}\" relname=\"{group[2]}\">\\n'\n",
    "                \n",
    "            return res\n",
    "                \n",
    "        \n",
    "        res = '\\t<body>\\n' +\\\n",
    "                wrap_segments(extract_segments(tree)) +\\\n",
    "                wrap_groups(extract_groups(tree)) +\\\n",
    "              '\\t</body>\\n'\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    return '<rst>\\n' + make_header() + make_body() + '</rst>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, id, left_id, right_id):\n",
    "        self.id = id\n",
    "        self.left_id = left_id\n",
    "        self.right_id = right_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spans(tree):\n",
    "    span_counter = tree.id + 1\n",
    "    edus = []\n",
    "    spans = []\n",
    "\n",
    "    if tree and tree.left and tree.left.relation == 'edu':\n",
    "        spans.append(Span(span_counter, tree.left.id, tree.right.id))\n",
    "        #temp_edu = ((tree.left.id, tree.right.id, tree.relation, tree.left.text))  # id, parent, relation, text\n",
    "\n",
    "        if tree.nuclearity == 'NS':\n",
    "            edus.append((tree.left.id, span_counter, 'span', tree.left.text))  # id, parent, relation, text\n",
    "            edus.append((tree.right.id, tree.left.id, tree.relation, tree.right.text))\n",
    "        elif tree.nuclearity == 'SN':\n",
    "            edus.append((tree.left.id, tree.right.id, tree.relation, tree.left.text))\n",
    "            edus.append((tree.right.id, span_counter, 'span', tree.right.text))\n",
    "        elif tree.nuclearity == 'NN':\n",
    "            edus.append((tree.left.id, span_counter, 'multinuc', tree.left.text))\n",
    "            edus.append((tree.right.id, span_counter, 'multinuc', tree.right.text))\n",
    "            \n",
    "    if tree.left:\n",
    "        n_spans, n_edus = extract_spans(tree.left)\n",
    "        spans += n_spans\n",
    "        edus += n_edus\n",
    "        \n",
    "    if tree. right:\n",
    "        n_spans, n_edus = extract_spans(tree.right)\n",
    "        spans += n_spans\n",
    "        edus += n_edus        \n",
    "\n",
    "    return sorted(spans, key=lambda x: x.id), edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = read_json('data/news1_62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.order.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed[0].nuclearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parsed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(export_rs3(parsed[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
