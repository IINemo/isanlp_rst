{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../logs/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (EDUs only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_pickle(filename + '.gold.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_json(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.annot.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f533ae0ec27a426ab1551b2ed020ec7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "TARGET = 'category_id'\n",
    "\n",
    "df = []\n",
    "for file in tqdm(glob.glob('data/*.edus')):\n",
    "    filename = file.replace('.edus', '')\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    \n",
    "    def label_edu(discourse_unit):\n",
    "        return discourse_unit in edus\n",
    "    \n",
    "    gold['edu_x'] = gold.snippet_x.map(label_edu)\n",
    "    gold['edu_y'] = gold.snippet_y.map(label_edu)\n",
    "    gold['edu_pair'] = gold['edu_x'] & gold['edu_y']\n",
    "    gold = gold[gold.edu_pair]\n",
    "    gold = gold.drop(columns=['edu_x', 'edu_y', 'edu_pair'])\n",
    "    df.append(gold)\n",
    "    \n",
    "df = pd.concat(df)\n",
    "df = df.drop_duplicates(['snippet_x', 'snippet_y', TARGET])\n",
    "df = df[df['snippet_x'].map(len) > 0]\n",
    "df = df[df['snippet_y'].map(len) > 0]\n",
    "\n",
    "TARGET = 'category_id'\n",
    "\n",
    "df[TARGET] = df[TARGET].replace(['cause-effect_r', 'effect_r'], 'cause_r')\n",
    "df[TARGET] = df[TARGET].replace(['interpretation-evaluation_r', 'conclusion_r'], 'evaluation_r')\n",
    "\n",
    "y_stat = df[TARGET].value_counts()\n",
    "drop_ys = y_stat[y_stat < 100].index #+ ['elaboration_r', 'joint_m', 'same-unit_m']\n",
    "\n",
    "for dy in drop_ys:\n",
    "    df = df[df[TARGET] != dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elaboration_r    1491\n",
       "joint_m          1376\n",
       "cause_r           604\n",
       "condition_r       519\n",
       "purpose_r         476\n",
       "attribution_r     328\n",
       "contrast_m        196\n",
       "evidence_r        124\n",
       "comparison_m      110\n",
       "restatement_m     108\n",
       "Name: category_id, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = df[TARGET].to_frame(), df.drop(TARGET, axis=1).drop(columns=['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'filename', 'order', 'postags_x', 'postags_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.python.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from tensorflow.python.keras.layers import Reshape\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input, Layer\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Permute, Add\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import GaussianNoise\n",
    "from tensorflow.python.keras.layers import UpSampling1D\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape, Layer, InputSpec\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INNER_SIZE = len(df.category_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_ae(input_shape):\n",
    "    std_dev = 1e-1\n",
    "    regul_constant = 1e-2\n",
    "    \n",
    "    def encode_plain_input(input_layer):\n",
    "        input_layer = GaussianNoise(std_dev)(input_layer)\n",
    "        enc_1 = Dense(input_shape[0] // 4, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='enc1')(input_layer)\n",
    "        enc_2 = Dense(_INNER_SIZE, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='embedding')(enc_1)\n",
    "        return enc_2\n",
    "    \n",
    "    def decode_plain_input(latent):\n",
    "        dec_1 = Dense(input_shape[0] // 4, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='dec1')(latent)\n",
    "        dec_2 = Dense(input_shape[0], activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='dec2')(dec_1)\n",
    "        return dec_2\n",
    "        \n",
    "    \n",
    "    input_pair = Input(shape=input_shape, name='input_pair')\n",
    "    latent = encode_plain_input(input_pair)\n",
    "    decoded = decode_plain_input(latent)\n",
    "    \n",
    "    model = Model(inputs=[input_pair], \n",
    "                  outputs=[decoded])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_pair (InputLayer)      (None, 2590)              0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 2590)              0         \n",
      "_________________________________________________________________\n",
      "enc1 (Dense)                 (None, 647)               1676377   \n",
      "_________________________________________________________________\n",
      "enc2 (Dense)                 (None, 10)                6480      \n",
      "_________________________________________________________________\n",
      "dec1 (Dense)                 (None, 647)               7117      \n",
      "_________________________________________________________________\n",
      "dec2 (Dense)                 (None, 2590)              1678320   \n",
      "=================================================================\n",
      "Total params: 3,368,294\n",
      "Trainable params: 3,368,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5332/5332 [==============================] - 1s 116us/step - loss: 10.9529\n",
      "Epoch 2/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 2.2046\n",
      "Epoch 3/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.4811\n",
      "Epoch 4/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.1547\n",
      "Epoch 5/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0908\n",
      "Epoch 6/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0757\n",
      "Epoch 7/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0697\n",
      "Epoch 8/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0658\n",
      "Epoch 9/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0624\n",
      "Epoch 10/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0594\n",
      "Epoch 11/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0565\n",
      "Epoch 12/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0539\n",
      "Epoch 13/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0515\n",
      "Epoch 14/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0493\n",
      "Epoch 15/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0472\n",
      "Epoch 16/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0452\n",
      "Epoch 17/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0433\n",
      "Epoch 18/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0416\n",
      "Epoch 19/200\n",
      "5332/5332 [==============================] - 0s 62us/step - loss: 0.0400\n",
      "Epoch 20/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0384\n",
      "Epoch 21/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0369\n",
      "Epoch 22/200\n",
      "5332/5332 [==============================] - 0s 62us/step - loss: 0.0355\n",
      "Epoch 23/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0342\n",
      "Epoch 24/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0329\n",
      "Epoch 25/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0317\n",
      "Epoch 26/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0306\n",
      "Epoch 27/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0295\n",
      "Epoch 28/200\n",
      "5332/5332 [==============================] - 0s 68us/step - loss: 0.0284\n",
      "Epoch 29/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0275\n",
      "Epoch 30/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0266\n",
      "Epoch 31/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0258\n",
      "Epoch 32/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0250\n",
      "Epoch 33/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0243\n",
      "Epoch 34/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0237\n",
      "Epoch 35/200\n",
      "5332/5332 [==============================] - 0s 66us/step - loss: 0.0231\n",
      "Epoch 36/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0225\n",
      "Epoch 37/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0220\n",
      "Epoch 38/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0215\n",
      "Epoch 39/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0210\n",
      "Epoch 40/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0205\n",
      "Epoch 41/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0201\n",
      "Epoch 42/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0197\n",
      "Epoch 43/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0193\n",
      "Epoch 44/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0190\n",
      "Epoch 45/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0186\n",
      "Epoch 46/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0183\n",
      "Epoch 47/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0180\n",
      "Epoch 48/200\n",
      "5332/5332 [==============================] - 0s 65us/step - loss: 0.0176\n",
      "Epoch 49/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0173\n",
      "Epoch 50/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0171\n",
      "Epoch 51/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0168\n",
      "Epoch 52/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0165\n",
      "Epoch 53/200\n",
      "5332/5332 [==============================] - 0s 68us/step - loss: 0.0163\n",
      "Epoch 54/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0160\n",
      "Epoch 55/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0158\n",
      "Epoch 56/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0156\n",
      "Epoch 57/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0154\n",
      "Epoch 58/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0152\n",
      "Epoch 59/200\n",
      "5332/5332 [==============================] - 0s 66us/step - loss: 0.0150\n",
      "Epoch 60/200\n",
      "5332/5332 [==============================] - 0s 63us/step - loss: 0.0148\n",
      "Epoch 61/200\n",
      "5332/5332 [==============================] - 0s 64us/step - loss: 0.0146\n",
      "Epoch 62/200\n",
      "3712/5332 [===================>..........] - ETA: 0s - loss: 0.0144"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-08fca8ac855b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m model.fit(x=[X],\n\u001b[0;32m----> 9\u001b[0;31m           y=[X], epochs=200, batch_size=128)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = noised_ae((X.shape[1:]))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "model.fit(x=[X],\n",
    "          y=[X], epochs=200, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.deep_clustering as deep_clustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "def loop(iteration):\n",
    "    result = []\n",
    "    \n",
    "    while iteration:\n",
    "        K.clear_session()\n",
    "\n",
    "        save_dir = 'idec'\n",
    "        ! mkdir $save_dir\n",
    "\n",
    "        idec = deep_clustering.IDEC(input_shape=(X.shape[1:]),\n",
    "                                    autoencoder_ctor=lambda input_shape: noised_ae(input_shape),#restore_rel(input_shape),  # select model here\n",
    "                                    n_clusters=_INNER_SIZE,\n",
    "                                    pretrain_epochs=2,\n",
    "                                    maxiter=100,\n",
    "                                    save_dir=save_dir, \n",
    "                                    log_dir=logPath)\n",
    "\n",
    "        plot_model(idec._model, to_file=os.path.join(save_dir, 'idec_model.png'), show_shapes=True)\n",
    "        idec.compile(gamma=.1)\n",
    "        idec.fit([X], batch_size=512)\n",
    "        result.append(v_measure_score(y[TARGET].values, idec._y_pred))\n",
    "        iteration -= 1\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:40:09,007 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:40:09,193 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:40:11,343 [MainThread  ] [INFO ]  Pretraining time: 2.082761764526367\n",
      "2019-12-17 12:40:11,550 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:40:11,551 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:40:12,621 [MainThread  ] [INFO ]  Cluster centers initialized: 1.0693936347961426\n",
      "2019-12-17 12:40:12,622 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:40:12,622 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:40:12,623 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:40:12,623 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:40:13,738 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:40:15,193 [MainThread  ] [INFO ]  Done. 2.5710442066192627\n",
      "2019-12-17 12:40:15,194 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:40:15,226 [MainThread  ] [INFO ]  Pretrain time: 2.3583695888519287\n",
      "2019-12-17 12:40:15,227 [MainThread  ] [INFO ]  Clustering time: 3.6748852729797363\n",
      "2019-12-17 12:40:15,227 [MainThread  ] [INFO ]  Total time: 6.033254861831665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:40:15,574 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:40:15,775 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:40:18,035 [MainThread  ] [INFO ]  Pretraining time: 2.188121795654297\n",
      "2019-12-17 12:40:18,240 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:40:18,241 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:40:19,462 [MainThread  ] [INFO ]  Cluster centers initialized: 1.2207067012786865\n",
      "2019-12-17 12:40:19,463 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:40:19,463 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:40:19,464 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:40:19,464 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:40:20,559 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:40:22,025 [MainThread  ] [INFO ]  Done. 2.561309576034546\n",
      "2019-12-17 12:40:22,025 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:40:22,057 [MainThread  ] [INFO ]  Pretrain time: 2.4667022228240967\n",
      "2019-12-17 12:40:22,058 [MainThread  ] [INFO ]  Clustering time: 3.8157715797424316\n",
      "2019-12-17 12:40:22,058 [MainThread  ] [INFO ]  Total time: 6.282473802566528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:40:22,408 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:40:22,604 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:40:24,931 [MainThread  ] [INFO ]  Pretraining time: 2.120739698410034\n",
      "2019-12-17 12:40:25,128 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:40:25,129 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:40:26,138 [MainThread  ] [INFO ]  Cluster centers initialized: 1.0084447860717773\n",
      "2019-12-17 12:40:26,139 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:40:26,139 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:40:26,139 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:40:26,140 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:40:27,205 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:40:28,698 [MainThread  ] [INFO ]  Done. 2.559394121170044\n",
      "2019-12-17 12:40:28,699 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:40:29,327 [MainThread  ] [INFO ]  Pretrain time: 2.525473117828369\n",
      "2019-12-17 12:40:29,328 [MainThread  ] [INFO ]  Clustering time: 4.197856664657593\n",
      "2019-12-17 12:40:29,329 [MainThread  ] [INFO ]  Total time: 6.723329782485962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:40:29,678 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:40:29,872 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:40:32,113 [MainThread  ] [INFO ]  Pretraining time: 2.170664072036743\n",
      "2019-12-17 12:40:32,321 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:40:32,322 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:40:33,372 [MainThread  ] [INFO ]  Cluster centers initialized: 1.049647331237793\n",
      "2019-12-17 12:40:33,373 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:40:33,373 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:40:33,374 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:40:33,374 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:40:34,550 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:40:36,010 [MainThread  ] [INFO ]  Done. 2.636526107788086\n",
      "2019-12-17 12:40:36,010 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:40:36,048 [MainThread  ] [INFO ]  Pretrain time: 2.4500932693481445\n",
      "2019-12-17 12:40:36,048 [MainThread  ] [INFO ]  Clustering time: 3.7256295680999756\n",
      "2019-12-17 12:40:36,049 [MainThread  ] [INFO ]  Total time: 6.17572283744812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:40:36,396 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:40:36,592 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:40:38,757 [MainThread  ] [INFO ]  Pretraining time: 2.0952465534210205\n",
      "2019-12-17 12:40:38,948 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:40:38,948 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:40:39,915 [MainThread  ] [INFO ]  Cluster centers initialized: 0.9664773941040039\n",
      "2019-12-17 12:40:39,916 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:40:39,917 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:40:39,917 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:40:39,918 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:40:41,207 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:40:42,550 [MainThread  ] [INFO ]  Done. 2.6334660053253174\n",
      "2019-12-17 12:40:42,551 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:40:42,580 [MainThread  ] [INFO ]  Pretrain time: 2.356815814971924\n",
      "2019-12-17 12:40:42,580 [MainThread  ] [INFO ]  Clustering time: 3.6307804584503174\n",
      "2019-12-17 12:40:42,581 [MainThread  ] [INFO ]  Total time: 5.987596273422241\n"
     ]
    }
   ],
   "source": [
    "res = loop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013608557674555419, 0.004258955299045326)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘idec’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 12:42:04,545 [MainThread  ] [INFO ]  Initialized tolerance = 0.05.\n",
      "2019-12-17 12:42:04,816 [MainThread  ] [INFO ]  Pretraining...\n",
      "2019-12-17 12:42:08,189 [MainThread  ] [INFO ]  Pretraining time: 3.2700369358062744\n",
      "2019-12-17 12:42:08,469 [MainThread  ] [INFO ]  Pretrained weights are saved to idec/pretrain_cae_model.h5\n",
      "2019-12-17 12:42:08,471 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "2019-12-17 12:42:09,748 [MainThread  ] [INFO ]  Cluster centers initialized: 1.2767457962036133\n",
      "2019-12-17 12:42:09,749 [MainThread  ] [INFO ]  Training model.\n",
      "2019-12-17 12:42:09,749 [MainThread  ] [INFO ]  Update interval 140\n",
      "2019-12-17 12:42:09,750 [MainThread  ] [INFO ]  Save interval 52.0703125\n",
      "2019-12-17 12:42:09,750 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2019-12-17 12:42:11,465 [MainThread  ] [INFO ]  saving model to: idec/dcec_model_0.h5\n",
      "2019-12-17 12:42:13,456 [MainThread  ] [INFO ]  Done. 3.7070164680480957\n",
      "2019-12-17 12:42:13,457 [MainThread  ] [INFO ]  Saving model to: idec/dcec_model_final.h5\n",
      "2019-12-17 12:42:13,498 [MainThread  ] [INFO ]  Pretrain time: 3.6550135612487793\n",
      "2019-12-17 12:42:13,499 [MainThread  ] [INFO ]  Clustering time: 5.026551961898804\n",
      "2019-12-17 12:42:13,499 [MainThread  ] [INFO ]  Total time: 8.681565523147583\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "save_dir = 'idec'\n",
    "! mkdir $save_dir\n",
    "\n",
    "idec = deep_clustering.IDEC(input_shape=(X.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: noised_ae(input_shape),#restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=_INNER_SIZE,\n",
    "                            pretrain_epochs=2,\n",
    "                            maxiter=100,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "\n",
    "plot_model(idec._model, to_file=os.path.join(save_dir, 'idec_model.png'), show_shapes=True)\n",
    "idec.compile(gamma=.1)\n",
    "idec.fit([X], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012207883152652052"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(y[TARGET].values, idec._y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.pred = idec._y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elaboration_r    180\n",
       "joint_m          167\n",
       "purpose_r         56\n",
       "condition_r       53\n",
       "cause_r           41\n",
       "attribution_r     32\n",
       "contrast_m        15\n",
       "comparison_m      10\n",
       "restatement_m     10\n",
       "evidence_r         8\n",
       "Name: category_id, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pred==9][TARGET].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
