{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../logs/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (EDUs only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html_map = {\n",
    "    r'\\n': r' ',\n",
    "    r'&gt;': r'>',\n",
    "    r'&lt;': r'<',\n",
    "    r'&amp;': r'&',\n",
    "    r'&quot;': r'\"',\n",
    "    r'&ndash;': r'–',\n",
    "    r'##### ': r'',\n",
    "    r'\\\\\\\\\\\\\\\\': r'\\\\',\n",
    "    r'  ': r' ',\n",
    "    r'——': r'-',\n",
    "    r'—': r'-',\n",
    "    r'/': r'',\n",
    "    r'\\^': r'',\n",
    "    r'^': r'',\n",
    "    r'±': r'+',\n",
    "    r'y': r'у',\n",
    "    r'x': r'х'\n",
    "}\n",
    "\n",
    "def read_edus(filename):\n",
    "    edus = []\n",
    "    with open(filename + '.edus', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            edu = str(line.strip())\n",
    "            for key, value in text_html_map.items():\n",
    "                edu = edu.replace(key, value)\n",
    "            edus.append(edu)\n",
    "    return edus\n",
    "\n",
    "def read_gold(filename):\n",
    "    df = pd.read_pickle(filename + '.gold.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_json(filename):\n",
    "    df = pd.read_json(filename + '.json')\n",
    "    for key in text_html_map.keys():\n",
    "        df['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        df['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_annotation(filename):\n",
    "    annot = pd.read_pickle(filename + '.annot.pkl')\n",
    "    for key in text_html_map.keys():\n",
    "        annot['text'] = annot['text'].replace(key, text_html_map[key])\n",
    "        for token in annot['tokens']:\n",
    "            token.text = token.text.replace(key, text_html_map[key])\n",
    "    \n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5efe8a2c8bb4e1f97948495b1abccb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=305), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "TARGET = 'category_id'\n",
    "\n",
    "df = []\n",
    "for file in tqdm(glob.glob('data/*.edus')):\n",
    "    filename = file.replace('.edus', '')\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    \n",
    "    def label_edu(discourse_unit):\n",
    "        return discourse_unit in edus\n",
    "    \n",
    "    gold['edu_x'] = gold.snippet_x.map(label_edu)\n",
    "    gold['edu_y'] = gold.snippet_y.map(label_edu)\n",
    "    gold['edu_pair'] = gold['edu_x'] & gold['edu_y']\n",
    "    gold = gold[gold.edu_pair]\n",
    "    gold = gold.drop(columns=['edu_x', 'edu_y', 'edu_pair'])\n",
    "    df.append(gold)\n",
    "    \n",
    "df = pd.concat(df)\n",
    "df = df.drop_duplicates(['snippet_x', 'snippet_y', TARGET])\n",
    "df = df[df['snippet_x'].map(len) > 0]\n",
    "df = df[df['snippet_y'].map(len) > 0]\n",
    "\n",
    "TARGET = 'category_id'\n",
    "\n",
    "df[TARGET] = df[TARGET].replace(['cause-effect_r', 'effect_r'], 'cause_r')\n",
    "df[TARGET] = df[TARGET].replace(['interpretation-evaluation_r', 'conclusion_r'], 'evaluation_r')\n",
    "\n",
    "y_stat = df[TARGET].value_counts()\n",
    "drop_ys = y_stat[y_stat < 100].index #+ ['elaboration_r', 'joint_m', 'same-unit_m']\n",
    "\n",
    "for dy in drop_ys:\n",
    "    df = df[df[TARGET] != dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_m          2767\n",
       "elaboration_r    2224\n",
       "cause_r          1180\n",
       "condition_r      1133\n",
       "purpose_r         844\n",
       "contrast_m        636\n",
       "attribution_r     516\n",
       "evaluation_r      353\n",
       "background_r      227\n",
       "comparison_m      187\n",
       "evidence_r        176\n",
       "concession_r      170\n",
       "sequence_m        165\n",
       "restatement_m     143\n",
       "preparation_r     138\n",
       "Name: category_id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = df[TARGET].to_frame(), df.drop(TARGET, axis=1).drop(columns=['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'filename', 'order', 'postags_x', 'postags_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.python.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from tensorflow.python.keras.layers import Reshape\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input, Layer\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Permute, Add\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import GaussianNoise\n",
    "from tensorflow.python.keras.layers import UpSampling1D\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape, Layer, InputSpec\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INNER_SIZE = len(df.category_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_ae(input_shape):\n",
    "    std_dev = 1e-1\n",
    "    regul_constant = 1e-1\n",
    "    \n",
    "    def encode_plain_input(input_layer):\n",
    "        input_layer = GaussianNoise(std_dev)(input_layer)\n",
    "        enc_1 = Dense(input_shape[0] // 6, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='enc1')(input_layer)\n",
    "        enc_2 = Dense(_INNER_SIZE, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='embedding')(enc_1)\n",
    "        return enc_2\n",
    "    \n",
    "    def decode_plain_input(latent):\n",
    "        dec_1 = Dense(input_shape[0] // 6, activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='dec1')(latent)\n",
    "        dec_2 = Dense(input_shape[0], activation='tanh',\n",
    "                      kernel_regularizer=regularizers.l2(regul_constant),\n",
    "                      name='dec2')(dec_1)\n",
    "        return dec_2\n",
    "        \n",
    "    \n",
    "    input_pair = Input(shape=input_shape, name='input_pair')\n",
    "    latent = encode_plain_input(input_pair)\n",
    "    decoded = decode_plain_input(latent)\n",
    "    \n",
    "    model = Model(inputs=[input_pair], \n",
    "                  outputs=[decoded])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = noised_ae((X.shape[1:]))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "model.fit(x=[X],\n",
    "          y=[X], epochs=200, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import utils.deep_clustering as deep_clustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "def loop(iteration):\n",
    "    result = []\n",
    "    \n",
    "    while iteration:\n",
    "        K.clear_session()\n",
    "\n",
    "        save_dir = 'idec'\n",
    "        ! mkdir $save_dir\n",
    "\n",
    "        idec = deep_clustering.IDEC(input_shape=(X.shape[1:]),\n",
    "                                    autoencoder_ctor=lambda input_shape: noised_ae(input_shape),#restore_rel(input_shape),  # select model here\n",
    "                                    n_clusters=_INNER_SIZE,\n",
    "                                    pretrain_epochs=2,\n",
    "                                    maxiter=100,\n",
    "                                    save_dir=save_dir, \n",
    "                                    log_dir=logPath)\n",
    "\n",
    "        plot_model(idec._model, to_file=os.path.join(save_dir, 'idec_model.png'), show_shapes=True)\n",
    "        idec.compile(gamma=.1)\n",
    "        idec.fit([X], batch_size=512)\n",
    "        result.append(v_measure_score(y[TARGET].values, idec._y_pred))\n",
    "        iteration -= 1\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = loop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0315648759780115, 0.0023137329105746085)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "save_dir = 'idec'\n",
    "! mkdir $save_dir\n",
    "\n",
    "idec = deep_clustering.IDEC(input_shape=(X.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: noised_ae(input_shape),#restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=_INNER_SIZE,\n",
    "                            pretrain_epochs=2,\n",
    "                            maxiter=100,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "\n",
    "plot_model(idec._model, to_file=os.path.join(save_dir, 'idec_model.png'), show_shapes=True)\n",
    "idec.compile(gamma=.1)\n",
    "idec.fit([X], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.742625201269691e-16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(y[TARGET].values, idec._y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pred = idec._y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_m          2767\n",
       "elaboration_r    2224\n",
       "cause_r          1180\n",
       "condition_r      1133\n",
       "purpose_r         844\n",
       "contrast_m        636\n",
       "attribution_r     516\n",
       "evaluation_r      353\n",
       "background_r      227\n",
       "comparison_m      187\n",
       "evidence_r        176\n",
       "concession_r      170\n",
       "sequence_m        165\n",
       "restatement_m     143\n",
       "preparation_r     138\n",
       "Name: category_id, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pred==4][TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_unif_loop(iteration):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        predicted = np.random.randint(0, _INNER_SIZE+1, size=df.shape[0])\n",
    "        result.append(v_measure_score(y[TARGET].values, predicted))\n",
    "        \n",
    "    return np.array(result)\n",
    "\n",
    "def random_exp_loop(iteration):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        predicted = np.random.exponential(scale=0.4, size=df.shape[0])\n",
    "        predicted = predicted/predicted.max()*15.\n",
    "        predicted = predicted.astype(int)\n",
    "        result.append(v_measure_score(y[TARGET].values, predicted))\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003925391325476878, 0.00036319401489300405)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_res = random_unif_loop(50)\n",
    "random_res.mean(), random_res.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
