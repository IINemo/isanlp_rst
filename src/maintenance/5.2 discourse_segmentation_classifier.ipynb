{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_triplets(edus, annot):\n",
    "    \"\"\" marker = start of edu \"\"\"\n",
    "    triplets = []\n",
    "    cursor = 0\n",
    "    \n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "            marker = 0  # class label, 1 is for 'start of edu'\n",
    "            start_of_sentence = 0\n",
    "            \n",
    "            if token == annot['sentences'][sentence].begin:\n",
    "                start_of_sentence = 1\n",
    "                if token > 0:\n",
    "                    left_neighbour = (#annot['tokens'][token-1].text,\n",
    "                                      annot['lemma'][sentence-1][-1],\n",
    "                                      annot['postag'][sentence-1][-1],\n",
    "                                      annot['syntax_dep_tree'][sentence-1][-1].link_name)\n",
    "                    original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                else:\n",
    "                    left_neighbour = ('', '', '')\n",
    "                    original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "            else:\n",
    "                left_neighbour = (#annot['tokens'][token-1].text,\n",
    "                                  annot['lemma'][sentence][token-1-annot['sentences'][sentence].begin],\n",
    "                                  annot['postag'][sentence][token-1-annot['sentences'][sentence].begin],\n",
    "                                  annot['syntax_dep_tree'][sentence][token-1-annot['sentences'][sentence].begin].link_name)\n",
    "                original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                \n",
    "            token_itself = (#annot['tokens'][token].text, \n",
    "                            int(annot['tokens'][token].text.istitle()),\n",
    "                            annot['lemma'][sentence][token-annot['sentences'][sentence].begin],\n",
    "                            annot['postag'][sentence][token-annot['sentences'][sentence].begin],\n",
    "                            annot['syntax_dep_tree'][sentence][token-annot['sentences'][sentence].begin].link_name)\n",
    "            \n",
    "            if token == annot['sentences'][sentence].end-1:\n",
    "                if token + 1 < len(annot['tokens']):\n",
    "                    right_neighbour = (#annot['tokens'][token+1].text, \n",
    "                                       annot['lemma'][sentence+1][0],\n",
    "                                        annot['postag'][sentence+1][0],\n",
    "                                        annot['syntax_dep_tree'][sentence+1][0].link_name)\n",
    "                    original_text += annot['text'][annot['tokens'][token].end:annot['tokens'][token].end]\n",
    "                else:\n",
    "                    right_neighbour = ('', '', '')\n",
    "            else:\n",
    "                right_neighbour = (#annot['tokens'][token+1].text, \n",
    "                                    annot['lemma'][sentence][token+1-annot['sentences'][sentence].begin],\n",
    "                                   annot['postag'][sentence][token+1-annot['sentences'][sentence].begin],\n",
    "                                   annot['syntax_dep_tree'][sentence][token+1-annot['sentences'][sentence].begin].link_name)\n",
    "                original_text += annot['text'][annot['tokens'][token].end:annot['tokens'][token].end]\n",
    "            \n",
    "            if cursor < len(edus):\n",
    "                if edus[cursor].startswith(original_text):\n",
    "                    marker = 1\n",
    "                    cursor += 1\n",
    "                \n",
    "            triplets.append(left_neighbour + token_itself + right_neighbour + (start_of_sentence, marker))\n",
    "            del left_neighbour, token_itself, right_neighbour, marker\n",
    "                \n",
    "            if cursor > len(edus):\n",
    "                break\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset as everywhere in this directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from utils.train_test_split import split_data\n",
    "\n",
    "train, test = split_data('data/', 0.2, seed=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triplets_bank = []\n",
    "filenames = []\n",
    "\n",
    "for file in train:\n",
    "    filename = file[:file.rfind('.edus')]\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    triplets = extract_triplets(edus, annot)\n",
    "    triplets_bank += triplets\n",
    "    filenames += [file] * len(triplets)\n",
    "    \n",
    "train = pd.DataFrame(triplets_bank, columns=['left_token', 'left_pos', 'left_link', \n",
    "                                             'is_title', 'token', 'pos', 'link', \n",
    "                                             'right_token', 'right_pos', 'right_link', \n",
    "                                             'start_sentence', 'class'])\n",
    "train['non_noun_tok'] = ((train['pos'] != 'NOUN') & (train['pos'] != 'VERB') & (train['pos'] != '')) * train['token']\n",
    "train['filename'] = filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_bank = []\n",
    "filenames = []\n",
    "\n",
    "for file in test:\n",
    "    filename = file[:file.rfind('.edus')]\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    triplets = extract_triplets(edus, annot)\n",
    "    triplets_bank += triplets\n",
    "    filenames += [file] * len(triplets)\n",
    "    \n",
    "test = pd.DataFrame(triplets_bank, columns=['left_token', 'left_pos', 'left_link', \n",
    "                                            'is_title', 'token', 'pos', 'link', \n",
    "                                            'right_token', 'right_pos', 'right_link',\n",
    "                                            'start_sentence', 'class'])\n",
    "test['non_noun_tok'] = ((test['pos'] != 'NOUN') & (test['pos'] != 'VERB') & (test['pos'] != '')) * test['token']\n",
    "test['filename'] = filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_token</th>\n",
       "      <th>left_pos</th>\n",
       "      <th>left_link</th>\n",
       "      <th>is_title</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>link</th>\n",
       "      <th>right_token</th>\n",
       "      <th>right_pos</th>\n",
       "      <th>right_link</th>\n",
       "      <th>start_sentence</th>\n",
       "      <th>class</th>\n",
       "      <th>non_noun_tok</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>брюссель</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>–</td>\n",
       "      <td></td>\n",
       "      <td>punct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>data/news1_23.edus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–</td>\n",
       "      <td></td>\n",
       "      <td>punct</td>\n",
       "      <td>1</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>case</td>\n",
       "      <td>этот</td>\n",
       "      <td>PRON</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>в</td>\n",
       "      <td>data/news1_23.edus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>punct</td>\n",
       "      <td>0</td>\n",
       "      <td>однако</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td>всемирный</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>однако</td>\n",
       "      <td>data/news1_23.edus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>punct</td>\n",
       "      <td>1</td>\n",
       "      <td>этот</td>\n",
       "      <td>PRON</td>\n",
       "      <td>det</td>\n",
       "      <td>встреча</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>этот</td>\n",
       "      <td>data/news1_23.edus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>punct</td>\n",
       "      <td>0</td>\n",
       "      <td>который</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>быть</td>\n",
       "      <td>VERB</td>\n",
       "      <td>aux</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>который</td>\n",
       "      <td>data/news1_23.edus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_token  ...            filename\n",
       "0              ...  data/news1_23.edus\n",
       "2           –  ...  data/news1_23.edus\n",
       "10          ,  ...  data/news1_23.edus\n",
       "28          .  ...  data/news1_23.edus\n",
       "31          ,  ...  data/news1_23.edus\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['class'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    366511\n",
       "1     26775\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "embed_model_path='models/w2v/segmentator/model2_tokenized'\n",
    "word2vec_model = Word2Vec.load(embed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeddings(embedder, word):\n",
    "    try:\n",
    "        return embedder[word.lower()]\n",
    "    except KeyError:\n",
    "        return np.zeros(embedder.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_for_embeddings = False\n",
    "\n",
    "if tag_for_embeddings:\n",
    "    train['e_left'] = train.apply(lambda row: get_embeddings(word2vec_model, '_'.join([row.left_token, row.left_pos])), axis=1)\n",
    "else:\n",
    "    train['e_left'] = train.left_token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "    train['e_token'] = train.token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "    train['e_right'] = train.right_token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "    test['e_left'] = test.left_token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "    test['e_token'] = test.token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "    test['e_right'] = test.right_token.map(lambda row: get_embeddings(word2vec_model, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "    \n",
    "#not_categ_features = {'arg_address', 'ex_id', 'rel_pos'}\n",
    "\n",
    "categ_feats = ['left_pos', 'left_link',\n",
    "               'pos', 'link',\n",
    "               'right_pos', 'right_link',\n",
    "               'non_noun_tok'\n",
    "              ]\n",
    "\n",
    "print('Category features:', categ_feats)\n",
    "#print('Not category features:\\n', not_categ)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "vectorizer.fit(train[categ_feats].to_dict(orient='records'))\n",
    "one_hot_feats = vectorizer.transform(train[categ_feats].to_dict(orient='records'))\n",
    "print('shape of one hot transformed features:', one_hot_feats.shape)\n",
    "\n",
    "main_model_path = 'models/segmentator/'\n",
    "! mkdir $main_model_path\n",
    "\n",
    "with open(main_model_path + 'vectorizer.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(main_model_path + 'category_features.pckl', 'wb') as f:\n",
    "    pickle.dump(categ_feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_categ = ['left_token', 'token', 'right_token', 'class', 'e_left', 'e_token', 'e_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test = vectorizer.transform(test[categ_feats].to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input, Dense, concatenate, Conv1D, Conv2D, BatchNormalization, Activation, MaxPooling1D, \\\n",
    "    MaxPooling2D, Dropout, GlobalMaxPool2D, Flatten, Bidirectional, Conv1D, GlobalMaxPool1D, GlobalMaxPooling1D, \\\n",
    "    GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Permute\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')\n",
    "\n",
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_simple_model(input_length, plain_length, output_length):\n",
    "    inner_size = 80\n",
    "    activation='tanh'\n",
    "    dropout = .4\n",
    "    \n",
    "    input_token = Input(shape=(input_length,))\n",
    "    l1 = BatchNormalization()(input_token)\n",
    "    input_left = Input(shape=(input_length,))\n",
    "    l2 = BatchNormalization()(input_left)\n",
    "    input_right = Input(shape=(input_length,))\n",
    "    l3 = BatchNormalization()(input_right)\n",
    "    input_plain = Input(shape=(plain_length,))\n",
    "    l4 = BatchNormalization()(input_plain)\n",
    "    \n",
    "    l4 = Dense(int(inner_size * 10))(l4)\n",
    "    l4 = BatchNormalization()(l4)\n",
    "    l4 = Activation(activation)(l4)\n",
    "    l4 = Dropout(dropout)(l4)\n",
    "    x = concatenate([l1, l2, l3, l4], axis=-1)\n",
    "    x = Dense(120)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_token, input_left, input_right, input_plain], outputs=outputs)\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_left_train = np.stack(train.e_left.values)\n",
    "e_token_train = np.stack(train.e_token.values)\n",
    "e_right_train = np.stack(train.e_right.values)\n",
    "\n",
    "e_left_test = np.stack(test.e_left.values)\n",
    "e_token_test = np.stack(test.e_token.values)\n",
    "e_right_test = np.stack(test.e_right.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train['class'].values)\n",
    "y_test = to_categorical(test['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = construct_simple_model(input_length=e_left_train.shape[1],\n",
    "                               plain_length=one_hot_feats.shape[1],\n",
    "                               output_length=2)\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, \n",
    "                               mode='auto', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x=[e_token_train, e_left_train, e_right_train, one_hot_feats], \n",
    "                    y=y_train, epochs=200, batch_size=512, \n",
    "                    validation_data=([e_token_test, e_left_test, e_right_test, one_hot_test], y_test),\n",
    "                    shuffle=True, callbacks = [early_stopping,],\n",
    "                    class_weight={0:1, 1:14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict([e_token_test, e_left_test, e_right_test, one_hot_test])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_classes = np.argmax(predicted, axis=1)\n",
    "\n",
    "print('pr:', precision_score(test['class'].values, pr_classes))\n",
    "print('re:', recall_score(test['class'].values, pr_classes))\n",
    "print('f1:', f1_score(test['class'].values, pr_classes))\n",
    "print()\n",
    "print(classification_report(y, pr_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! pip install -U imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X = [np.concatenate([e_left_train[i], e_token_train[i], e_right_train[i], one_hot_feats[i]]) \n",
    "     for i in range(len(np.argmax(y_train, axis=1)))]\n",
    "X_res, y_res = sm.fit_resample(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_left_over = [emb[:100] for emb in X_res]\n",
    "e_token_over = [emb[100:200] for emb in X_res]\n",
    "e_right_over = [emb[200:300] for emb in X_res]\n",
    "plain_over = [emb[300:] for emb in X_res]\n",
    "y_over = to_categorical(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = construct_simple_model(input_length=e_left_train.shape[1],\n",
    "                               plain_length=one_hot_test.shape[1],\n",
    "                              output_length=2)\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, \n",
    "                               mode='auto', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x=[e_token_over, e_left_over, e_right_over, plain_over], \n",
    "                    y=y_over, epochs=25, batch_size=512,# validation_split=0.1, \n",
    "                    validation_data=([e_token_test, e_left_test, e_right_test, one_hot_test], y_test),\n",
    "                    #, one_hot_test], y_test),\n",
    "                    shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='models/segmentator/model.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/segmentator/neural_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "import os\n",
    "\n",
    "model = load_model(os.path.join('models', 'segmentator', 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplets_to_text(data):\n",
    "    text = []\n",
    "    for i, row in data.iterrows():\n",
    "        if row['class']:\n",
    "            text.append('\\n')\n",
    "        text.append(row.token)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict([e_token_test, e_left_test, e_right_test, one_hot_test])  \n",
    "pr_classes = np.argmax(predicted, axis=1)\n",
    "result['class'] = pr_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = triplets_to_text(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.temp', 'w') as f:\n",
    "    f.write(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [unit.strip() for unit in plain.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по их мнение ,\n",
      ">>> чувство ненависть\n",
      ">>> и являться основной орудие вербовка .\n",
      ">>> при это отправитель письмо напрямую связывать удар американский беспилотник с серия теракт в париж ,\n",
      ">>> который происходить 13 ноябрь 2015 год .\n",
      ">>> \" мы не мочь спокойно сидеть\n",
      ">>> и наблюдать за такой трагедия , как атака в париж ,\n",
      ">>> знать ,\n",
      ">>> какой разрушительный последствие за рубеж\n",
      ">>> и дома иметь программа использование бпло \" ,\n",
      ">>> - говориться в открытый письмо экс - военный .\n",
      ">>> американский программа опосредовать борьба с терроризм в страна африка и ближний восток с самый начало вызывать критика мировой сообщество в связь с многочисленный нарушение международный норма и неотъемлемый право человек .\n",
      ">>> по официально неподтвержденный данные , до 90 % человек ,\n",
      ">>> убивать\n",
      ">>> в результат атака беспилотник ,\n"
     ]
    }
   ],
   "source": [
    "print('\\n>>> '.join(temp[15:30]))  # predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse plain text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = annot['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(extract_triplets(annot['text'], annot), \n",
    "                      columns=['left_token', 'left_pos', 'left_link', \n",
    "                               'is_title', 'token', 'pos', 'link', \n",
    "                               'right_token', 'right_pos', 'right_link', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_sample = vectorizer.transform(sample[categ_feats].to_dict(orient='records'))\n",
    "print(one_hot_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_for_embeddings = False\n",
    "\n",
    "sample['e_left'] = sample.left_token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "sample['e_token'] = sample.token.map(lambda row: get_embeddings(word2vec_model, row))\n",
    "sample['e_right'] = sample.right_token.map(lambda row: get_embeddings(word2vec_model, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_left = np.stack(sample['e_left'].values)\n",
    "embed_lemma = np.stack(sample['e_token'].values)\n",
    "embed_right = np.stack(sample['e_right'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_categ_columns = np.concatenate(tuple(sample.loc[:, e].values.reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sample[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['e_left'].values.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict([embed_left, embed_lemma, embed_right, one_hot_sample], batch_size=120)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['class'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['class'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['class'] = sample['class'].map(lambda row: row > 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplets_to_text(data):\n",
    "    text = []\n",
    "    for i, row in data.iterrows():\n",
    "        text.append(row.token)\n",
    "        if row['class']:\n",
    "            text.append('\\n')\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = triplets_to_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(plain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
