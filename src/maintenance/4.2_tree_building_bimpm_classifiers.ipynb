{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree building evaluation on gold EDUs (mostly) and playground for tree building scripts\n",
    "\n",
    "1. Modifications of library components for tree building\n",
    "2. Scripts for test and evaluation of Sklearn-, AllenNLP- and gold-annotation-based RST parsers on manually segmented corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation_rst import DiscourseUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(tree):\n",
    "    def _(n):\n",
    "        if n.relation:\n",
    "            value = (n.relation, \"%.2f\"%(n.proba))\n",
    "        else:\n",
    "            value = n.text\n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    return printBTree(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "\n",
    "pr = Predictor.from_path('../isanlp_rst/models/structure_predictor_lstm/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_rst.src.isanlp_rst.sklearn_classifier import SklearnClassifier\n",
    "from isanlp_rst.src.isanlp_rst.allennlp_classifier import AllenNLPClassifier\n",
    "from isanlp_rst.src.isanlp_rst.rst_tree_predictor import *\n",
    "from isanlp_rst.src.isanlp_rst.greedy_rst_parser import GreedyRSTParser\n",
    "from isanlp_rst.src.isanlp_rst.features_extractor import FeaturesExtractor\n",
    "from isanlp_rst.src.isanlp_rst.features_processor_tokenizer import FeaturesProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.5344827586206896,\tin dev: 0.6470588235294118,\tin test: 0.6086956521739131\n",
      "ling in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "comp in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "blog in train: 0.43103448275862066,\tin dev: 0.5294117647058824,\tin test: 0.4782608695652174\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SPAN_PREDICTOR = {\n",
    "    'lstm': (AllenNLPClassifier, 'structure_predictor_lstm', 0.1, 0.5),\n",
    "    'ensemble': (SklearnClassifier, 'structure_predictor', 0.15, 0.2),\n",
    "}\n",
    "\n",
    "_LABEL_PREDICTOR = {\n",
    "    'lstm': (AllenNLPClassifier, 'label_predictor_lstm'),\n",
    "    'ensemble': (SklearnClassifier, 'label_predictor'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier = AllenNLPClassifier('../isanlp_rst/models/structure_predictor_lstm/')\n",
    "label_classifier = AllenNLPClassifier('../isanlp_rst/models/label_predictor_lstm/')\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)\n",
    "features_extractor = FeaturesExtractor(features_processor)\n",
    "\n",
    "predictor = NNTreePredictor(features_processor=features_extractor, \n",
    "                            relation_predictor_sentence=None,\n",
    "                            relation_predictor_text=binary_classifier, \n",
    "                            label_predictor=label_classifier)\n",
    "\n",
    "paragraph_parser = GreedyRSTParser(predictor,\n",
    "                                   confidence_threshold=_SPAN_PREDICTOR['lstm'][2])\n",
    "\n",
    "document_parser = GreedyRSTParser(predictor,\n",
    "                                  confidence_threshold=_SPAN_PREDICTOR['lstm'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_document_parser = GreedyRSTParser(predictor,\n",
    "                                             confidence_threshold=_SPAN_PREDICTOR['lstm'][3]-0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation import Sentence\n",
    "\n",
    "def split_by_paragraphs(annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                        annot_syntax_dep_tree):\n",
    "\n",
    "    def split_on_two(sents, boundary):\n",
    "        list_sum = lambda l: sum([len(sublist) for sublist in l])\n",
    "\n",
    "        i = 1\n",
    "        while list_sum(sents[:i]) < boundary and i < len(sents):\n",
    "            i += 1\n",
    "\n",
    "        intersentence_boundary = min(len(sents[i - 1]), boundary - list_sum(sents[:i - 1]))\n",
    "        return (sents[:i - 1] + [sents[i - 1][:intersentence_boundary]],\n",
    "                [sents[i - 1][intersentence_boundary:]] + sents[i:])\n",
    "\n",
    "    def recount_sentences(chunk):\n",
    "        sentences = []\n",
    "        lemma = []\n",
    "        morph = []\n",
    "        postag = []\n",
    "        syntax_dep_tree = []\n",
    "        tokens_cursor = 0\n",
    "\n",
    "        for i, sent in enumerate(chunk['syntax_dep_tree']):\n",
    "            if len(sent) > 0:\n",
    "                sentences.append(Sentence(tokens_cursor, tokens_cursor + len(sent)))\n",
    "                lemma.append(chunk['lemma'][i])\n",
    "                morph.append(chunk['morph'][i])\n",
    "                postag.append(chunk['postag'][i])\n",
    "                syntax_dep_tree.append(chunk['syntax_dep_tree'][i])\n",
    "                tokens_cursor += len(sent)\n",
    "\n",
    "        chunk['sentences'] = sentences\n",
    "        chunk['lemma'] = lemma\n",
    "        chunk['morph'] = morph\n",
    "        chunk['postag'] = postag\n",
    "        chunk['syntax_dep_tree'] = syntax_dep_tree\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    chunks = []\n",
    "    prev_right_boundary = -1\n",
    "\n",
    "    for i, token in enumerate(annot_tokens[:-1]):\n",
    "\n",
    "        if '\\n' in annot_text[token.end:annot_tokens[i + 1].begin]:\n",
    "            if prev_right_boundary > -1:\n",
    "                chunk = {\n",
    "                    'text': annot_text[annot_tokens[prev_right_boundary].end:token.end + 1].strip(),\n",
    "                    'tokens': annot_tokens[prev_right_boundary + 1:i + 1]\n",
    "                }\n",
    "            else:\n",
    "                chunk = {\n",
    "                    'text': annot_text[:token.end + 1].strip(),\n",
    "                    'tokens': annot_tokens[:i + 1]\n",
    "                }\n",
    "\n",
    "            lemma, annot_lemma = split_on_two(annot_lemma, i - prev_right_boundary)\n",
    "            morph, annot_morph = split_on_two(annot_morph, i - prev_right_boundary)\n",
    "            postag, annot_postag = split_on_two(annot_postag, i - prev_right_boundary)\n",
    "            syntax_dep_tree, annot_syntax_dep_tree = split_on_two(annot_syntax_dep_tree, i - prev_right_boundary)\n",
    "\n",
    "            chunk.update({\n",
    "                'lemma': lemma,\n",
    "                'morph': morph,\n",
    "                'postag': postag,\n",
    "                'syntax_dep_tree': syntax_dep_tree,\n",
    "            })\n",
    "            chunks.append(recount_sentences(chunk))\n",
    "\n",
    "            prev_right_boundary = i  # number of last token in the last chunk\n",
    "\n",
    "    chunk = {\n",
    "        'text': annot_text[annot_tokens[prev_right_boundary].end:].strip(),\n",
    "        'tokens': annot_tokens[prev_right_boundary + 1:],\n",
    "        'lemma': annot_lemma,\n",
    "        'morph': annot_morph,\n",
    "        'postag': annot_postag,\n",
    "        'syntax_dep_tree': annot_syntax_dep_tree,\n",
    "    }\n",
    "\n",
    "    chunks.append(recount_sentences(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Новость: Выставка песчаной скульптуры \"Волшебный песок\"\\nУникальная выставка песчаной скульптуры \"Волшебный песок\" действует на пл.Речников (Речной порт). Для создания скульптур из капризного материала, как песок приехали скульпторы из Архангельска, Санкт-Петербурга, Ижевска, Екатеринбурга, Нижнего Тагила, Латвии,г.Лиепая, так же участвовали скульпторы из Чебоксары. Скульпторы из российских городов, Ижевска, Нижнего Тагила, Санкт-Петербурга, Архангельска, Чебоксар Латвии соорудили настоящие шедевры из песка. Конкуренцию им составили двое местных мастеров, один из которых - организатор выставки Андрей Молоков. И это было только начало. Через несколько дней отдельный конкурс был организован для чебоксарских студентов художественных факультетов и училищ.\\nТема выставки - «Песчаные замки». По условиям конкурса в композиции обязательно должны присутствовать элементы архитектуры. Это может быть что угодно: дома, замки, крепости - всё, что подскажет фантазия автора.\\nПесчаный город состоит из 16 больших композиций высотой 3 метра, и 5 маленьких - двухметровых. Сооружение песчаных скульптур - довольно быстрый процесс: на воздвижение больших отводится 5 дней, маленьких - три.Жюри - местные скульпторы, архитекторы и художники - присудили первое место в конкурсе скульптур присудили Ирине Алимурзаевой из Санкт-Петербурга, её творение называется «Невидимая сила», представляет собой город, над которым «лик» Иисуса Христа. Ирина хочет, чтобы её композиция наталкивала посетителей выставки на философские размышления: «Моя идея заключается в том, что всё в руках Господа ― и архитектура, и жизни людей, потому и название такое - «Невидимая сила»...»\\nВторое место досталось скульптуре Сергея Поташева из Архангельска под названием «Дом ангела». «Композиция соответствует своему названию, имеет художественную ценность, необычна, выполнена мастерски - очень много сложных деталей...», - рассказывает Андрей Молоков.\\nБронзовым призёром стал Айнарас Зигнис из латвийского города Лиепая со своей композицией «Сегодня в городе солнечная погода». В своей песчаной фантазии Айнарас воплотил свои любимые места Европы, где он бывал, запомнившиеся здания. Тут и дома, которые поразили его в Швейцарии, Германии, пражские домики, и центральная церковь латышского города Лиепая...\\nВыставка песчаных скульптур работает ежедневно без выходных с 9 до 22.00, её закрытие планируется лишь в конце сентября. Так что в запасе есть еще достаточно времени, чтобы полюбоваться песчаными шедеврами и, может быть, вдохновиться на создание своего маленького замка из песка. телефон для справок 38-79-17'"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_paragraphs_edus(edus, text):\n",
    "    res = []\n",
    "    parag = []\n",
    "    \n",
    "    for edu in edus:\n",
    "        parag.append(edu)\n",
    "        boundary = text.find(edu)+len(edu)\n",
    "        if boundary < len(text):\n",
    "            if text[boundary] == '\\n':\n",
    "                res.append(parag)\n",
    "                parag = []\n",
    "         \n",
    "    if parag:\n",
    "        res.append(parag)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Новость: Выставка песчаной скульптуры \"Волшебный песок\"'],\n",
       " ['Уникальная выставка песчаной скульптуры \"Волшебный песок\" действует на пл.Речников (Речной порт).',\n",
       "  'Для создания скульптур из капризного материала, как песок',\n",
       "  'приехали скульпторы из Архангельска, Санкт-Петербурга, Ижевска, Екатеринбурга, Нижнего Тагила, Латвии,г.Лиепая,',\n",
       "  'так же участвовали скульпторы из Чебоксары.',\n",
       "  'Скульпторы из российских городов, Ижевска, Нижнего Тагила, Санкт-Петербурга, Архангельска, Чебоксар Латвии соорудили настоящие шедевры из песка.',\n",
       "  'Конкуренцию им составили двое местных мастеров,',\n",
       "  'один из которых - организатор выставки Андрей Молоков.',\n",
       "  'И это было только начало.',\n",
       "  'Через несколько дней отдельный конкурс был организован для чебоксарских студентов художественных факультетов и училищ.'],\n",
       " ['Тема выставки - «Песчаные замки».',\n",
       "  'По условиям конкурса в композиции обязательно должны присутствовать элементы архитектуры.',\n",
       "  'Это может быть что угодно: дома, замки, крепости',\n",
       "  '- всё, что подскажет фантазия автора.'],\n",
       " ['Песчаный город состоит из 16 больших композиций высотой 3 метра, и 5 маленьких - двухметровых.',\n",
       "  'Сооружение песчаных скульптур - довольно быстрый процесс:',\n",
       "  'на воздвижение больших отводится 5 дней, маленьких',\n",
       "  '- три.Жюри - местные скульпторы, архитекторы и художники - присудили первое место в конкурсе скульптур присудили Ирине Алимурзаевой из Санкт-Петербурга,',\n",
       "  'её творение называется «Невидимая сила»,',\n",
       "  'представляет собой город, над которым «лик» Иисуса Христа.',\n",
       "  'Ирина хочет,',\n",
       "  'чтобы её композиция наталкивала посетителей выставки на философские размышления:',\n",
       "  '«Моя идея заключается в том, что всё в руках Господа ― и архитектура, и жизни людей,',\n",
       "  'потому и название такое - «Невидимая сила»...»'],\n",
       " ['Второе место досталось скульптуре Сергея Поташева из Архангельска под названием «Дом ангела».',\n",
       "  '«Композиция соответствует своему названию,',\n",
       "  'имеет художественную ценность,',\n",
       "  'необычна,',\n",
       "  'выполнена мастерски - очень много сложных деталей...»,',\n",
       "  '- рассказывает Андрей Молоков.'],\n",
       " ['Бронзовым призёром стал Айнарас Зигнис из латвийского города Лиепая со своей композицией «Сегодня в городе солнечная погода».',\n",
       "  'В своей песчаной фантазии Айнарас воплотил свои любимые места Европы,',\n",
       "  'где он бывал, запомнившиеся здания.',\n",
       "  'Тут и дома, которые поразили его в Швейцарии, Германии, пражские домики, и центральная церковь латышского города Лиепая...'],\n",
       " ['Выставка песчаных скульптур работает ежедневно без выходных с 9 до 22.00,',\n",
       "  'её закрытие планируется лишь в конце сентября.',\n",
       "  'Так что в запасе есть еще достаточно времени,',\n",
       "  'чтобы полюбоваться песчаными шедеврами и,',\n",
       "  'может быть, вдохновиться на создание своего маленького замка из песка.',\n",
       "  'телефон для справок 38-79-17']]"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_paragraphs_edus(edus, annot['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gold_pairs(gold_pairs):\n",
    "    TARGET = 'category_id'\n",
    "\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace([0.0], 'same-unit_m')\n",
    "    gold_pairs['order'] = gold_pairs['order'].replace([0.0], 'NN')\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace(['antithesis_r',], 'contrast_m')\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace(['cause_r', 'effect_r'], 'cause-effect_r')\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace(['conclusion_r',], 'restatement_m')\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace(['evaluation_r'], 'interpretation-evaluation_r')\n",
    "    gold_pairs[TARGET] = gold_pairs[TARGET].replace(['motivation_r',], 'condition_r')\n",
    "    gold_pairs['relation'] = gold_pairs[TARGET].map(lambda row: row[:-1]) + gold_pairs['order']\n",
    "    gold_pairs['relation'].value_counts()\n",
    "    gold_pairs['relation'] = gold_pairs['relation'].replace(['restatement_SN', 'restatement_NS'], 'restatement_NN')\n",
    "    gold_pairs['relation'] = gold_pairs['relation'].replace(['contrast_SN', 'contrast_NS'], 'contrast_NN')\n",
    "    gold_pairs['relation'] = gold_pairs['relation'].replace(['solutionhood_NS', 'preparation_NS'], 'elaboration_NS')\n",
    "    gold_pairs['relation'] = gold_pairs['relation'].replace(['concession_SN', 'evaluation_SN', \n",
    "                                                             'elaboration_SN', 'evidence_SN'], 'preparation_SN')\n",
    "\n",
    "    _class_mapper = {\n",
    "            'background_NS': 'elaboration_NS',\n",
    "            'background_SN': 'preparation_SN',\n",
    "            'comparison_NN': 'contrast_NN',\n",
    "            'interpretation-evaluation_SN': 'elaboration_NS',\n",
    "            'evidence_NS': 'elaboration_NS',\n",
    "            'restatement_NN': 'joint_NN',\n",
    "            'sequence_NN': 'joint_NN'\n",
    "        }\n",
    "\n",
    "    for key, value in _class_mapper.items():\n",
    "        gold_pairs['relation'] = gold_pairs['relation'].replace(key, value)\n",
    "        \n",
    "    gold_pairs['order'] = gold_pairs['relation'].map(lambda row: row.split('_')[1])\n",
    "    gold_pairs[TARGET] = gold_pairs['relation'].map(lambda row: row.split('_')[0])\n",
    "        \n",
    "    return gold_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://odin-moу-den.livejournal.com/2215985.html\\nВсем здравствуйте!\\nМеня зовут Мария, я живу в Москве, и это один мой рабочий день.\\nВообще-то я собиралась показать вам вчерашний рабочий день, по средам и воскресеньям моя дорога на работу гораздо красочнее, и внутри места, где эта работа проходит, красивее, и движухи больше. Но в среду я напрочь забыла о своем намерении снимать ОМД. Так что показываю место работы, где я бываю по вторникам и четвергам.\\nКак вы уже поняли, будет много текста, и 53 телефонофотографии.\\nГде-то в сообществе есть еще два моих поста, но я не смогла их найти.\\nIMG\\nИтак, 8-55, меня разбудил будильник. Так как легла я оочень для себя поздно, будильник я сегодня ненавижу. Долго крутилась в постели, пытаясь заставить телефон что-нибудь \"увидеть\". В зимних сумерках \"увидеть\" он смог только окно. Вот и оно. На подоконнике махровый плед - котиковое хозяйство. Самого кота там нет. Он в изгнании на кухне.\\nIMG\\nНа окнах сетки \"антикошка\". Из-за этого я вечно чувствую себя в заточении, даже в окно выглядывать не охота.\\nIMG\\nМоя сонная физиономия, грязное зеркало и микрованная (моя боль и печаль). Угу, к концу года я устала, поэтому слегка в миноре.\\nIMG\\nИду в комнату включить комп и впустить кота. Нет, ВПУСТИТЬ кота. Он каждый раз так влетает, будно всю ночь только об этом и мечтал.\\nНет, это не бардак на столе. Бардак на столе был пару недель назад. А сейчас - это так, легкий хаосик: бумаги с работы, мой предновогодний подарок, мобильный и разного по мелочи. Знаете, бывают подарки новогодние, бывают - предновогодние. Первые иногда становятся вторыми, если даритель или одариваемый недотерпел, а бывают специально предновогодними. Вот этот учебник - он как раз такой, я купила его себе в подарок, для приятности.\\nIMG\\nНадо дособирать вещи, позавтракать, понять, кто и во сколько окажется дома вечером, чтобы кормить кота по очереди. А то тяжкое это занятие, знаете ли, кормление котика.\\nСегодня у меня в планах: поработать, сходить на собрание по январскому лагерю, потом к стоматологу, а потом что-нибудь полезное сделать дома.\\nВот мой завтрак. Мне надоела моя любимая овсянка, да и обед сегодня не предвиделся, поэтому я решила позавтракать и за обед заодно. Вареники с картошкой и горячая вода. И утренние дела в компе.\\nIMG\\nПотом я поняла, что время уже не просто выходить, а выпуливаться из дома. Вот я выпулилась: лифт, странное лицо и проч.\\nIMG\\nНедавно тротуар у нашего дома обнесли оградой. Наверняка это был улучшайзинг. Но что-то недодумали, и теперь желающим попасть на тротуар нужно идти по проезжей части. А тем, кто выбрасывал мусор в помойку, идти по дороге в два раза дольше. Ну или лезть через ограду. Хм.\\nIMG\\nЯ уже в метро. Через несколько станций даже села. Достаю свой предновогодний подарок. Я учусь на курсах, у них между ступенями почему-то перерывы по нескольку месяцев. А мне так нравится язык, что прерываться совсем не хочется. Следующий уровень начнется уже в январе, а пока повторяю пройденное.\\nIMG\\nПересела на зеленую ветку, там всегда толпа, так что стоять с толстым учебником не удобно, читаю книжку на телефоне.\\nЧитаю Харрис \"Другой класс\", снова о частной школе. Кажется, это продолжение \"Джентельменов и игроков\". Надо будет вечером вспомнить, погуглить так ли это.\\nIMG\\nВыхожу из метро и вижу хвост моего автобуса. Пробежаться? По льду? Без особого смысла? Конечно, бегу! Ехать мне всего две остановки, так что можно бы и дойти. Но… отчего бы и не пробежаться?\\nВ фонде, где я работаю, есть несколько проектов. Мы работаем со взрослыми с ограниченными возможностями здоровья.\\nИ вот один из проектов, \"День не зря\", проходит на территории ПНИ. Дважды в неделю мы приезжаем: сотрудники и молодые люди-участники проекта, приходят проживающие в интернате, и мы занимаемся физкультурой, играем, гуляем, готовим и едим, варим мыло, валяем или рисуем и шьем всякую красоту. А главное, общаемся и тусим.\\nIMG\\nВремени 11-05. Постепенно народ подтягивается. Мой buddу тоже пришел. Пусть его зовут А. К сожалению, А. сегодня в печали. Тем не менее соглашается зайти в спортзал.\\nIMG\\nПотом как-то все пошло, пошло, пошло, и вот уже пора гулять и идти в нашу комнату, где мы оставляем вещи и идем готовить салат на обед. Наша комната в здании лечебно-трудовых мастерских. Там сейчас меняют окна, поэтому, например, в прошлый раз в коридоре не было стекол и был жуткий дубак. Сегодня стеклопакет уже поставили. Значит, будет тепло, хорошо.\\nIMG\\nПока все готовят, мы с А. пришли побыть в тишине в нашу комнату-мастерскую. Вот, фотографирую ее для вас. Весь декабрь шли новогодние ярмарки, в которых участвовало и наше мыло. Раскупили почти все, осталось, кажется, не больше десяти кусочков, а было несколько больших коробок. Мыловары круты!\\nIMG\\nВ коридоре разруха и ремонт. Стеллажи с бутылками, строительная пыль и неуют. Надеюсь, когда мы вернемся в январе, станет лучше.\\nIMG\\nВо время обеда я внезапно, ага, обедала :) Поэтому фото нет. Но время знаю точно, у нас же расписание. 13-00.\\nА это мы уже с А. носим посуду из столовой обратно в комнату, подмели, и вообще молодцы.\\nIMG\\nВот рабочее расписание А. Некоторые карточки я убрала, сохраняя анонимность молодого человека. По такому или подобному расписанию проходит день у всех.\\nIMG\\nНа сегодня в ЛТМ мы закончили, на очереди сенсорная комната, подведение итогов дня и прощание до следующего года.\\nИду перед выходом в туалет. Тут это целое приключение. В корпусе ЛТМ туалеты общедоступные (к сожалению, во всех смыслах): общие для мужчин и женщин, без кабинок и дверей. Меня это уже который год и печалит и бесит, да все без толку. А после замены деревянных рам на стеклопакеты туалеты стали общедоступными и в плане видимости: затемнения на нижней части стекла нет. Ни в общедоступном туалете проживающих, ни в туалете сотрудников. Мы пользуемся туалетом сотрудников, он запирается. Мама одной из девушек, участвующих в проекте, полдня заклеивала окна скотчем.\\nIMG\\nПотолки в ЛТМ высоченные.\\nIMG\\nМы с А. собрались первыми, вышли. Ждем остальных. А., наконец-то, улыбается, я - радуюсь снегу и улыбке А.\\nIMG\\nИнтернат устроен таким образом, что в любую его точку можно попасть, не выходя на улицу. Система коридоров и переходов. Обычно ей не пользуемся, но сегодня - исключение.\\nIMG\\nВ 15 часов мы заканчиваем, за А. пришли мама и бабушка, он радостно машет мне рукой и готов идти домой. Я даю часть карточек маме А., чтобы они пробовали вводить альтернативную коммуникацию дома, и мы прощаемся.\\nНам нужно очень быстро распрощаться со всеми, и продолжить собрание по лагерю, начатое еще во время сенсорной комнаты.\\nВот, встретили по пути борзую. В махровых тапочках. Его милая хозяйка в ответ на наши восторги стала подробно рассказывать о псе. Говорит, по всей Москве всего 100 представителей этой породы.\\nIMG\\nСпешим мы в наше \"совещательное\" кафе. Мы там всегда собираемся на педсоветы и просто что-то рабочее обсудить. Там очень уютно.\\nIMG\\nС нами пошли две студентки. Они в лагерь не едут, но, видимо, им интересно послушать. Итого, нас шестеро. Кроме нас в зале никого.\\nОбсуждаем лагерь: расписание, занятость, кто за что отвечает. Сегодня мы потрясающе сконцентрированы и почти не ржем просто так. Сами себе удивляемся.\\nIMG\\nЯ должна уйти в 16-20, т.к. у меня в 17-20 стоматолог на другом конце света города. ЕМИАС внезапно меня отправил в институт челюстно-лицевой хирургии при попытке просто записаться к стоматологу. Я не могу пропустить это.\\nНа первой фоторафии время 16-10 и я собираюсь съесть последнюю печеньку, задать последний вопрос и отчалить. Ровно в момент фотографирования коллега протягивает руку и забирает печеньку. На второй фотографии - я очнулась, на часах 16-52.\\nIMG\\nПонятно, что ни к какому врачу я уже не успеваю. Остаюсь обсуждать январский лагерь дальше. Обсуждение активное, по делу, все как я люблю:)\\nНет, я не какой-то специальный любитель туалетофоток. Просто так совпало:) Таксы, и я все в той же позе. Не знаю как сделать селфи, чтобы лицо не приобретало ложного налета героизма.\\nIMG\\nМы договорили, дошли до метро и… я дома.\\nIMG\\nПереодеться, умыться и навестить холодильник - святое. А в холодильнике - намек.\\nIMG\\nАга, помню, обещала. Но так лень. Но обещала. Да и самой супа хочется. Но лень. Пойду пошатаюсь по квартире, подумаю.\\nНа кухне обнаруживается кот в ошметках собственной когтеточки. Не скучал, значит. И еще одно доказательство нескучания - мышь кота, застрявшая под дверью в спальню.\\nIMG\\nЭх, ну суп или сейчас делать, или уже забить. На часах почти восемь. Достаю овощи и специи, достаю тетардь с рецептом…\\nИ тут я смотрю на часы и понимаю, что если я не хочу сама завтра тащиться в неведомую даль, то нужно отправить в эту даль другого сделать заказ доброму человеку прямо сейчас. Дело в том, что я в лагере еще и в качестве медсестры буду (заодно и образование проветрю), и мне нужно собрать аптечку с собой. Часть лекарств у нас есть, а часть - надо докупить. Вот добрый человек и докупит по списку :)\\nIMG\\nПолчаса, и список медикаментов составлен, обещание сварить суп выполнено, а у меня есть ужин. Осталось только копченый сыр порезать. В суп, ага.\\nIMG\\nВот мое место вечернего заседания. Стремного вида покрывало на диване - от кота, точнее от его когтей. Сам диван из жесткого материала, о который очень круто точить когти. Поэтому приходится диван прятать под неинтересной коту хб-шкой. Когда-нибудь я найду ткань такого же идеального терракотового цвета, как и сам диван, и сошью из нее чехол. Пока не нашла. Но я все еще надеюсь.\\nIMG\\nНад местом моего вечернего заседания - моя любимая картина. Увидела ее на весеннем базарчике в Вильнюсе. Несколько раз уходила, возвращалась, в итоге не смогла не купить.\\nIMG\\nОткрываю рабочую почту, беру тарелку с супом, и разгребаю последние завалы в этом году. У меня еще будет рабочий день - в субботу, 29-го. А потом я буду мужественно НЕ читать почту и мессенджеры. По ходу дела кормлю кота. Кот дик и неподкупен, поэтому фото \"человек и кот в одном кадре\" невозможно, зато кота можно подманить, замерев. Тогда он забывает про тебя, и его можно рассмотреть. Недолго))\\nВремени около половины десятого.\\nIMG\\nПотом я встречала хорошего человека с сумкой таблеток :) И внезапно обнаружила, что уже час ночи. Хм, как так?\\nВот мое новое супер мягкое и теплое постельное белье из байки. На картинке в интернете рисунок выглядел менее… ммм… жизнерадостным. А когда заказ привезли, нам потребовалось несколько минут, чтобы понять, что рисунок тот же самый, просто впечатляет сильнее:)\\nIMG\\nТо же самое и с котом. По рассказам передержки это должен был быть тихий, самодостаточный кот, с которым можно и поиграть и не_поиграть. В реальности кот оказался драматическим актером в амплуа неврастеника, но другого кота у меня нет. Люблю того, который есть :)\\nВот он пришел меня укладывать спать. Телепатически. Люблю, когда он так приходит.\\nСпокойной ночи!\\n'"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711401b2543430d9b88c09fdb2d1c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_x'] = df.snippet_x.map(self.annot_text.find)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_y'] = df.apply(lambda row: self._find_y(row.snippet_x, row.snippet_y, row.loc_x), axis=1)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_x'] = df.loc_x.map(self.locate_token)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_y'] = df.loc_y.map(self.locate_token)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_x'] = df.snippet_x.map(self.annot_text.find)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_y'] = df.apply(lambda row: self._find_y(row.snippet_x, row.snippet_y, row.loc_x), axis=1)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_x'] = df.loc_x.map(self.locate_token)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_y'] = df.loc_y.map(self.locate_token)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_x'] = df.snippet_x.map(self.annot_text.find)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['loc_y'] = df.apply(lambda row: self._find_y(row.snippet_x, row.snippet_y, row.loc_x), axis=1)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_x'] = df.loc_x.map(self.locate_token)\n",
      "/notebook/isanlp_rst/src/isanlp_rst/features_processor_tokenizer.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['token_begin_y'] = df.loc_y.map(self.locate_token)\n",
      "Unable to locate first snippet >>> [['Заниматься фитнесом нужно хотя бы трижды в неделю по 45-60 минут.'\n",
      "  'Перед занятием обязательно надо разогреть все мышцы,' 7 7 1342 1408]\n",
      " ['Перед занятием обязательно надо разогреть все мышцы,'\n",
      "  'например, потанцевав несколько минут.' 7 7 1408 1461]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-735-fb577fbd198a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             trees = paragraph_parser(_edus,\n\u001b[1;32m     59\u001b[0m                 \u001b[0mannot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 chunk['morph'], chunk['postag'], chunk['syntax_dep_tree'])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mdus\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/isanlp_rst/src/isanlp_rst/greedy_rst_parser.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, edus, annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag, annot_syntax_dep_tree, genre)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# make the new node by merging node[j] + node[j+1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mrelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuclearity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             temp = DiscourseUnit(\n",
      "\u001b[0;32m/notebook/isanlp_rst/src/isanlp_rst/rst_tree_predictor.py\u001b[0m in \u001b[0;36mpredict_label\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             result = self.label_predictor.predict(features.loc['snippet_x'],\n\u001b[0;32m--> 260\u001b[0;31m                                                   features.loc['snippet_y'])\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/isanlp_rst/src/isanlp_rst/allennlp_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, snippet_x, snippet_y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnippet_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnippet_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         return self._model.predict(self._prepare_sequence(snippet_x, is_left_snippet=True),\n\u001b[0;32m---> 75\u001b[0;31m                                    self._prepare_sequence(snippet_y, is_left_snippet=False))['label']\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnippet_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnippet_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/predictors/decomposable_attention.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mentailment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontradiction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneutral\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"premise\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hypothesis\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjson_to_labeled_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensors\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minto\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     def forward_on_instances(self,\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minstance_separated_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/models/bimpm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis, label, metadata)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# embedding and encoding of the premise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0membedded_premise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mencoded_premise1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_premise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mencoded_premise2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_premise1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;31m# is bijective and just use the key directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_field_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mtoken_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0membedded_representations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, word_inputs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0melmo_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0melmo_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elmo_representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, word_inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# run the biLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mbilm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_word_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mmask_with_bos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, word_inputs)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mtype_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mtoken_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mtype_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beginning_of_sentence_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_of_sentence_characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/nn/util.py\u001b[0m in \u001b[0;36madd_sentence_boundary_token_ids\u001b[0;34m(tensor, mask, sentence_begin_token, sentence_end_token)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \"\"\"\n\u001b[1;32m   1287\u001b[0m     \u001b[0;31m# TODO: matthewp, profile this transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0msequence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m     \u001b[0mtensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "\n",
    "\n",
    "broken_files = []\n",
    "smallest_file = 'data/news2_4.edus'\n",
    "coolest_file = 'data/blogs_17.edus'\n",
    "#test[:1]\n",
    "for file in tqdm(test):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    #gold = read_gold(filename)\n",
    "    gold = prepare_gold_pairs(read_gold(filename, features=True))\n",
    "    \n",
    "    annot = read_annotation(filename)\n",
    "    annot['text'] = annot['text'].replace('\\nIMG', ' IMG')\n",
    "    \n",
    "    if '\\n' in annot['text']:\n",
    "        chunks = split_by_paragraphs(\n",
    "            annot['text'],\n",
    "            annot['tokens'], \n",
    "            annot['sentences'], \n",
    "            annot['lemma'], \n",
    "            annot['morph'], \n",
    "            annot['postag'], \n",
    "            annot['syntax_dep_tree'])\n",
    "        \n",
    "        chunked_edus = split_by_paragraphs_edus(edus, annot['text'])\n",
    "    \n",
    "    dus = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        _edus = []\n",
    "        last_end = 0\n",
    "        \n",
    "        for max_id in range(len(chunked_edus[i])):\n",
    "            start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(chunked_edus[i][max_id])\n",
    "            end = start + len(chunked_edus[i][max_id])\n",
    "            temp = DiscourseUnit(\n",
    "                    id=max_id,\n",
    "                    left=None,\n",
    "                    right=None,\n",
    "                    relation='edu',\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    orig_text=annot['text'],\n",
    "                    proba=1.,\n",
    "                )\n",
    "\n",
    "            _edus.append(temp)\n",
    "            last_end = end + 1\n",
    "            \n",
    "        if len(_edus) == 1:\n",
    "            dus += _edus\n",
    "            start_id = _edus[-1].id + 1\n",
    "\n",
    "        elif len(_edus) > 1:\n",
    "            trees = paragraph_parser(_edus,\n",
    "                annot['text'], chunk['tokens'], chunk['sentences'], chunk['lemma'],\n",
    "                chunk['morph'], chunk['postag'], chunk['syntax_dep_tree'])\n",
    "            \n",
    "            dus += trees\n",
    "#             print('::: chunk processed :::')\n",
    "#             print(dus[-1].text)\n",
    "            start_id = dus[-1].id + 1\n",
    "        \n",
    "    parsed = document_parser(\n",
    "                dus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['lemma'], \n",
    "                annot['morph'], \n",
    "                annot['postag'], \n",
    "                annot['syntax_dep_tree'],\n",
    "                genre=filename.split('_')[0])\n",
    "    \n",
    "    if len(parsed) > len(annot['text']) // 400:\n",
    "        parsed = additional_document_parser(\n",
    "            parsed, \n",
    "            annot['text'], \n",
    "            annot['tokens'], \n",
    "            annot['sentences'], \n",
    "            annot['lemma'], \n",
    "            annot['morph'], \n",
    "            annot['postag'], \n",
    "            annot['syntax_dep_tree'],\n",
    "            genre=filename.split('_')[0]\n",
    "        )\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed, annot['text']), \n",
    "                                columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "    evaluation = eval_pipeline(parsed, edus, gold, annot['text'])\n",
    "    evaluation['filename'] = file\n",
    "    cache.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/blogs_31'"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://odin-moу-den.livejournal.com/2215985.html',\n",
       " 'Всем здравствуйте!',\n",
       " 'Меня зовут Мария,',\n",
       " 'я живу в Москве,',\n",
       " 'и это один мой рабочий день.',\n",
       " 'Вообще-то я собиралась показать вам вчерашний рабочий день,',\n",
       " 'по средам и воскресеньям моя дорога на работу гораздо красочнее,',\n",
       " 'и внутри места, где эта работа проходит, красивее,',\n",
       " 'и движухи больше.',\n",
       " 'Но в среду я напрочь забыла о своем намерении снимать ОМД.',\n",
       " 'Так что показываю место работы,',\n",
       " 'где я бываю по вторникам и четвергам.',\n",
       " 'Как вы уже поняли, будет много текста, и 53 телефонофотографии.',\n",
       " 'Где-то в сообществе есть еще два моих поста,',\n",
       " 'но я не смогла их найти. IMG',\n",
       " 'Итак, 8-55, меня разбудил будильник.',\n",
       " 'Так как легла я оочень для себя поздно,',\n",
       " 'будильник я сегодня ненавижу.',\n",
       " 'Долго крутилась в постели,',\n",
       " 'пытаясь заставить телефон что-нибудь \"увидеть\".',\n",
       " 'В зимних сумерках \"увидеть\" он смог только окно. Вот и оно.',\n",
       " 'На подоконнике махровый плед - котиковое хозяйство.',\n",
       " 'Самого кота там нет.',\n",
       " 'Он в изгнании на кухне. IMG',\n",
       " 'На окнах сетки \"антикошка\".',\n",
       " 'Из-за этого я вечно чувствую себя в заточении,',\n",
       " 'даже в окно выглядывать не охота. IMG',\n",
       " 'Моя сонная физиономия, грязное зеркало и микрованная (моя боль и печаль).',\n",
       " 'Угу, к концу года я устала,',\n",
       " 'поэтому слегка в миноре. IMG',\n",
       " 'Иду в комнату включить комп',\n",
       " 'и впустить кота.',\n",
       " 'Нет, ВПУСТИТЬ кота.',\n",
       " 'Он каждый раз так влетает,',\n",
       " 'будно всю ночь только об этом и мечтал.',\n",
       " 'Нет, это не бардак на столе.',\n",
       " 'Бардак на столе был пару недель назад.',\n",
       " 'А сейчас - это так, легкий хаосик: бумаги с работы, мой предновогодний подарок, мобильный и разного по мелочи.',\n",
       " 'Знаете, бывают подарки новогодние,',\n",
       " 'бывают - предновогодние.',\n",
       " 'Первые иногда становятся вторыми,',\n",
       " 'если даритель или одариваемый недотерпел,',\n",
       " 'а бывают специально предновогодними.',\n",
       " 'Вот этот учебник - он как раз такой,',\n",
       " 'я купила его себе в подарок,',\n",
       " 'для приятности.',\n",
       " 'IMG',\n",
       " 'Надо дособирать вещи,',\n",
       " 'позавтракать,',\n",
       " 'понять, кто и во сколько окажется дома вечером,',\n",
       " 'чтобы кормить кота по очереди.',\n",
       " 'А то тяжкое это занятие, знаете ли, кормление котика.',\n",
       " 'Сегодня у меня в планах: поработать,',\n",
       " 'сходить на собрание по январскому лагерю,',\n",
       " 'потом к стоматологу,',\n",
       " 'а потом что-нибудь полезное сделать дома.',\n",
       " 'Вот мой завтрак.',\n",
       " 'Мне надоела моя любимая овсянка,',\n",
       " 'да и обед сегодня не предвиделся,',\n",
       " 'поэтому я решила позавтракать и за обед заодно.',\n",
       " 'Вареники с картошкой и горячая вода.',\n",
       " 'И утренние дела в компе. IMG',\n",
       " 'Потом я поняла, что время уже не просто выходить,',\n",
       " 'а выпуливаться из дома.',\n",
       " 'Вот я выпулилась: лифт, странное лицо и проч. IMG',\n",
       " 'Недавно тротуар у нашего дома обнесли оградой.',\n",
       " 'Наверняка это был улучшайзинг.',\n",
       " 'Но что-то недодумали,',\n",
       " 'и теперь желающим попасть на тротуар нужно идти по проезжей части.',\n",
       " 'А тем, кто выбрасывал мусор в помойку,',\n",
       " 'идти по дороге в два раза дольше.',\n",
       " 'Ну или лезть через ограду. Хм. IMG',\n",
       " 'Я уже в метро.',\n",
       " 'Через несколько станций даже села.',\n",
       " 'Достаю свой предновогодний подарок.',\n",
       " 'Я учусь на курсах,',\n",
       " 'у них между ступенями почему-то перерывы по нескольку месяцев.',\n",
       " 'А мне так нравится язык,',\n",
       " 'что прерываться совсем не хочется.',\n",
       " 'Следующий уровень начнется уже в январе,',\n",
       " 'а пока повторяю пройденное. IMG',\n",
       " 'Пересела на зеленую ветку,',\n",
       " 'там всегда толпа,',\n",
       " 'так что стоять с толстым учебником не удобно,',\n",
       " 'читаю книжку на телефоне.',\n",
       " 'Читаю Харрис \"Другой класс\", снова о частной школе.',\n",
       " 'Кажется, это продолжение \"Джентельменов и игроков\".',\n",
       " 'Надо будет вечером вспомнить, погуглить так ли это. IMG',\n",
       " 'Выхожу из метро',\n",
       " 'и вижу хвост моего автобуса.',\n",
       " 'Пробежаться? По льду? Без особого смысла?',\n",
       " 'Конечно, бегу!',\n",
       " 'Ехать мне всего две остановки,',\n",
       " 'так что можно бы и дойти.',\n",
       " 'Но… отчего бы и не пробежаться?',\n",
       " 'В фонде, где я работаю, есть несколько проектов.',\n",
       " 'Мы работаем со взрослыми с ограниченными возможностями здоровья.',\n",
       " 'И вот один из проектов, \"День не зря\", проходит на территории ПНИ.',\n",
       " 'Дважды в неделю мы приезжаем:',\n",
       " 'сотрудники и молодые люди-участники проекта, приходят проживающие в интернате,',\n",
       " 'и мы занимаемся физкультурой,',\n",
       " 'играем,',\n",
       " 'гуляем,',\n",
       " 'готовим',\n",
       " 'и едим,',\n",
       " 'варим мыло,',\n",
       " 'валяем',\n",
       " 'или рисуем',\n",
       " 'и шьем всякую красоту.',\n",
       " 'А главное, общаемся',\n",
       " 'и тусим.',\n",
       " 'IMG',\n",
       " 'Времени 11-05.',\n",
       " 'Постепенно народ подтягивается.',\n",
       " 'Мой buddу тоже пришел.',\n",
       " 'Пусть его зовут А.',\n",
       " 'К сожалению, А. сегодня в печали.',\n",
       " 'Тем не менее соглашается зайти в спортзал. IMG',\n",
       " 'Потом как-то все пошло, пошло, пошло, и вот уже пора гулять',\n",
       " 'и идти в нашу комнату,',\n",
       " 'где мы оставляем вещи',\n",
       " 'и идем готовить салат на обед.',\n",
       " 'Наша комната в здании лечебно-трудовых мастерских.',\n",
       " 'Там сейчас меняют окна,',\n",
       " 'поэтому, например, в прошлый раз в коридоре не было стекол',\n",
       " 'и был жуткий дубак.',\n",
       " 'Сегодня стеклопакет уже поставили.',\n",
       " 'Значит, будет тепло, хорошо. IMG',\n",
       " 'Пока все готовят,',\n",
       " 'мы с А. пришли побыть в тишине в нашу комнату-мастерскую.',\n",
       " 'Вот, фотографирую ее для вас.',\n",
       " 'Весь декабрь шли новогодние ярмарки, в которых участвовало и наше мыло.',\n",
       " 'Раскупили почти все,',\n",
       " 'осталось, кажется, не больше десяти кусочков,',\n",
       " 'а было несколько больших коробок.',\n",
       " 'Мыловары круты! IMG',\n",
       " 'В коридоре разруха и ремонт.',\n",
       " 'Стеллажи с бутылками, строительная пыль и неуют.',\n",
       " 'Надеюсь,',\n",
       " 'когда мы вернемся в январе,',\n",
       " 'станет лучше. IMG',\n",
       " 'Во время обеда я внезапно, ага, обедала :)',\n",
       " 'Поэтому фото нет.',\n",
       " 'Но время знаю точно,',\n",
       " 'у нас же расписание. 13-00.',\n",
       " 'А это мы уже с А. носим посуду из столовой обратно в комнату,',\n",
       " 'подмели,',\n",
       " 'и вообще молодцы. IMG',\n",
       " 'Вот рабочее расписание А.',\n",
       " 'Некоторые карточки я убрала,',\n",
       " 'сохраняя анонимность молодого человека.',\n",
       " 'По такому или подобному расписанию проходит день у всех. IMG',\n",
       " 'На сегодня в ЛТМ мы закончили, на очереди сенсорная комната, подведение итогов дня и прощание до следующего года.',\n",
       " 'Иду перед выходом в туалет.',\n",
       " 'Тут это целое приключение.',\n",
       " 'В корпусе ЛТМ туалеты общедоступные (к сожалению, во всех смыслах): общие для мужчин и женщин, без кабинок и дверей.',\n",
       " 'Меня это уже который год и печалит',\n",
       " 'и бесит, да все без толку.',\n",
       " 'А после замены деревянных рам на стеклопакеты туалеты стали общедоступными и в плане видимости:',\n",
       " 'затемнения на нижней части стекла нет. Ни в общедоступном туалете проживающих, ни в туалете сотрудников.',\n",
       " 'Мы пользуемся туалетом сотрудников,',\n",
       " 'он запирается.',\n",
       " 'Мама одной из девушек, участвующих в проекте, полдня заклеивала окна скотчем. IMG',\n",
       " 'Потолки в ЛТМ высоченные. IMG',\n",
       " 'Мы с А. собрались первыми,',\n",
       " 'вышли.',\n",
       " 'Ждем остальных.',\n",
       " 'А., наконец-то, улыбается,',\n",
       " 'я - радуюсь снегу и улыбке А. IMG',\n",
       " 'Интернат устроен таким образом,',\n",
       " 'что в любую его точку можно попасть,',\n",
       " 'не выходя на улицу.',\n",
       " 'Система коридоров и переходов.',\n",
       " 'Обычно ей не пользуемся, но сегодня - исключение. IMG',\n",
       " 'В 15 часов мы заканчиваем,',\n",
       " 'за А. пришли мама и бабушка,',\n",
       " 'он радостно машет мне рукой',\n",
       " 'и готов идти домой.',\n",
       " 'Я даю часть карточек маме А.,',\n",
       " 'чтобы они пробовали вводить альтернативную коммуникацию дома,',\n",
       " 'и мы прощаемся.',\n",
       " 'Нам нужно очень быстро распрощаться со всеми,',\n",
       " 'и продолжить собрание по лагерю,',\n",
       " 'начатое еще во время сенсорной комнаты.',\n",
       " 'Вот, встретили по пути борзую. В махровых тапочках.',\n",
       " 'Его милая хозяйка в ответ на наши восторги стала подробно рассказывать о псе.',\n",
       " 'Говорит, по всей Москве всего 100 представителей этой породы. IMG',\n",
       " 'Спешим мы в наше \"совещательное\" кафе.',\n",
       " 'Мы там всегда собираемся на педсоветы и просто что-то рабочее обсудить.',\n",
       " 'Там очень уютно. IMG',\n",
       " 'С нами пошли две студентки.',\n",
       " 'Они в лагерь не едут,',\n",
       " 'но, видимо, им интересно послушать.',\n",
       " 'Итого, нас шестеро.',\n",
       " 'Кроме нас в зале никого.',\n",
       " 'Обсуждаем лагерь: расписание, занятость,',\n",
       " 'кто за что отвечает.',\n",
       " 'Сегодня мы потрясающе сконцентрированы',\n",
       " 'и почти не ржем просто так.',\n",
       " 'Сами себе удивляемся. IMG',\n",
       " 'Я должна уйти в 16-20,',\n",
       " 'т.к. у меня в 17-20 стоматолог на другом конце света города.',\n",
       " 'ЕМИАС внезапно меня отправил в институт челюстно-лицевой хирургии',\n",
       " 'при попытке просто записаться к стоматологу.',\n",
       " 'Я не могу пропустить это.',\n",
       " 'На первой фоторафии время 16-10',\n",
       " 'и я собираюсь съесть последнюю печеньку,',\n",
       " 'задать последний вопрос',\n",
       " 'и отчалить.',\n",
       " 'Ровно в момент фотографирования коллега протягивает руку',\n",
       " 'и забирает печеньку.',\n",
       " 'На второй фотографии - я очнулась,',\n",
       " 'на часах 16-52. IMG',\n",
       " 'Понятно, что ни к какому врачу я уже не успеваю.',\n",
       " 'Остаюсь обсуждать январский лагерь дальше.',\n",
       " 'Обсуждение активное, по делу,',\n",
       " 'все как я люблю:)',\n",
       " 'Нет, я не какой-то специальный любитель туалетофоток.',\n",
       " 'Просто так совпало:)',\n",
       " 'Таксы, и я все в той же позе.',\n",
       " 'Не знаю как сделать селфи,',\n",
       " 'чтобы лицо не приобретало ложного налета героизма. IMG',\n",
       " 'Мы договорили,',\n",
       " 'дошли до метро',\n",
       " 'и… я дома. IMG',\n",
       " 'Переодеться,',\n",
       " 'умыться',\n",
       " 'и навестить холодильник - святое.',\n",
       " 'А в холодильнике - намек. IMG',\n",
       " 'Ага, помню, обещала.',\n",
       " 'Но так лень.',\n",
       " 'Но обещала.',\n",
       " 'Да и самой супа хочется.',\n",
       " 'Но лень.',\n",
       " 'Пойду пошатаюсь по квартире, подумаю.',\n",
       " 'На кухне обнаруживается кот в ошметках собственной когтеточки.',\n",
       " 'Не скучал, значит.',\n",
       " 'И еще одно доказательство нескучания - мышь кота, застрявшая под дверью в спальню. IMG',\n",
       " 'Эх, ну суп или сейчас делать,',\n",
       " 'или уже забить.',\n",
       " 'На часах почти восемь.',\n",
       " 'Достаю овощи и специи, достаю тетардь с рецептом…',\n",
       " 'И тут я смотрю на часы',\n",
       " 'и понимаю,',\n",
       " 'что если я не хочу сама завтра тащиться в неведомую даль,',\n",
       " 'то нужно отправить в эту даль другого сделать заказ доброму человеку прямо сейчас.',\n",
       " 'Дело в том, что я в лагере еще и в качестве медсестры буду',\n",
       " '(заодно и образование проветрю),',\n",
       " 'и мне нужно собрать аптечку с собой.',\n",
       " 'Часть лекарств у нас есть,',\n",
       " 'а часть - надо докупить.',\n",
       " 'Вот добрый человек и докупит по списку :) IMG',\n",
       " 'Полчаса, и список медикаментов составлен,',\n",
       " 'обещание сварить суп выполнено,',\n",
       " 'а у меня есть ужин.',\n",
       " 'Осталось только копченый сыр порезать. В суп, ага. IMG',\n",
       " 'Вот мое место вечернего заседания.',\n",
       " 'Стремного вида покрывало на диване - от кота, точнее от его когтей.',\n",
       " 'Сам диван из жесткого материала, о который очень круто точить когти.',\n",
       " 'Поэтому приходится диван прятать под неинтересной коту хб-шкой.',\n",
       " 'Когда-нибудь я найду ткань такого же идеального терракотового цвета, как и сам диван,',\n",
       " 'и сошью из нее чехол.',\n",
       " 'Пока не нашла.',\n",
       " 'Но я все еще надеюсь. IMG',\n",
       " 'Над местом моего вечернего заседания - моя любимая картина.',\n",
       " 'Увидела ее на весеннем базарчике в Вильнюсе.',\n",
       " 'Несколько раз уходила,',\n",
       " 'возвращалась,',\n",
       " 'в итоге не смогла не купить. IMG',\n",
       " 'Открываю рабочую почту,',\n",
       " 'беру тарелку с супом,',\n",
       " 'и разгребаю последние завалы в этом году.',\n",
       " 'У меня еще будет рабочий день - в субботу, 29-го.',\n",
       " 'А потом я буду мужественно НЕ читать почту и мессенджеры.',\n",
       " 'По ходу дела кормлю кота.',\n",
       " 'Кот дик',\n",
       " 'и неподкупен,',\n",
       " 'поэтому фото \"человек и кот в одном кадре\" невозможно,',\n",
       " 'зато кота можно подманить,',\n",
       " 'замерев.',\n",
       " 'Тогда он забывает про тебя,',\n",
       " 'и его можно рассмотреть. Недолго))',\n",
       " 'Времени около половины десятого. IMG',\n",
       " 'Потом я встречала хорошего человека с сумкой таблеток :)',\n",
       " 'И внезапно обнаружила, что уже час ночи.',\n",
       " 'Хм, как так?',\n",
       " 'Вот мое новое супер мягкое и теплое постельное белье из байки.',\n",
       " 'На картинке в интернете рисунок выглядел менее… ммм… жизнерадостным.',\n",
       " 'А когда заказ привезли,',\n",
       " 'нам потребовалось несколько минут,',\n",
       " 'чтобы понять, что рисунок тот же самый,',\n",
       " 'просто впечатляет сильнее:) IMG',\n",
       " 'То же самое и с котом.',\n",
       " 'По рассказам передержки это должен был быть тихий, самодостаточный кот,',\n",
       " 'с которым можно и поиграть',\n",
       " 'и не_поиграть.',\n",
       " 'В реальности кот оказался драматическим актером в амплуа неврастеника,',\n",
       " 'но другого кота у меня нет.',\n",
       " 'Люблю того, который есть :)',\n",
       " 'Вот он пришел меня укладывать спать. Телепатически.',\n",
       " 'Люблю, когда он так приходит.',\n",
       " 'Спокойной ночи!']"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Где',\n",
       " '-',\n",
       " 'то',\n",
       " 'в',\n",
       " 'сообществе',\n",
       " 'есть',\n",
       " 'еще',\n",
       " 'два',\n",
       " 'моих',\n",
       " 'поста',\n",
       " ',',\n",
       " 'но',\n",
       " 'я',\n",
       " 'не',\n",
       " 'смогла',\n",
       " 'их',\n",
       " 'найти',\n",
       " '.']"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.text for tok in chunks[5]['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Где-то в сообществе есть еще два моих поста,',\n",
       " 'но я не смогла их найти. IMG',\n",
       " 'Итак, 8-55, меня разбудил будильник.',\n",
       " 'Так как легла я оочень для себя поздно,',\n",
       " 'будильник я сегодня ненавижу.',\n",
       " 'Долго крутилась в постели,',\n",
       " 'пытаясь заставить телефон что-нибудь \"увидеть\".',\n",
       " 'В зимних сумерках \"увидеть\" он смог только окно. Вот и оно.',\n",
       " 'На подоконнике махровый плед - котиковое хозяйство.',\n",
       " 'Самого кота там нет.',\n",
       " 'Он в изгнании на кухне. IMG',\n",
       " 'На окнах сетки \"антикошка\".',\n",
       " 'Из-за этого я вечно чувствую себя в заточении,',\n",
       " 'даже в окно выглядывать не охота. IMG',\n",
       " 'Моя сонная физиономия, грязное зеркало и микрованная (моя боль и печаль).',\n",
       " 'Угу, к концу года я устала,',\n",
       " 'поэтому слегка в миноре. IMG',\n",
       " 'Иду в комнату включить комп',\n",
       " 'и впустить кота.',\n",
       " 'Нет, ВПУСТИТЬ кота.',\n",
       " 'Он каждый раз так влетает,',\n",
       " 'будно всю ночь только об этом и мечтал.']"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_edus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет, это не бардак на столе.\n",
      "1313\n",
      "Бардак на столе был пару недель назад.\n",
      "1342\n",
      "А сейчас - это так, легкий хаосик: бумаги с работы, мой предновогодний подарок, мобильный и разного по мелочи.\n",
      "1381\n",
      "Знаете, бывают подарки новогодние,\n",
      "1492\n",
      "бывают - предновогодние.\n",
      "1527\n",
      "Первые иногда становятся вторыми,\n",
      "1552\n",
      "если даритель или одариваемый недотерпел,\n",
      "1586\n",
      "а бывают специально предновогодними.\n",
      "1628\n",
      "Вот этот учебник - он как раз такой,\n",
      "1665\n",
      "я купила его себе в подарок,\n",
      "1702\n",
      "для приятности.\n",
      "1731\n"
     ]
    }
   ],
   "source": [
    "for edu in _edus:\n",
    "    print(edu.text)\n",
    "    print(annot['text'].find(edu.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/blogs_21'"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> edu_\n",
      ">>> edu_\n",
      ">>> joint_NN\n",
      "Меня зовут Мария, я живу в Москве, и это один мой рабочий день.\n",
      ">>> contrast_NN\n",
      "Вообще-то я собиралась показать вам вчерашний рабочий день, по средам и воскресеньям моя дорога на работу гораздо красочнее, и внутри места, где эта работа проходит, красивее, и движухи больше. Но в среду я напрочь забыла о своем намерении снимать ОМД. Так что показываю место работы, где я бываю по вторникам и четвергам.\n",
      ">>> edu_\n",
      ">>> elaboration_NS\n",
      "Где-то в сообществе есть еще два моих поста, но я не смогла их найти.\n",
      "IMG\n",
      "Итак, 8-55, меня разбудил будильник. Так как легла я оочень для себя поздно, будильник я сегодня ненавижу. Долго крутилась в постели, пытаясь заставить телефон что-нибудь \"увидеть\". В зимних сумерках \"увидеть\" он смог только окно. Вот и оно. На подоконнике махровый плед - котиковое хозяйство. Самого кота там нет. Он в изгнании на кухне.\n",
      "IMG\n",
      "На окнах сетки \"антикошка\". Из-за этого я вечно чувствую себя в заточении, даже в окно выглядывать не охота.\n",
      "IMG\n",
      "Моя сонная физиономия, грязное зеркало и микрованная (моя боль и печаль). Угу, к концу года я устала, поэтому слегка в миноре.\n",
      "IMG\n",
      "Иду в комнату включить комп и впустить кота. Нет, ВПУСТИТЬ кота. Он каждый раз так влетает, будно всю ночь только об этом и мечтал.\n"
     ]
    }
   ],
   "source": [
    "for tree in dus:\n",
    "    print('>>>', tree.relation + '_' + tree.nuclearity)\n",
    "    if tree.left:\n",
    "        print(tree.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.evaluation import eval_pipeline\n",
    "\n",
    "evaluation = eval_pipeline(parsed, edus, gold, annot['text'])\n",
    "evaluation['filename'] = file\n",
    "cache = [evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/blogs_17.edus'"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_all_pred</th>\n",
       "      <th>full_all_true</th>\n",
       "      <th>full_true_pred</th>\n",
       "      <th>lab_all_pred</th>\n",
       "      <th>lab_all_true</th>\n",
       "      <th>lab_true_pred</th>\n",
       "      <th>nuc_all_pred</th>\n",
       "      <th>nuc_all_true</th>\n",
       "      <th>nuc_true_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_seg</th>\n",
       "      <th>pr_unlab</th>\n",
       "      <th>re_unlab</th>\n",
       "      <th>f1_unlab</th>\n",
       "      <th>pr_lab</th>\n",
       "      <th>re_lab</th>\n",
       "      <th>f1_lab</th>\n",
       "      <th>pr_nuc</th>\n",
       "      <th>re_nuc</th>\n",
       "      <th>f1_nuc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/blogs_21.edus</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>71</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>76</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>0.553785</td>\n",
       "      <td>0.523540</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.286252</td>\n",
       "      <td>0.317857</td>\n",
       "      <td>0.354582</td>\n",
       "      <td>0.335217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/blogs_17.edus</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>25</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.556213</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.402367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/blogs_21.edus</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>71</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>76</td>\n",
       "      <td>280</td>\n",
       "      <td>251</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>0.553785</td>\n",
       "      <td>0.523540</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.286252</td>\n",
       "      <td>0.317857</td>\n",
       "      <td>0.354582</td>\n",
       "      <td>0.335217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  full_all_pred  full_all_true  full_true_pred  \\\n",
       "0  ./data/blogs_21.edus            280            251              71   \n",
       "1  ./data/blogs_17.edus             86             83              25   \n",
       "2  ./data/blogs_21.edus            280            251              71   \n",
       "\n",
       "   lab_all_pred  lab_all_true  lab_true_pred  nuc_all_pred  nuc_all_true  \\\n",
       "0           280           251             76           280           251   \n",
       "1            86            83             26            86            83   \n",
       "2           280           251             76           280           251   \n",
       "\n",
       "   nuc_true_pred  ...  f1_seg  pr_unlab  re_unlab  f1_unlab    pr_lab  \\\n",
       "0             89  ...     1.0  0.496429  0.553785  0.523540  0.271429   \n",
       "1             34  ...     1.0  0.546512  0.566265  0.556213  0.302326   \n",
       "2             89  ...     1.0  0.496429  0.553785  0.523540  0.271429   \n",
       "\n",
       "     re_lab    f1_lab    pr_nuc    re_nuc    f1_nuc  \n",
       "0  0.302789  0.286252  0.317857  0.354582  0.335217  \n",
       "1  0.313253  0.307692  0.395349  0.409639  0.402367  \n",
       "2  0.302789  0.286252  0.317857  0.354582  0.335217  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(cache)\n",
    "tmp['pr_seg'] = tmp.seg_true_pred / tmp.seg_all_pred\n",
    "tmp['re_seg'] = tmp.seg_true_pred / tmp.seg_all_true\n",
    "tmp['f1_seg'] = 2 * tmp.pr_seg * tmp.re_seg / (tmp.pr_seg + tmp.re_seg)\n",
    "tmp['pr_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_pred\n",
    "tmp['re_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_true\n",
    "tmp['f1_unlab'] = 2 * tmp.pr_unlab * tmp.re_unlab / (tmp.pr_unlab + tmp.re_unlab)\n",
    "tmp['pr_lab'] = tmp.lab_true_pred / tmp.lab_all_pred\n",
    "tmp['re_lab'] = tmp.lab_true_pred / tmp.lab_all_true\n",
    "tmp['f1_lab'] = 2 * tmp.pr_lab * tmp.re_lab / (tmp.pr_lab + tmp.re_lab)\n",
    "tmp['pr_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_pred\n",
    "tmp['re_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_true\n",
    "tmp['f1_nuc'] = 2 * tmp.pr_nuc * tmp.re_nuc / (tmp.pr_nuc + tmp.re_nuc)\n",
    "tmp.sort_values('f1_seg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_all_pred</th>\n",
       "      <th>full_all_true</th>\n",
       "      <th>full_true_pred</th>\n",
       "      <th>lab_all_pred</th>\n",
       "      <th>lab_all_true</th>\n",
       "      <th>lab_true_pred</th>\n",
       "      <th>nuc_all_pred</th>\n",
       "      <th>nuc_all_true</th>\n",
       "      <th>nuc_true_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_seg</th>\n",
       "      <th>pr_unlab</th>\n",
       "      <th>re_unlab</th>\n",
       "      <th>f1_unlab</th>\n",
       "      <th>pr_lab</th>\n",
       "      <th>re_lab</th>\n",
       "      <th>f1_lab</th>\n",
       "      <th>pr_nuc</th>\n",
       "      <th>re_nuc</th>\n",
       "      <th>f1_nuc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/blogs_17.edus</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>25</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.556213</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.402367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename  full_all_pred  full_all_true  full_true_pred  \\\n",
       "0  data/blogs_17.edus             86             83              25   \n",
       "\n",
       "   lab_all_pred  lab_all_true  lab_true_pred  nuc_all_pred  nuc_all_true  \\\n",
       "0            86            83             26            86            83   \n",
       "\n",
       "   nuc_true_pred  ...  f1_seg  pr_unlab  re_unlab  f1_unlab    pr_lab  \\\n",
       "0             34  ...     1.0  0.546512  0.566265  0.556213  0.302326   \n",
       "\n",
       "     re_lab    f1_lab    pr_nuc    re_nuc    f1_nuc  \n",
       "0  0.313253  0.307692  0.395349  0.409639  0.402367  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import _not_parsed_as_in_gold\n",
    "\n",
    "err = _not_parsed_as_in_gold(parsed_pairs, gold, labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>category_id_gold</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>loc_x</th>\n",
       "      <th>loc_y</th>\n",
       "      <th>order_gold</th>\n",
       "      <th>filename</th>\n",
       "      <th>category_id_parsed</th>\n",
       "      <th>order_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>«Моя идея заключается в том, что всё в руках Г...</td>\n",
       "      <td>cause</td>\n",
       "      <td>потому и название такое - «Невидимая сила»...»</td>\n",
       "      <td>1523</td>\n",
       "      <td>1608</td>\n",
       "      <td>NS</td>\n",
       "      <td>news2_4</td>\n",
       "      <td>concession</td>\n",
       "      <td>NS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>«Композиция соответствует своему названию, име...</td>\n",
       "      <td>evaluation</td>\n",
       "      <td>выполнена мастерски - очень много сложных дета...</td>\n",
       "      <td>1749</td>\n",
       "      <td>1833</td>\n",
       "      <td>NS</td>\n",
       "      <td>news2_4</td>\n",
       "      <td>joint</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>чтобы полюбоваться песчаными шедеврами и,</td>\n",
       "      <td>joint</td>\n",
       "      <td>может быть, вдохновиться на создание своего ма...</td>\n",
       "      <td>2441</td>\n",
       "      <td>2483</td>\n",
       "      <td>NN</td>\n",
       "      <td>news2_4</td>\n",
       "      <td>purpose</td>\n",
       "      <td>SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Бронзовым призёром стал Айнарас Зигнис из латв...</td>\n",
       "      <td>preparation</td>\n",
       "      <td>В своей песчаной фантазии Айнарас воплотил сво...</td>\n",
       "      <td>1919</td>\n",
       "      <td>2045</td>\n",
       "      <td>SN</td>\n",
       "      <td>news2_4</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>NS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            snippet_x category_id_gold  \\\n",
       "10  «Моя идея заключается в том, что всё в руках Г...            cause   \n",
       "14  «Композиция соответствует своему названию, име...       evaluation   \n",
       "17          чтобы полюбоваться песчаными шедеврами и,            joint   \n",
       "29  Бронзовым призёром стал Айнарас Зигнис из латв...      preparation   \n",
       "\n",
       "                                            snippet_y  loc_x  loc_y  \\\n",
       "10     потому и название такое - «Невидимая сила»...»   1523   1608   \n",
       "14  выполнена мастерски - очень много сложных дета...   1749   1833   \n",
       "17  может быть, вдохновиться на создание своего ма...   2441   2483   \n",
       "29  В своей песчаной фантазии Айнарас воплотил сво...   1919   2045   \n",
       "\n",
       "   order_gold filename category_id_parsed order_parsed  \n",
       "10         NS  news2_4         concession           NS  \n",
       "14         NS  news2_4              joint           NN  \n",
       "17         NN  news2_4            purpose           SN  \n",
       "29         SN  news2_4        elaboration           NS  "
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval\n",
    "\n",
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news1_47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold  recall  precision  F1\n",
       "0  data/news1_47         0           2        96     0.0        0.0 NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>112</td>\n",
       "      <td>174</td>\n",
       "      <td>153</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.685015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/news1_47</td>\n",
       "      <td>49</td>\n",
       "      <td>106</td>\n",
       "      <td>96</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>0.485149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "0  data/news2_44       112         174       153  0.732026   0.643678   \n",
       "1  data/news1_47        49         106        96  0.510417   0.462264   \n",
       "\n",
       "         F1  \n",
       "0  0.685015  \n",
       "1  0.485149  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>112</td>\n",
       "      <td>174</td>\n",
       "      <td>153</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.685015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "0  data/news2_44       112         174       153  0.732026   0.643678   \n",
       "\n",
       "         F1  \n",
       "0  0.685015  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>112</td>\n",
       "      <td>174</td>\n",
       "      <td>153</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.685015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/news1_72</td>\n",
       "      <td>41</td>\n",
       "      <td>78</td>\n",
       "      <td>66</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/news1_47</td>\n",
       "      <td>49</td>\n",
       "      <td>106</td>\n",
       "      <td>96</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>0.485149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/news1_74</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.559322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/news2_38</td>\n",
       "      <td>113</td>\n",
       "      <td>226</td>\n",
       "      <td>201</td>\n",
       "      <td>0.562189</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.529274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/news2_31</td>\n",
       "      <td>72</td>\n",
       "      <td>117</td>\n",
       "      <td>99</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/news2_21</td>\n",
       "      <td>63</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/news1_39</td>\n",
       "      <td>51</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.569832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/news1_5</td>\n",
       "      <td>25</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.609756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/news2_3</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/news1_62</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/news2_28</td>\n",
       "      <td>148</td>\n",
       "      <td>207</td>\n",
       "      <td>190</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.714976</td>\n",
       "      <td>0.745592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/news1_22</td>\n",
       "      <td>58</td>\n",
       "      <td>89</td>\n",
       "      <td>83</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/news1_21</td>\n",
       "      <td>46</td>\n",
       "      <td>72</td>\n",
       "      <td>65</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.671533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/news1_64</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.491803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/news2_6</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/news1_52</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.623377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/news2_14</td>\n",
       "      <td>35</td>\n",
       "      <td>69</td>\n",
       "      <td>66</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/news1_50</td>\n",
       "      <td>68</td>\n",
       "      <td>152</td>\n",
       "      <td>143</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.461017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/news1_41</td>\n",
       "      <td>61</td>\n",
       "      <td>115</td>\n",
       "      <td>108</td>\n",
       "      <td>0.564815</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.547085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/news2_15</td>\n",
       "      <td>43</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/news1_26</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>data/news1_61</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>data/news1_1</td>\n",
       "      <td>38</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.562963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>data/news1_20</td>\n",
       "      <td>41</td>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.635659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>data/news1_29</td>\n",
       "      <td>37</td>\n",
       "      <td>81</td>\n",
       "      <td>78</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.465409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>data/sci.ling_33</td>\n",
       "      <td>54</td>\n",
       "      <td>152</td>\n",
       "      <td>99</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.430279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>data/sci.ling_31</td>\n",
       "      <td>67</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.489051</td>\n",
       "      <td>0.553719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>data/sci.ling_18</td>\n",
       "      <td>21</td>\n",
       "      <td>80</td>\n",
       "      <td>57</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.306569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>data/sci.ling_43</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>data/sci.ling_55</td>\n",
       "      <td>85</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>data/sci.ling_28</td>\n",
       "      <td>62</td>\n",
       "      <td>211</td>\n",
       "      <td>98</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.293839</td>\n",
       "      <td>0.401294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>data/sci.ling_39</td>\n",
       "      <td>41</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.394231</td>\n",
       "      <td>0.509317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>data/sci.ling_57</td>\n",
       "      <td>67</td>\n",
       "      <td>140</td>\n",
       "      <td>118</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.519380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>data/sci.ling_37</td>\n",
       "      <td>52</td>\n",
       "      <td>166</td>\n",
       "      <td>117</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.367491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>data/sci.ling_52</td>\n",
       "      <td>82</td>\n",
       "      <td>192</td>\n",
       "      <td>151</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.478134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>data/sci.comp_15</td>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>data/sci.comp_40</td>\n",
       "      <td>34</td>\n",
       "      <td>66</td>\n",
       "      <td>53</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>data/sci.comp_44</td>\n",
       "      <td>67</td>\n",
       "      <td>139</td>\n",
       "      <td>117</td>\n",
       "      <td>0.572650</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>0.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>data/sci.comp_37</td>\n",
       "      <td>32</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>data/sci.comp_5</td>\n",
       "      <td>50</td>\n",
       "      <td>101</td>\n",
       "      <td>87</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.531915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>data/sci.comp_32</td>\n",
       "      <td>59</td>\n",
       "      <td>120</td>\n",
       "      <td>88</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.567308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>data/sci.comp_54</td>\n",
       "      <td>64</td>\n",
       "      <td>152</td>\n",
       "      <td>120</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>data/sci.comp_3</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "      <td>40</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.350515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>data/sci.comp_38</td>\n",
       "      <td>86</td>\n",
       "      <td>231</td>\n",
       "      <td>183</td>\n",
       "      <td>0.469945</td>\n",
       "      <td>0.372294</td>\n",
       "      <td>0.415459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>data/sci.comp_4</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "      <td>34</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>data/blogs_15</td>\n",
       "      <td>51</td>\n",
       "      <td>138</td>\n",
       "      <td>116</td>\n",
       "      <td>0.439655</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.401575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>data/blogs_74</td>\n",
       "      <td>45</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>data/blogs_60</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.593407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>data/blogs_4</td>\n",
       "      <td>157</td>\n",
       "      <td>274</td>\n",
       "      <td>299</td>\n",
       "      <td>0.525084</td>\n",
       "      <td>0.572993</td>\n",
       "      <td>0.547993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>data/blogs_21</td>\n",
       "      <td>134</td>\n",
       "      <td>287</td>\n",
       "      <td>247</td>\n",
       "      <td>0.542510</td>\n",
       "      <td>0.466899</td>\n",
       "      <td>0.501873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>data/blogs_48</td>\n",
       "      <td>65</td>\n",
       "      <td>139</td>\n",
       "      <td>116</td>\n",
       "      <td>0.560345</td>\n",
       "      <td>0.467626</td>\n",
       "      <td>0.509804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>data/blogs_76</td>\n",
       "      <td>52</td>\n",
       "      <td>101</td>\n",
       "      <td>91</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.514851</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>data/blogs_61</td>\n",
       "      <td>32</td>\n",
       "      <td>63</td>\n",
       "      <td>58</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.528926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>data/blogs_28</td>\n",
       "      <td>119</td>\n",
       "      <td>198</td>\n",
       "      <td>192</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.610256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "0      data/news2_44       112         174       153  0.732026   0.643678   \n",
       "1      data/news1_72        41          78        66  0.621212   0.525641   \n",
       "2      data/news1_47        49         106        96  0.510417   0.462264   \n",
       "3      data/news1_74        33          60        58  0.568966   0.550000   \n",
       "4      data/news2_38       113         226       201  0.562189   0.500000   \n",
       "5      data/news2_31        72         117        99  0.727273   0.615385   \n",
       "6      data/news2_21        63         103       104  0.605769   0.611650   \n",
       "7      data/news1_39        51          87        92  0.554348   0.586207   \n",
       "8       data/news1_5        25          42        40  0.625000   0.595238   \n",
       "9       data/news2_3        18          39        33  0.545455   0.461538   \n",
       "10     data/news1_62         9          18        17  0.529412   0.500000   \n",
       "11     data/news2_28       148         207       190  0.778947   0.714976   \n",
       "12     data/news1_22        58          89        83  0.698795   0.651685   \n",
       "13     data/news1_21        46          72        65  0.707692   0.638889   \n",
       "14     data/news1_64        15          33        28  0.535714   0.454545   \n",
       "15      data/news2_6         5          21        15  0.333333   0.238095   \n",
       "16     data/news1_52        24          38        39  0.615385   0.631579   \n",
       "17     data/news2_14        35          69        66  0.530303   0.507246   \n",
       "18     data/news1_50        68         152       143  0.475524   0.447368   \n",
       "19     data/news1_41        61         115       108  0.564815   0.530435   \n",
       "20     data/news2_15        43          87        85  0.505882   0.494253   \n",
       "21     data/news1_26        14          28        22  0.636364   0.500000   \n",
       "22     data/news1_61        14          27        23  0.608696   0.518519   \n",
       "23      data/news1_1        38          70        65  0.584615   0.542857   \n",
       "24     data/news1_20        41          67        62  0.661290   0.611940   \n",
       "25     data/news1_29        37          81        78  0.474359   0.456790   \n",
       "26  data/sci.ling_33        54         152        99  0.545455   0.355263   \n",
       "27  data/sci.ling_31        67         137       105  0.638095   0.489051   \n",
       "28  data/sci.ling_18        21          80        57  0.368421   0.262500   \n",
       "29  data/sci.ling_43        24          48        38  0.631579   0.500000   \n",
       "30  data/sci.ling_55        85         145       145  0.586207   0.586207   \n",
       "31  data/sci.ling_28        62         211        98  0.632653   0.293839   \n",
       "32  data/sci.ling_39        41         104        57  0.719298   0.394231   \n",
       "33  data/sci.ling_57        67         140       118  0.567797   0.478571   \n",
       "34  data/sci.ling_37        52         166       117  0.444444   0.313253   \n",
       "35  data/sci.ling_52        82         192       151  0.543046   0.427083   \n",
       "36  data/sci.comp_15        17          49        36  0.472222   0.346939   \n",
       "37  data/sci.comp_40        34          66        53  0.641509   0.515152   \n",
       "38  data/sci.comp_44        67         139       117  0.572650   0.482014   \n",
       "39  data/sci.comp_37        32          66        62  0.516129   0.484848   \n",
       "40   data/sci.comp_5        50         101        87  0.574713   0.495050   \n",
       "41  data/sci.comp_32        59         120        88  0.670455   0.491667   \n",
       "42  data/sci.comp_54        64         152       120  0.533333   0.421053   \n",
       "43   data/sci.comp_3        17          57        40  0.425000   0.298246   \n",
       "44  data/sci.comp_38        86         231       183  0.469945   0.372294   \n",
       "45   data/sci.comp_4        12          44        34  0.352941   0.272727   \n",
       "46     data/blogs_15        51         138       116  0.439655   0.369565   \n",
       "47     data/blogs_74        45          87        83  0.542169   0.517241   \n",
       "48     data/blogs_60        27          43        48  0.562500   0.627907   \n",
       "49      data/blogs_4       157         274       299  0.525084   0.572993   \n",
       "50     data/blogs_21       134         287       247  0.542510   0.466899   \n",
       "51     data/blogs_48        65         139       116  0.560345   0.467626   \n",
       "52     data/blogs_76        52         101        91  0.571429   0.514851   \n",
       "53     data/blogs_61        32          63        58  0.551724   0.507937   \n",
       "54     data/blogs_28       119         198       192  0.619792   0.601010   \n",
       "\n",
       "          F1  \n",
       "0   0.685015  \n",
       "1   0.569444  \n",
       "2   0.485149  \n",
       "3   0.559322  \n",
       "4   0.529274  \n",
       "5   0.666667  \n",
       "6   0.608696  \n",
       "7   0.569832  \n",
       "8   0.609756  \n",
       "9   0.500000  \n",
       "10  0.514286  \n",
       "11  0.745592  \n",
       "12  0.674419  \n",
       "13  0.671533  \n",
       "14  0.491803  \n",
       "15  0.277778  \n",
       "16  0.623377  \n",
       "17  0.518519  \n",
       "18  0.461017  \n",
       "19  0.547085  \n",
       "20  0.500000  \n",
       "21  0.560000  \n",
       "22  0.560000  \n",
       "23  0.562963  \n",
       "24  0.635659  \n",
       "25  0.465409  \n",
       "26  0.430279  \n",
       "27  0.553719  \n",
       "28  0.306569  \n",
       "29  0.558140  \n",
       "30  0.586207  \n",
       "31  0.401294  \n",
       "32  0.509317  \n",
       "33  0.519380  \n",
       "34  0.367491  \n",
       "35  0.478134  \n",
       "36  0.400000  \n",
       "37  0.571429  \n",
       "38  0.523438  \n",
       "39  0.500000  \n",
       "40  0.531915  \n",
       "41  0.567308  \n",
       "42  0.470588  \n",
       "43  0.350515  \n",
       "44  0.415459  \n",
       "45  0.307692  \n",
       "46  0.401575  \n",
       "47  0.529412  \n",
       "48  0.593407  \n",
       "49  0.547993  \n",
       "50  0.501873  \n",
       "51  0.509804  \n",
       "52  0.541667  \n",
       "53  0.528926  \n",
       "54  0.610256  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results_20.03.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5366718683501206 0.47388199924840285 0.503326237360298\n"
     ]
    }
   ],
   "source": [
    "re = results['true_pos'].sum() / results['all_gold'].sum()\n",
    "pr = results['true_pos'].sum() / results['all_parsed'].sum()\n",
    "f1 = 2 * pr * re / (pr+re)\n",
    "print(re, pr, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5634517766497462 0.4036363636363636 0.4703389830508475\n"
     ]
    }
   ],
   "source": [
    "tmp = results[results.filename.str.contains('ling')]\n",
    "re = tmp['true_pos'].sum() / tmp['all_gold'].sum()\n",
    "pr = tmp['true_pos'].sum() / tmp['all_parsed'].sum()\n",
    "f1 = 2 * pr * re / (pr+re)\n",
    "print(re, pr, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>106</td>\n",
       "      <td>167</td>\n",
       "      <td>153</td>\n",
       "      <td>0.69281</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.6625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold   recall  precision      F1\n",
       "0  data/news2_44       106         167       153  0.69281   0.634731  0.6625"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1')  # eee th=.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>112</td>\n",
       "      <td>174</td>\n",
       "      <td>153</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.685015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "0  data/news2_44       112         174       153  0.732026   0.643678   \n",
       "\n",
       "         F1  \n",
       "0  0.685015  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1')  # eee th=.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/news2_44</td>\n",
       "      <td>112</td>\n",
       "      <td>176</td>\n",
       "      <td>153</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "0  data/news2_44       112         176       153  0.732026   0.636364   \n",
       "\n",
       "         F1  \n",
       "0  0.680851  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1')  # eee th=.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4119188649405266, 0.3704063070550079, 0.47726138642333527)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['F1'].mean(), results['recall'].mean(), results['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40254579065780033, 0.3449201654454985, 0.48762505038940707)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[results.filename.str.contains('blog')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4448437125215326, 0.4004284201602762, 0.5113166238620861)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[results.filename.str.contains('news')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4019318169329664, 0.37567099154362277, 0.4455345528051692)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[results.filename.str.contains('ling')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3810944830999073, 0.3589145654150725, 0.41109744867219583)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = results[results.filename.str.contains('comp')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval_df as metric_parseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_rst.src.isanlp_rst.rst_tree_predictor import GoldTreePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_golds(filename):\n",
    "    filename = '.'.join(filename.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), confidence_threshold=0.)\n",
    "    parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                    annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed, annot['text']), columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "    parsed_pairs[0] = parsed_pairs.snippet_x\n",
    "    parsed_pairs[1] = parsed_pairs.snippet_y\n",
    "    \n",
    "    return (filename,) + metric_parseval(parsed_pairs, gold)#, parsed_pairs, parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.1 ms, sys: 278 ms, total: 339 ms\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(5)\n",
    "result = pool.map(parse_golds, dev)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['filename', 'true_pos', 'all_parsed', 'all_gold'], data=result)\n",
    "\n",
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>./data/blogs_20</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>151</td>\n",
       "      <td>0.350993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.519608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>./data/blogs_10</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>83</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.677165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>./data/blogs_9</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>134</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>./data/blogs_22</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>361</td>\n",
       "      <td>0.540166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>./data/blogs_5</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>105</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.748538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>./data/blogs_19</td>\n",
       "      <td>167</td>\n",
       "      <td>189</td>\n",
       "      <td>255</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>0.752252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>./data/blogs_26</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>67</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>./data/blogs_64</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>98</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.785276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>./data/news1_40</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>./data/blogs_103</td>\n",
       "      <td>118</td>\n",
       "      <td>119</td>\n",
       "      <td>168</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./data/news2_21</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>104</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.866310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>./data/blogs_54</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>175</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./data/news2_30</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>90</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/news2_44</td>\n",
       "      <td>140</td>\n",
       "      <td>141</td>\n",
       "      <td>171</td>\n",
       "      <td>0.818713</td>\n",
       "      <td>0.992908</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/news2_1</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./data/news1_76</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>./data/blogs_15</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>122</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.911504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>./data/blogs_30</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "      <td>84</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./data/news1_50</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>148</td>\n",
       "      <td>0.858108</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.920290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>./data/blogs_83</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./data/news2_13</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./data/news2_43</td>\n",
       "      <td>146</td>\n",
       "      <td>148</td>\n",
       "      <td>165</td>\n",
       "      <td>0.884848</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.932907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./data/news2_23</td>\n",
       "      <td>212</td>\n",
       "      <td>214</td>\n",
       "      <td>240</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.933921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./data/news2_11</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./data/news1_60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>./data/blogs_57</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/news2_49</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>./data/blogs_76</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>92</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./data/news1_57</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./data/news1_19</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./data/news1_22</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./data/news1_20</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>./data/blogs_33</td>\n",
       "      <td>107</td>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>0.963964</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.968326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./data/news1_6</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/news1_59</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./data/news1_36</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>./data/blogs_100</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./data/news1_44</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/news1_79</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>./data/blogs_49</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  true_pos  all_parsed  all_gold    recall  precision  \\\n",
       "25   ./data/blogs_20        53          53       151  0.350993   1.000000   \n",
       "29   ./data/blogs_10        43          44        83  0.518072   0.977273   \n",
       "30    ./data/blogs_9        72          73       134  0.537313   0.986301   \n",
       "33   ./data/blogs_22       195         195       361  0.540166   1.000000   \n",
       "26    ./data/blogs_5        64          66       105  0.609524   0.969697   \n",
       "38   ./data/blogs_19       167         189       255  0.654902   0.883598   \n",
       "39   ./data/blogs_26        41          41        67  0.611940   1.000000   \n",
       "36   ./data/blogs_64        64          65        98  0.653061   0.984615   \n",
       "21   ./data/news1_40        18          18        26  0.692308   1.000000   \n",
       "35  ./data/blogs_103       118         119       168  0.702381   0.991597   \n",
       "5    ./data/news2_21        81          83       104  0.778846   0.975904   \n",
       "28   ./data/blogs_54       136         136       175  0.777143   1.000000   \n",
       "13   ./data/news2_30        73          73        90  0.811111   1.000000   \n",
       "0    ./data/news2_44       140         141       171  0.818713   0.992908   \n",
       "3     ./data/news2_1        20          20        24  0.833333   1.000000   \n",
       "6    ./data/news1_76        45          46        53  0.849057   0.978261   \n",
       "22   ./data/blogs_15       103         104       122  0.844262   0.990385   \n",
       "24   ./data/blogs_30        73          76        84  0.869048   0.960526   \n",
       "15   ./data/news1_50       127         128       148  0.858108   0.992188   \n",
       "23   ./data/blogs_83        24          24        28  0.857143   1.000000   \n",
       "11   ./data/news2_13        36          36        42  0.857143   1.000000   \n",
       "9    ./data/news2_43       146         148       165  0.884848   0.986486   \n",
       "7    ./data/news2_23       212         214       240  0.883333   0.990654   \n",
       "17   ./data/news2_11        58          58        66  0.878788   1.000000   \n",
       "12   ./data/news1_60        30          30        34  0.882353   1.000000   \n",
       "32   ./data/blogs_57        39          40        42  0.928571   0.975000   \n",
       "1    ./data/news2_49       108         108       119  0.907563   1.000000   \n",
       "27   ./data/blogs_76        84          84        92  0.913043   1.000000   \n",
       "16   ./data/news1_57        32          32        35  0.914286   1.000000   \n",
       "8    ./data/news1_19        44          45        47  0.936170   0.977778   \n",
       "10   ./data/news1_22        77          77        83  0.927711   1.000000   \n",
       "20   ./data/news1_20        58          58        62  0.935484   1.000000   \n",
       "34   ./data/blogs_33       107         110       111  0.963964   0.972727   \n",
       "19    ./data/news1_6        86          86        91  0.945055   1.000000   \n",
       "4    ./data/news1_59        35          35        37  0.945946   1.000000   \n",
       "18   ./data/news1_36        53          53        54  0.981481   1.000000   \n",
       "31  ./data/blogs_100        77          77        78  0.987179   1.000000   \n",
       "14   ./data/news1_44       121         121       122  0.991803   1.000000   \n",
       "2    ./data/news1_79        24          24        24  1.000000   1.000000   \n",
       "37   ./data/blogs_49       127         127       127  1.000000   1.000000   \n",
       "\n",
       "          F1  \n",
       "25  0.519608  \n",
       "29  0.677165  \n",
       "30  0.695652  \n",
       "33  0.701439  \n",
       "26  0.748538  \n",
       "38  0.752252  \n",
       "39  0.759259  \n",
       "36  0.785276  \n",
       "21  0.818182  \n",
       "35  0.822300  \n",
       "5   0.866310  \n",
       "28  0.874598  \n",
       "13  0.895706  \n",
       "0   0.897436  \n",
       "3   0.909091  \n",
       "6   0.909091  \n",
       "22  0.911504  \n",
       "24  0.912500  \n",
       "15  0.920290  \n",
       "23  0.923077  \n",
       "11  0.923077  \n",
       "9   0.932907  \n",
       "7   0.933921  \n",
       "17  0.935484  \n",
       "12  0.937500  \n",
       "32  0.951220  \n",
       "1   0.951542  \n",
       "27  0.954545  \n",
       "16  0.955224  \n",
       "8   0.956522  \n",
       "10  0.962500  \n",
       "20  0.966667  \n",
       "34  0.968326  \n",
       "19  0.971751  \n",
       "4   0.972222  \n",
       "18  0.990654  \n",
       "31  0.993548  \n",
       "14  0.995885  \n",
       "2   1.000000  \n",
       "37  1.000000  "
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888192175356536"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.F1.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
