{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None, \n",
    "                 orig_text=None, relation=None, nuclearity=None, proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = str(proba)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            self.start = left.start\n",
    "            self.end = right.end+1\n",
    "        \n",
    "        if orig_text:            \n",
    "            self.text = orig_text[self.start:self.end].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}\\ntext: {self.text}\\nrelation: {self.relation}\\nleft: {self.left.text if self.left else None}\\nright: {self.right.text if self.right else None}\\nstart: {self.start}\\nend: {self.end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(tree):\n",
    "    def _(n):\n",
    "        if n.relation:\n",
    "            value = (n.relation, \"%.2f\"%(n.proba))\n",
    "        else:\n",
    "            value = n.text\n",
    "        return str(value), n.left, n.right\n",
    "\n",
    "    return printBTree(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SklearnClassifier:\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn/catboost classification model along with preprocessors, saved in the same directory:\n",
    "        [required]\n",
    "        - model.pkl            : trained model\n",
    "        [optional]\n",
    "        - drop_columns.pkl     : list of names for columns to drop before prediction\n",
    "        - categorical_cols.pkl : list of names for columns with categorical features\n",
    "        - one_hot_encoder.pkl  : trained one-hot sklearn encoder model\n",
    "        - scaler.pkl           : trained sklearn scaler model\n",
    "        - label_encoder.pkl    : trained label encoder to decode predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir_path):\n",
    "        self.model_dir_path = model_dir_path\n",
    "\n",
    "        file_drop_columns = os.path.join(self.model_dir_path, 'drop_columns.pkl')\n",
    "        self._drop_columns = pickle.load(open(file_drop_columns, 'rb')) if os.path.isfile(\n",
    "            file_drop_columns) else None\n",
    "        if self._drop_columns:\n",
    "            self._drop_columns = [value for value in self._drop_columns if\n",
    "                                  not value in ('category_id' 'filename' 'order')]\n",
    "\n",
    "        file_scaler = os.path.join(self.model_dir_path, 'scaler.pkl')\n",
    "        self._scaler = pickle.load(open(file_scaler, 'rb')) if os.path.isfile(\n",
    "            file_scaler) else None\n",
    "\n",
    "        file_categorical_cols = os.path.join(self.model_dir_path, 'categorical_cols.pkl')\n",
    "        self._categorical_cols = pickle.load(open(file_categorical_cols, 'rb')) if os.path.isfile(\n",
    "            file_categorical_cols) else None\n",
    "\n",
    "        file_one_hot_encoder = os.path.join(self.model_dir_path, 'one_hot_encoder.pkl')\n",
    "        self._one_hot_encoder = pickle.load(open(file_one_hot_encoder, 'rb')) if os.path.isfile(\n",
    "            file_one_hot_encoder) else None\n",
    "\n",
    "        file_label_encoder = os.path.join(self.model_dir_path, 'label_encoder.pkl')\n",
    "        self._label_encoder = pickle.load(open(file_label_encoder, 'rb')) if os.path.isfile(\n",
    "            file_label_encoder) else None\n",
    "\n",
    "        self._model = pickle.load(open(os.path.join(self.model_dir_path, 'model.pkl'), 'rb'))\n",
    "        self.classes_ = self._model.classes_\n",
    "\n",
    "    def predict_proba(self, features):\n",
    "        return self._model.predict_proba(self._preprocess_features(features))\n",
    "\n",
    "    def predict(self, features):\n",
    "        if self._label_encoder:\n",
    "            return self._label_encoder.inverse_transform(self._model.predict(self._preprocess_features(features)))\n",
    "\n",
    "        return self._model.predict(self._preprocess_features(features))\n",
    "\n",
    "    def _preprocess_features(self, _features):\n",
    "        features = _features[:]\n",
    "        \n",
    "        if self._categorical_cols:\n",
    "            if self._label_encoder:\n",
    "                features[self._categorical_cols] = features[self._categorical_cols].apply(\n",
    "                    lambda col: self._label_encoder.fit_transform(col))\n",
    "\n",
    "            if self._one_hot_encoder:\n",
    "                features_ohe = self._one_hot_encoder.transform(features[self._categorical_cols].values)\n",
    "                features_ohe = pd.DataFrame(features_ohe, features.index,\n",
    "                                            columns=self._one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                features = features.join(\n",
    "                    pd.DataFrame(features_ohe, features.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols)\n",
    "\n",
    "        if self._drop_columns:\n",
    "            features = features.drop(columns=self._drop_columns)\n",
    "\n",
    "        if 'category_id' in features.keys():\n",
    "            features = features.drop(columns=['category_id', 'filename', 'order'])\n",
    "\n",
    "        if self._scaler:\n",
    "            return self._scaler.transform(features.values)\n",
    "\n",
    "        return features.values.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "\n",
    "pr = Predictor.from_path('models/structure_predictor_lstm/results_all/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.predict('В целом арабские страны поддерживают эту инициативу ,',\n",
    "           'так как многие из них - особенно после 2011 года - были серьезно затронуты действиями радикальных исламистов .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pr.predict_batch_json([{'premise': 'В целом арабские страны поддерживают эту инициативу ,',\n",
    "           'hypothesis': 'так как многие из них - особенно после 2011 года - были серьезно затронуты действиями радикальных исламистов .'}], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SklearnClassifier:\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn/catboost classification model along with preprocessors, saved in the same directory:\n",
    "        [required]\n",
    "        - model.pkl            : trained model\n",
    "        [optional]\n",
    "        - drop_columns.pkl     : list of names for columns to drop before prediction\n",
    "        - categorical_cols.pkl : list of names for columns with categorical features\n",
    "        - one_hot_encoder.pkl  : trained one-hot sklearn encoder model\n",
    "        - scaler.pkl           : trained sklearn scaler model\n",
    "        - label_encoder.pkl    : trained label encoder to decode predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir_path):\n",
    "        self.model_dir_path = model_dir_path\n",
    "\n",
    "        file_drop_columns = os.path.join(self.model_dir_path, 'drop_columns.pkl')\n",
    "        self._drop_columns = pickle.load(open(file_drop_columns, 'rb')) if os.path.isfile(\n",
    "            file_drop_columns) else None\n",
    "        if self._drop_columns:\n",
    "            self._drop_columns = [value for value in self._drop_columns if\n",
    "                                  not value in ('category_id' 'filename' 'order')]\n",
    "\n",
    "        file_scaler = os.path.join(self.model_dir_path, 'scaler.pkl')\n",
    "        self._scaler = pickle.load(open(file_scaler, 'rb')) if os.path.isfile(\n",
    "            file_scaler) else None\n",
    "\n",
    "        file_categorical_cols = os.path.join(self.model_dir_path, 'categorical_cols.pkl')\n",
    "        self._categorical_cols = pickle.load(open(file_categorical_cols, 'rb')) if os.path.isfile(\n",
    "            file_categorical_cols) else None\n",
    "\n",
    "        file_one_hot_encoder = os.path.join(self.model_dir_path, 'one_hot_encoder.pkl')\n",
    "        self._one_hot_encoder = pickle.load(open(file_one_hot_encoder, 'rb')) if os.path.isfile(\n",
    "            file_one_hot_encoder) else None\n",
    "\n",
    "        file_label_encoder = os.path.join(self.model_dir_path, 'label_encoder.pkl')\n",
    "        self._label_encoder = pickle.load(open(file_label_encoder, 'rb')) if os.path.isfile(\n",
    "            file_label_encoder) else None\n",
    "\n",
    "        self._model = pickle.load(open(os.path.join(self.model_dir_path, 'model.pkl'), 'rb'))\n",
    "        self.classes_ = self._model.classes_\n",
    "\n",
    "    def predict_proba(self, features):\n",
    "        return self._model.predict_proba(self._preprocess_features(features))\n",
    "\n",
    "    def predict(self, features):\n",
    "        if self._label_encoder:\n",
    "            return self._label_encoder.inverse_transform(self._model.predict(self._preprocess_features(features)))\n",
    "\n",
    "        return self._model.predict(self._preprocess_features(features))\n",
    "\n",
    "    def _preprocess_features(self, _features):\n",
    "        features = _features[:]\n",
    "        \n",
    "        if self._categorical_cols:\n",
    "            if self._label_encoder:\n",
    "                features[self._categorical_cols] = features[self._categorical_cols].apply(\n",
    "                    lambda col: self._label_encoder.fit_transform(col))\n",
    "\n",
    "            if self._one_hot_encoder:\n",
    "                features_ohe = self._one_hot_encoder.transform(features[self._categorical_cols].values)\n",
    "                features_ohe = pd.DataFrame(features_ohe, features.index,\n",
    "                                            columns=self._one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                features = features.join(\n",
    "                    pd.DataFrame(features_ohe, features.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols)\n",
    "\n",
    "        if self._drop_columns:\n",
    "            features = features.drop(columns=self._drop_columns)\n",
    "\n",
    "        if 'category_id' in features.keys():\n",
    "            features = features.drop(columns=['category_id', 'filename', 'order'])\n",
    "\n",
    "        if self._scaler:\n",
    "            return self._scaler.transform(features.values)\n",
    "\n",
    "        return features.values.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AllenNLPClassifier:\n",
    "    \"\"\"\n",
    "    Wrapper for allennlp classification model along with preprocessors, saved in the same directory:\n",
    "        [required]\n",
    "        - model.tar.gz            : trained model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir_path):\n",
    "        self.model_dir_path = model_dir_path\n",
    "        self._max_len = 300\n",
    "        \n",
    "        self._model = Predictor.from_path(os.path.join(self.model_dir_path, 'model.tar.gz'))\n",
    "\n",
    "    def predict_proba(self, snippet_x, snippet_y):\n",
    "        if len(snippet_x.split()) > self._max_len or len(snippet_y.split()) > self._max_len:\n",
    "            return [1., 0.]\n",
    "        \n",
    "        return self._model.predict(snippet_x, snippet_y)['probs']\n",
    "    \n",
    "    def predict_proba_batch(self, snippet_x, snippet_y):\n",
    "        predictions = pr.predict_batch_json([\n",
    "            {'premise': snippet_x[i],\n",
    "             'hypothesis': snippet_y[i]}\n",
    "            for i in range(len(snippet_x))])\n",
    "        return [prediction['probs'] for prediction in predictions]\n",
    "\n",
    "    def predict(self, snippet_x, snippet_y):\n",
    "        return self._model.predict(snippet_x, snippet_y)['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "\n",
    "class RSTTreePredictor:\n",
    "    \"\"\"\n",
    "    Contains classifiers and processors needed for tree building.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor, nuclearity_predictor):\n",
    "        self.features_processor = features_processor\n",
    "        self.relation_predictor = relation_predictor\n",
    "        self.label_predictor = label_predictor\n",
    "        if self.label_predictor:\n",
    "            self.labels = self.label_predictor.classes_\n",
    "\n",
    "        self.nuclearity_predictor = nuclearity_predictor\n",
    "        if self.nuclearity_predictor:\n",
    "            self.nuclearities = self.nuclearity_predictor.classes_\n",
    "\n",
    "        self.genre = None\n",
    "\n",
    "\n",
    "class GoldTreePredictor(RSTTreePredictor):\n",
    "    \"\"\"\n",
    "    Contains classifiers and processors needed for gold tree building from corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        :param pandas.DataFrame corpus:\n",
    "            columns=['snippet_x', 'snippet_y', 'category_id']\n",
    "            rows=[all the relations pairs from corpus]\n",
    "        \"\"\"\n",
    "        RSTTreePredictor.__init__(self, None, None, None, None)\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def extract_features(self, *args):\n",
    "        return pd.DataFrame({\n",
    "            'snippet_x': [args[0].text, ],\n",
    "            'snippet_y': [args[1].text, ]\n",
    "        })\n",
    "\n",
    "    def initialize_features(self, *args):\n",
    "        return pd.DataFrame({\n",
    "            'snippet_x': [args[0][i].text for i in range(len(args[0]) - 1)],\n",
    "            'snippet_y': [args[0][i].text for i in range(1, len(args[0]))]\n",
    "        })\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "            return float((((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "                         or ((self.corpus.snippet_y == left_snippet) & (self.corpus.snippet_x == right_snippet)).sum(\n",
    "                axis=0) != 0)\n",
    "\n",
    "        result = features.apply(lambda row: _check_snippet_pair_in_dataset(row.snippet_x, row.snippet_y), axis=1)\n",
    "        return result.values.tolist()\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        def _get_label(left_snippet, right_snippet):\n",
    "            label = self.corpus[\n",
    "                ((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].category_id.values\n",
    "            if label.size == 0:\n",
    "                return 'relation'\n",
    "\n",
    "            return label[0]\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            result = _get_label(features.loc['snippet_x'], features.loc['snippet_y'])\n",
    "            return result\n",
    "        else:\n",
    "            result = features.apply(lambda row: _get_label(row.snippet_x, row.snippet_y), axis=1)\n",
    "            return result.values.tolist()\n",
    "\n",
    "    def predict_nuclearity(self, features):\n",
    "        def _get_nuclearity(left_snippet, right_snippet):\n",
    "            nuclearity = self.corpus[\n",
    "                ((self.corpus.snippet_x == left_snippet) & (self.corpus.snippet_y == right_snippet))].order.values\n",
    "            if nuclearity.size == 0:\n",
    "                return '_'\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            result = _get_nuclearity(features.loc['snippet_x'], features.loc['snippet_y'])\n",
    "            return result\n",
    "        else:\n",
    "            result = features.apply(lambda row: _get_nuclearity(row.snippet_x, row.snippet_y), axis=1)\n",
    "            return result.values.tolist()\n",
    "\n",
    "\n",
    "class CustomTreePredictor(RSTTreePredictor):\n",
    "    \"\"\"\n",
    "    Contains trained classifiers and feature processors needed for tree prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features_processor, relation_predictor, label_predictor=None, nuclearity_predictor=None):\n",
    "        RSTTreePredictor.__init__(self, features_processor, relation_predictor, label_predictor, nuclearity_predictor)\n",
    "\n",
    "    def extract_features(self, left_node: DiscourseUnit, right_node: DiscourseUnit,\n",
    "                         annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                         annot_syntax_dep_tree):\n",
    "        pair = pd.DataFrame({\n",
    "            'snippet_x': [left_node.text.strip()],\n",
    "            'snippet_y': [right_node.text.strip()],\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_processor(pair, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def initialize_features(self, nodes,\n",
    "                            annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                            annot_syntax_dep_tree):\n",
    "        pairs = pd.DataFrame({\n",
    "            'snippet_x': [node.text.strip() for node in nodes[:-1]],\n",
    "            'snippet_y': [node.text.strip() for node in nodes[1:]]\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            features = self.features_processor(pairs, annot_text=annot_text,\n",
    "                                               annot_tokens=annot_tokens, annot_sentences=annot_sentences,\n",
    "                                               annot_postag=annot_postag, annot_morph=annot_morph,\n",
    "                                               annot_lemma=annot_lemma, annot_syntax_dep_tree=annot_syntax_dep_tree)\n",
    "            return features\n",
    "        except IndexError:\n",
    "            with open('errors.log', 'w+') as f:\n",
    "                f.write(str(pair.values))\n",
    "                f.write(annot_text)\n",
    "            return -1\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        _same_sentence_bonus = 0.5\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            probas = self.relation_predictor.predict_proba(features)\n",
    "            # results = list(map(lambda proba: proba[1], probas))\n",
    "            # return results\n",
    "            same_sentence_bonus = list(map(lambda value: float(value) * _same_sentence_bonus,\n",
    "                                           list(features['same_sentence'] == 1)))\n",
    "            return [probas[i][1] + same_sentence_bonus[i] for i in range(len(probas))]\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            return self.relation_predictor.predict_proba(features)[0][1] + (\n",
    "                    features.loc['same_sentence'] == 1) * _same_sentence_bonus\n",
    "\n",
    "        if type(features) == list:\n",
    "            return self.relation_predictor.predict_proba([features])[0][1]\n",
    "\n",
    "    def predict_label(self, features):\n",
    "        if not self.label_predictor:\n",
    "            return 'relation'\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            return self.label_predictor.predict(features)\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            return self.label_predictor.predict(features.to_frame().T)[0]\n",
    "\n",
    "    def predict_nuclearity(self, features):\n",
    "        if not self.nuclearity_predictor:\n",
    "            return 'unavail'\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            return self.nuclearity_predictor.predict(features)\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            return self.nuclearity_predictor.predict(features.to_frame().T)[0]\n",
    "\n",
    "\n",
    "class NNTreePredictor(CustomTreePredictor):\n",
    "    \"\"\"\n",
    "    Contains trained classifiers and feature processors needed for tree prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_features(self, nodes,\n",
    "                            annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                            annot_syntax_dep_tree):\n",
    "        features = super().initialize_features(nodes,\n",
    "                            annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                            annot_syntax_dep_tree)\n",
    "        features['snippet_x'] = features['tokens_x'].map(lambda row: ' '.join(row)).values\n",
    "        features['snippet_y'] = features['tokens_y'].map(lambda row: ' '.join(row)).values\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def predict_pair_proba(self, features):\n",
    "        _same_sentence_bonus = 0.\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            probas = features.apply(lambda row: self.relation_predictor.predict_proba(row.snippet_x, row.snippet_y), axis=1).values\n",
    "            same_sentence_bonus = list(map(lambda value: float(value) * _same_sentence_bonus,\n",
    "                                           list(features['same_sentence'] == 1)))\n",
    "\n",
    "            return [probas[i][1] + same_sentence_bonus[i] for i in range(len(probas))]\n",
    "\n",
    "        if type(features) == pd.Series:\n",
    "            return self.relation_predictor.predict_proba(' '.join(features.loc['tokens_x']),\n",
    "                                                         ' '.join(features.loc['tokens_y']))[0][1] + (\n",
    "                           features.loc['same_sentence'] == 1) * _same_sentence_bonus\n",
    "\n",
    "        if type(features) == list:\n",
    "            probas = []\n",
    "            \n",
    "            for i in range(len(features)):\n",
    "                snippet_x = features[i]['tokens_x'].map(lambda row: ' '.join(row)).values\n",
    "                snippet_y = features[i]['tokens_y'].map(lambda row: ' '.join(row)).values\n",
    "                probas.append(self.relation_predictor.predict_proba(snippet_x, snippet_y)[1])\n",
    "                \n",
    "            #probas = [self.relation_predictor.predict_proba(features[i]['', snippet_y[i]) for i in range(len(snippet_x))]\n",
    "            return probas#self.relation_predictor.predict_proba([features])[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "\n",
    "class GreedyRSTParser:\n",
    "    def __init__(self, tree_predictor, forest_threshold=0.05):\n",
    "        \"\"\"\n",
    "        :param RSTTreePredictor tree_predictor:\n",
    "        :param float forest_threshold: minimum relation probability to append the pair into the tree\n",
    "        \"\"\"\n",
    "        self.tree_predictor = tree_predictor\n",
    "        self.forest_threshold = forest_threshold\n",
    "\n",
    "    def __call__(self, edus, annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                 annot_syntax_dep_tree, genre=None):\n",
    "        \"\"\"\n",
    "        :param list edus: DiscourseUnit\n",
    "        :param str annot_text: original text\n",
    "        :param list annot_tokens: isanlp.annotation.Token\n",
    "        :param list annot_sentences: isanlp.annotation.Sentence\n",
    "        :param list annot_postag: lists of str for each sentence\n",
    "        :param annot_lemma: lists of str for each sentence\n",
    "        :param annot_syntax_dep_tree: list of isanlp.annotation.WordSynt for each sentence\n",
    "        :return: list of DiscourseUnit containing each extracted tree\n",
    "        \"\"\"\n",
    "\n",
    "        def to_merge(_scores):\n",
    "            return np.argmax(np.array(_scores))\n",
    "\n",
    "        self.tree_predictor.genre = genre\n",
    "\n",
    "        nodes = edus\n",
    "\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes, \n",
    "                                                           annot_text, annot_tokens,\n",
    "                                                           annot_sentences,\n",
    "                                                           annot_lemma, annot_morph, annot_postag,\n",
    "                                                           annot_syntax_dep_tree)\n",
    "\n",
    "        scores = self.tree_predictor.predict_pair_proba(features)\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=self.tree_predictor.predict_label(features.iloc[j]),\n",
    "                nuclearity=self.tree_predictor.predict_nuclearity(features.iloc[j]),\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features)\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = pd.concat([_features, features.iloc[j + 2:]])\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]],\n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_lemma, annot_morph, annot_postag,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features)\n",
    "                features = pd.concat([features.iloc[:j - 1], _features, features.iloc[j + 2:]])\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "            else:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features)\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = pd.concat([features.iloc[:j - 1], _features])\n",
    "\n",
    "        if len(scores) == 1 and scores[0] > self.forest_threshold:\n",
    "            root = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[0],\n",
    "                right=nodes[1],\n",
    "                relation='root',\n",
    "                proba=scores[0]\n",
    "            )\n",
    "            nodes = [root]\n",
    "\n",
    "        return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_embeddings(embedder, x, maxlen=100):\n",
    "    x_ = [text[:text.rfind('_')] for text in x.split()]\n",
    "    result = np.zeros((embedder.vector_size, maxlen))\n",
    "\n",
    "    for i in range(min(len(x_), maxlen)):\n",
    "        try:\n",
    "            result[i] = embedder[x_[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    DROP_COLUMNS = ['snippet_x', 'snippet_y', 'snippet_x_tmp', 'snippet_y_tmp', 'postags_x', 'postags_y']\n",
    "\n",
    "    def __init__(self, processor, scaler=None, categorical_cols=None, one_hot_encoder=None, label_encoder=None):\n",
    "        self.processor = processor\n",
    "        self.scaler = scaler\n",
    "        self._categorical_cols = categorical_cols\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __call__(self, df, \n",
    "                 annot_text, annot_tokens, annot_sentences, \n",
    "                 annot_lemma, annot_morph, annot_postag, annot_syntax_dep_tree):\n",
    "        x = self.processor(df, \n",
    "                           annot_text, annot_tokens, annot_sentences, \n",
    "                           annot_lemma, annot_morph, annot_postag, annot_syntax_dep_tree)\n",
    "\n",
    "        if self._categorical_cols:\n",
    "            if self.label_encoder:\n",
    "                x[self._categorical_cols] = x[self._categorical_cols].apply(\n",
    "                    lambda col: self.label_encoder.fit_transform(col))\n",
    "\n",
    "            if self.one_hot_encoder:\n",
    "                x_ohe = self.one_hot_encoder.transform(x[self._categorical_cols].values)\n",
    "                x_ohe = pd.DataFrame(x_ohe, x.index,\n",
    "                                     columns=self.one_hot_encoder.get_feature_names(self._categorical_cols))\n",
    "\n",
    "                x = x.join(\n",
    "                    pd.DataFrame(x_ohe, x.index).add_prefix('cat_'), how='right'\n",
    "                ).drop(columns=self._categorical_cols).drop(columns=self.DROP_COLUMNS)\n",
    "\n",
    "        if self.scaler:\n",
    "            return pd.DataFrame(self.scaler.transform(x.values), index=x.index, columns=x.columns)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_data\n",
    "\n",
    "train, test = split_data('data/', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_gold, read_annotation\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features_processor_default import FeaturesProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier = AllenNLPClassifier('models/structure_predictor_lstm/results_all/')\n",
    "label_classifier = SklearnClassifier('models/label_predictor/')\n",
    "features_extractor = FeaturesExtractor(features_processor)\n",
    "predictor = NNTreePredictor(features_extractor, binary_classifier, label_predictor=label_classifier)\n",
    "parser = GreedyRSTParser(predictor, forest_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "\n",
    "cache = {}\n",
    "broken_files = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus) - 1):\n",
    "        #start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "            )\n",
    "        #print(temp)\n",
    "        _edus.append(temp)\n",
    "        last_end = end + 1\n",
    "        \n",
    "    parsed = parser(_edus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['lemma'], \n",
    "                annot['morph'], \n",
    "                annot['postag'], \n",
    "                annot['syntax_dep_tree'],\n",
    "                genre=filename.split('_')[0])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    cache[filename] = (parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval_pd as metric_parseval\n",
    "\n",
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['F1'].mean(), results['recall'].mean(), results['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results[results.filename.str.contains('blog')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results[results.filename.str.contains('news')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results[results.filename.str.contains('ling')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results[results.filename.str.contains('comp')]\n",
    "\n",
    "tmp['F1'].mean(), tmp['recall'].mean(), tmp['precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_golds(filename):\n",
    "    filename = '.'.join(filename.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename)\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=max_id,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "    parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                    annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "    \n",
    "    return (filename,) + metric_parseval(parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(5)\n",
    "result = pool.map(parse_golds, test)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['filename', 'true_pos', 'all_parsed', 'all_gold'], data=result)\n",
    "\n",
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['difference'] = results['all_gold'] - results['all_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('difference', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad file analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/sci.comp_44'\n",
    "\n",
    "edus = read_edus(filename)\n",
    "gold = read_gold(filename)\n",
    "gold = gold.sort_values('snippet_y').drop_duplicates(subset=['snippet_y'])\n",
    "annot = read_annotation(filename)\n",
    "_edus = []\n",
    "last_end = 0\n",
    "for max_id in range(len(edus)):\n",
    "    start = annot['text'].find(edus[max_id], last_end)\n",
    "    end = start + len(edus[max_id])\n",
    "    temp = DiscourseUnit(\n",
    "            id=max_id,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            relation='edu',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            orig_text=annot['text'],\n",
    "            #text=edus[max_id],\n",
    "            proba=1.,\n",
    "            #text=edus[max_id]  #annot_text[nodes[j].start:nodes[j+1].end]\n",
    "        )\n",
    "    _edus.append(temp)\n",
    "    last_end = end\n",
    "\n",
    "parser = GreedyRSTParser(GoldTreePredictor(gold), forest_threshold=0.)\n",
    "#parsed = parser(_edus)\n",
    "\n",
    "parsed = parser(_edus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['postag'], \n",
    "                annot['morph'], \n",
    "                annot['lemma'], \n",
    "                annot['syntax_dep_tree'], \n",
    "                genre=filename.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for tree in parsed:\n",
    "    if tree.relation != 'edu':\n",
    "        print(vars(tree))\n",
    "        counter += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parsed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval, extr_pairs, extr_pairs_forest, _check_snippet_pair_in_dataset, _not_parsed_as_in_gold\n",
    "\n",
    "parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed), columns=['snippet_x', 'snippet_y', 'category_id'])\n",
    "print(parsed_pairs.shape, gold.shape)\n",
    "errors = _not_parsed_as_in_gold(parsed_pairs, gold)\n",
    "\n",
    "def find_edu_number(edus, error):\n",
    "    for i, edu in enumerate(edus):\n",
    "        if error[2].find(edu) > -1:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(find_edu_number(edus, errors.iloc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pairs[parsed_pairs.snippet_x.str.contains(\"В ту пору экзамены можно было сдавать экстерном.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold[gold.snippet_x.str.contains(\"В ту пору экзамены можно было сдавать экстерном.\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
