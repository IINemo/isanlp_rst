{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset for the system introduced in https://www.aclweb.org/anthology/W19-2715/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/IINemo/isanlp.git@discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset for model training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_map = {\n",
    "    'x': 'х',\n",
    "    'y': 'у',\n",
    "}\n",
    "\n",
    "def prepare_token(token):\n",
    "    for key, value in symbol_map.items():\n",
    "        token = token.replace(key, value)\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot2tags(annot, edus):\n",
    "    tags = []\n",
    "    cursor = 0\n",
    "    \n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        sentence_tags = []\n",
    "        previous_first_token = 0\n",
    "        previous_edu = ''\n",
    "        \n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "            \n",
    "            if cursor == len(edus):\n",
    "                is_first_token = False\n",
    "            \n",
    "            else:\n",
    "                is_first_token = False\n",
    "                start_of_sentence = 0\n",
    "                \n",
    "                tmp_edu = prepare_token(edus[cursor])\n",
    "\n",
    "                original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                original_text = prepare_token(original_text)\n",
    "\n",
    "                if tmp_edu.startswith(original_text):\n",
    "                    if previous_edu:\n",
    "                        if annot['text'][annot['tokens'][previous_first_token].begin:annot['tokens'][token].begin].strip() == previous_edu or original_text.lower() == \"сначала\":\n",
    "                            is_first_token = True\n",
    "                            previous_first_token = token\n",
    "                            previous_edu = tmp_edu\n",
    "                            cursor += 1\n",
    "                    else:\n",
    "                        is_first_token = True\n",
    "                        previous_first_token = token\n",
    "                        previous_edu = tmp_edu\n",
    "                        cursor += 1\n",
    "                \n",
    "            tag = 'BeginSeg=Yes' if is_first_token else '_'\n",
    "            sentence_tags.append(tag)\n",
    "            \n",
    "        tags.append(sentence_tags)\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.utils.annotation_conll_converter import AnnotationCONLLConverter\n",
    "\n",
    "converter = AnnotationCONLLConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.3886792452830189,\tin test: 0.3939393939393939\n",
      "ling in train: 0.1509433962264151,\tin test: 0.15151515151515152\n",
      "comp in train: 0.1471698113207547,\tin test: 0.15151515151515152\n",
      "blog in train: 0.3132075471698113,\tin test: 0.3181818181818182\n",
      "preprocess train set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d650726c19fb43fc9871848a50148467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=265), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sci.ling_21 ::: occured very long sentence; truncate to 230 tokens.\n",
      "data/sci.comp_8 ::: occured very long sentence; truncate to 230 tokens.\n",
      "data/blogs_2 ::: occured very long sentence; truncate to 230 tokens.\n",
      "\n",
      "preprocess test set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b67fb47fb4616807dfb38d49d6fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=67), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sci.ling_18 ::: occured very long sentence; truncate to 230 tokens.\n",
      "data/sci.ling_28 ::: occured very long sentence; truncate to 230 tokens.\n",
      "data/sci.comp_40 ::: occured very long sentence; truncate to 230 tokens.\n",
      "data/sci.comp_54 ::: occured very long sentence; truncate to 230 tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_data\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_annotation, read_edus\n",
    "import re\n",
    "\n",
    "\n",
    "train, test = split_data('data/')\n",
    "TRAIN_FILE = 'rus.rst.rrt_train.conll'\n",
    "TEST_FILE = 'rus.rst.rrt_test.conll'\n",
    "MAX_LEN = 230\n",
    "\n",
    "\n",
    "def preprocess(files, train=True):\n",
    "    print(f'preprocess {\"train\" if train else \"test\"} set')\n",
    "    \n",
    "    with open(TRAIN_FILE if train else TEST_FILE, 'w') as fo:\n",
    "        for filename in tqdm(files):\n",
    "            filename = filename.replace('.edus', '')\n",
    "            annot = read_annotation(filename)\n",
    "            edus = read_edus(filename)\n",
    "            tags = annot2tags(annot, edus)\n",
    "\n",
    "            sentence = 0\n",
    "            token = 0\n",
    "\n",
    "            for string in converter(filename.replace('data/', ''), annot):\n",
    "                if string == '\\n':\n",
    "                    fo.write(string)\n",
    "                    sentence += 1\n",
    "                    token = 0\n",
    "\n",
    "                elif string.startswith('# newdoc id ='):\n",
    "                    fo.write(string + '\\n')\n",
    "\n",
    "                else:\n",
    "                    if ' ' in string:\n",
    "                        string = re.sub(r' .*\\t', '\\t', string)\n",
    "                    if 'www' in string:\n",
    "                        string = re.sub(r'www[^\\t]*', '_html_', string)\n",
    "                    if 'http' in string:\n",
    "                        string = re.sub(r'http[^ \\t]*', '_html_', string)\n",
    "                    if '[' in string:\n",
    "                        string = re.sub(r'\\[(\\d+[ ;,с\\.]*)+\\]', '_ref_', string)\n",
    "#                         if token > 0:\n",
    "                    fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "\n",
    "#                         write first token only if it is the EDU boundary\n",
    "#                         elif tags[sentence][token] == 'BeginSeg=Yes':\n",
    "#                             fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "                    #else:\n",
    "                        # skip next {len(sentence)} tokens\n",
    "                        # ToDO:\n",
    "                    token += 1\n",
    "\n",
    "                if token == MAX_LEN:\n",
    "                    print(filename + ' ::: occured very long sentence; truncate to ' + str(MAX_LEN) + ' tokens.')\n",
    "                    fo.write('\\n')\n",
    "                    sentence += 1\n",
    "                    token = 0\n",
    "                    break\n",
    "\n",
    "preprocess(train)\n",
    "preprocess(test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_FILE\" \"$TEST_FILE\"\n",
    "\n",
    "export TONY_PATH=\"../tony/\"\n",
    "\n",
    "cp ${1} ${TONY_PATH}/data/rus.rst.rrt/${1}\n",
    "cp ${2} ${TONY_PATH}/data/rus.rst.rrt/${2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_FILE\" \"$TEST_FILE\"\n",
    "\n",
    "cp ${1} ${1}.002.backup\n",
    "cp ${2} ${2}.002.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scripts for the pipeline integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "\n",
    "class AllenNLPSegmentator:\n",
    "    TEXT = 0\n",
    "    TOKENS = 1\n",
    "    SENTENCES = 2\n",
    "    LEMMA = 3\n",
    "    POSTAG = 4\n",
    "    SYNTAX_DEP_TREE = 5\n",
    "    \n",
    "    def __init__(self, model_dir_path):\n",
    "        self._model_path = os.path.join(model_dir_path, 'tony_segmentator', 'model.tar.gz')\n",
    "        self.predictor = Predictor.from_path(self._model_path)\n",
    "        self._separator = 'U-S'\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._build_discourse_units(args[self.TEXT], args[self.TOKENS], \n",
    "                                           self._predict(args[self.TOKENS], args[self.SENTENCES]))\n",
    "        \n",
    "    def _predict(self, tokens, sentences):\n",
    "        \"\"\"\n",
    "        :return: numbers of tokens predicted as EDU left boundaries\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for sentence in sentences:\n",
    "            result += self.predictor.predict(' '.join([token.text for token in tokens[sentence.begin:sentence.end]])\n",
    "                                            )['tags']\n",
    "            \n",
    "        result = np.array(result)\n",
    "        return np.argwhere(result == self._separator)[:, 0]\n",
    "\n",
    "    def _build_discourse_units(self, text, tokens, numbers):\n",
    "        \"\"\"\n",
    "        :param text: original text\n",
    "        :param list tokens: isanlp.annotation.Token\n",
    "        :param numbers: positions of tokens predicted as EDU left boundaries (beginners)\n",
    "        :return: list of DiscourseUnit\n",
    "        \"\"\"\n",
    "        \n",
    "        edus = []\n",
    "    \n",
    "        if numbers.shape[0]:\n",
    "            for i in range(0, len(numbers)-1):\n",
    "                new_edu = DiscourseUnit(i,\n",
    "                                        start=tokens[numbers[i]].begin,\n",
    "                                        end=tokens[numbers[i+1]].begin - 1,\n",
    "                                        text=text[tokens[numbers[i]].begin:tokens[numbers[i+1]].begin],\n",
    "                                        relation='elementary')\n",
    "                edus.append(new_edu)\n",
    "\n",
    "            if numbers.shape[0] == 1:\n",
    "                i = -1\n",
    "            \n",
    "            new_edu = DiscourseUnit(i+1,\n",
    "                            start=tokens[numbers[-1]].begin,\n",
    "                            end=len(text),\n",
    "                            text=text[tokens[numbers[-1]].begin:],\n",
    "                            relation='elementary')\n",
    "            edus.append(new_edu)\n",
    "\n",
    "        return edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentator = AllenNLPSegmentator('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_annotation, read_edus\n",
    "annot = read_annotation('data/news1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = segmentator(annot['text'],\n",
    "                  annot['tokens'], annot['sentences'], \n",
    "                  annot['lemma'], annot['postag'], \n",
    "                  annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edu in res[15:25]:\n",
    "    print(edu.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/tony_segmentator ../isanlp_rst/models/tony_segmentator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map UD tags to penn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export TONY_PATH=../tony/\n",
    "export CONV_PATH=${TONY_PATH}/code/contextual_embeddings/conv2ner.py \n",
    "\n",
    "cp ${CONV_PATH} ${CONV_PATH}.backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tony/code/contextual_embeddings/conv2ner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/conv2ner.py \n",
    "\n",
    "\"\"\"\n",
    "Convert to ner Connl format to use allennlp dataset reader\n",
    "\n",
    "basically, just skip lines between docs, strip to 4 fields with words as 1st and tag as last, and format as BIO\n",
    "\n",
    "TODO: try BIOUL (L=last, U=unit entity = 1 token)\n",
    "\"\"\"\n",
    "import sys\n",
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"filepath\", help=\"path to file to convert\")\n",
    "parser.add_argument(\"--lemmatize\", default=False, action='store_true', help=\"to use with conll input: replace token with its lemma (useful for turk)\")\n",
    "parser.add_argument(\"--mark-end\", default=False, action='store_true', help=\"add explicit label for end of segment\")\n",
    "parser.add_argument(\"--split-too-long\", default=[False,180], help=\"split sentences longer than threshold\",nargs=2)\n",
    "parser.add_argument(\"--input-format\",default=\"tok\",help=\"input format: tok, split.tok, conll\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "maptags = {\"_\":\"O\",\n",
    "           \"BeginSeg=Yes\": \"B-S\",\n",
    "           \"Seg=B-Conn\":\"B-Conn\",\n",
    "           \"Seg=I-Conn\":\"I-Conn\",\n",
    "           \"SpaceAfter=No\":\"O\",\n",
    "           \"Typo=Yes\":\"O\",\n",
    "           }\n",
    "\n",
    "tags_map = {\n",
    "    'NOUN': 'NN',\n",
    "    'PROPN': 'NNP',\n",
    "    'PRON': 'EX',\n",
    "    #'VERB': 'VB',  # simple way\n",
    "    #'PUNCT': '``', # punctuation must be processed separately\n",
    "    'ADV': 'RB',\n",
    "    'ADP': 'RP',\n",
    "    'CCONJ': 'CC',\n",
    "    'ADJ': 'JJ',\n",
    "    'AUX': 'VB',\n",
    "    'SCONJ': 'CC',\n",
    "    'DET': 'DT',\n",
    "    'PART': 'TO',\n",
    "    'NUM': 'CD',\n",
    "    'SYM': 'SYM',\n",
    "    'X': 'LS',\n",
    "}\n",
    "\n",
    "punct_map = {\n",
    "    \"''\": \"''\",\n",
    "    \"'\": \"''\",\n",
    "    '\"': \"''\",\n",
    "    \",\": \",\",\n",
    "    \".\": \".\",\n",
    "    \"?\": \".\",\n",
    "    \"!\": \".\",\n",
    "    \"--\": \":\",\n",
    "    \":\": \":\",\n",
    "    \";\": \":\",\n",
    "    \"...\": \":\",\n",
    "    \"-\": \"HYPH\",\n",
    "    \"(\": \"-LRB-\",\n",
    "    \"«\": \"''\",\n",
    "    \"[\": \"[\",\n",
    "    \")\": \"-RRB-\",\n",
    "    \"»\": \"''\",\n",
    "    ']': \"]\"\n",
    "}\n",
    "\n",
    "verb_variants = [\n",
    "    (\"Tense=Past|VerbForm=Fin\", \"VBD\"),\n",
    "    (\"Tense=Past|VerbForm=Part\", \"VBN\"),\n",
    "    (\"Tense=Pres|VerbForm=Part\", \"VBG\"),\n",
    "    (\"Tense=Pres|VerbForm=Fin\", \"VBP\"),\n",
    "    (\"Tense=Imp|VerbForm=Fin\", \"VB\")\n",
    "]\n",
    "\n",
    "def convert_tag(tag, lemma, morph):\n",
    "    new_tag = tags_map.get(tag)\n",
    "    \n",
    "    if new_tag:\n",
    "        return new_tag\n",
    "    \n",
    "    new_tag = punct_map.get(lemma)\n",
    "    \n",
    "    if new_tag:\n",
    "        return new_tag\n",
    "    \n",
    "    for verb_var in verb_variants:\n",
    "        if verb_var[0] in morph:\n",
    "            return verb_var[1]\n",
    "        \n",
    "    if tag == \"VERB\":\n",
    "        return \"VB\"\n",
    "    \n",
    "    return 'NN'\n",
    "\n",
    "# \n",
    "MARK_END = args.mark_end\n",
    "# take lemmas instead of token forms (useful for turkish)\n",
    "# also tag all proper nouns with same token\n",
    "LEMMATIZE = args.lemmatize\n",
    "# split for too long sentences (default 180) for bert\n",
    "SPLIT_TOO_LONG= args.split_too_long[0]\n",
    "THRESHOLD = int(args.split_too_long[1])\n",
    "\n",
    "#filepath = sys.argv[1]\n",
    "filepath = args.filepath\n",
    "\n",
    "input_format = args.input_format\n",
    "\n",
    "\n",
    "if SPLIT_TOO_LONG:\n",
    "    print(\"warning: too-long sentence splitting mode = ON \",file=sys.stderr)\n",
    "\n",
    "\n",
    "with open(filepath) as f:\n",
    "    start_doc = False\n",
    "    res = []\n",
    "    for line in f:\n",
    "        if \"\\t\" not in line:\n",
    "            res.append([]) # [line.strip()])\n",
    "            start_doc = True\n",
    "        #elif line.strip()==\"\":\n",
    "        #    res.append([])\n",
    "        #    start_doc = True\n",
    "        else:\n",
    "            fields = line.strip().split()\n",
    "            #print(fields,file=sys.stderr)\n",
    "            token_number = int(fields[0].split(\"-\")[0])\n",
    "            if SPLIT_TOO_LONG and token_number>THRESHOLD:\n",
    "                # sentence too long: insert a newline to make a separate sequence\n",
    "                res.append([])\n",
    "            w = fields[1] if not(LEMMATIZE) else fields[2]\n",
    "            label = fields[-1].split(\"|\")[0]\n",
    "            if input_format==\"conll\":\n",
    "                if LEMMATIZE and fields[3]==\"PROPN\":\n",
    "                    w = \"NAME\"\n",
    "#                 print('field[3] =', fields[3])\n",
    "#                 print('field[2] =', fields[2])\n",
    "                pos = convert_tag(fields[3], fields[2], fields[5])\n",
    "            else:\n",
    "                pos = convert_tag(fields[3], fields[2], fields[5])\n",
    "            tag = maptags.get(label,\"O\")\n",
    "            #if start_doc:\n",
    "            #    tag = \"B-S\"\n",
    "            if not(start_doc) and MARK_END and tag==\"B-S\" and res[-1][-1]!=\"B-S\":\n",
    "                # then, previous token label is set to B-E to signal end of previous segment\n",
    "                res[-1][-1] = \"B-E\"\n",
    "            start_doc = False\n",
    "            if label not in maptags:\n",
    "                print(\"warning, strange label \",label,file=sys.stderr)\n",
    "            res.append([w,pos,\"O\",tag])\n",
    "            \n",
    "    for line in res:\n",
    "        print(\"\\t\".join(line))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
