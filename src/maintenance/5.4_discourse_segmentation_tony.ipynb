{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset for the system introduced in https://www.aclweb.org/anthology/W19-2715/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/IINemo/isanlp.git@discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset for model training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot2tags(annot, edus):\n",
    "    tags = []\n",
    "    cursor = 0\n",
    "    \n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        sentence_tags = []\n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "            \n",
    "            if cursor == len(edus):\n",
    "                is_first_token = False\n",
    "            \n",
    "            else:\n",
    "                is_first_token = False\n",
    "                start_of_sentence = 0\n",
    "\n",
    "                if token == annot['sentences'][sentence].begin:\n",
    "                    is_first_token = True\n",
    "                    cursor += 1\n",
    "                else:\n",
    "                    original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                    if edus[cursor].startswith(original_text):\n",
    "                        is_first_token = True\n",
    "                        cursor += 1\n",
    "                \n",
    "            tag = 'BeginSeg=Yes' if is_first_token else '_'\n",
    "            sentence_tags.append(tag)\n",
    "            \n",
    "        tags.append(sentence_tags)\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.utils.annotation_conll_converter import AnnotationCONLLConverter\n",
    "\n",
    "converter = AnnotationCONLLConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_data\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_annotation, read_edus\n",
    "import re\n",
    "\n",
    "\n",
    "train, test = split_data('data/')\n",
    "TRAIN_FILE = 'rus.rst.rrt_train.conll'\n",
    "TEST_FILE = 'rus.rst.rrt_test.conll'\n",
    "MAX_LEN = 220\n",
    "\n",
    "\n",
    "def preprocess(files, train=True):\n",
    "    print(f'preprocess {\"train\" if train else \"test\"} set')\n",
    "    \n",
    "    with open(TRAIN_FILE if train else TEST_FILE, 'w') as fo:\n",
    "        for filename in tqdm(files):\n",
    "            filename = filename.replace('.edus', '')\n",
    "            annot = read_annotation(filename)\n",
    "            edus = read_edus(filename)\n",
    "            tags = annot2tags(annot, edus)\n",
    "            \n",
    "            sentence = 0\n",
    "            token = 0\n",
    "\n",
    "            for string in converter(filename.replace('data/', ''), annot):\n",
    "                if string == '\\n':\n",
    "                    fo.write(string)\n",
    "                    sentence += 1\n",
    "                    token = 0\n",
    "                    \n",
    "                elif string.startswith('# newdoc id ='):\n",
    "                    fo.write(string + '\\n')\n",
    "                    \n",
    "                else:\n",
    "                    if ' ' in string:\n",
    "                        string = re.sub(r' .*\\t', '\\t', string)\n",
    "                    if 'www' in string:\n",
    "                        string = re.sub(r'www[^ \\t]*', '_html_', string)\n",
    "                    if 'http' in string:\n",
    "                        string = re.sub(r'http[^ \\t]*', '_html_', string)\n",
    "                    fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "                    token += 1\n",
    "                \n",
    "                if token > MAX_LEN:\n",
    "                    print(filename + ' ::: occured very long sentence; truncate to ' + str(MAX_LEN) + ' tokens.')\n",
    "                    break\n",
    "\n",
    "preprocess(train)\n",
    "preprocess(test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_FILE\" \"$TEST_FILE\"\n",
    "\n",
    "export TONY_PATH=\"../tony/\"\n",
    "\n",
    "cp ${1} ${TONY_PATH}/data/rus.rst.rrt/${1}\n",
    "cp ${2} ${TONY_PATH}/data/rus.rst.rrt/${2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scripts for the pipeline integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "\n",
    "class AllenNLPSegmentator:\n",
    "    TEXT = 0\n",
    "    TOKENS = 1\n",
    "    SENTENCES = 2\n",
    "    LEMMA = 3\n",
    "    POSTAG = 4\n",
    "    SYNTAX_DEP_TREE = 5\n",
    "    \n",
    "    def __init__(self, model_dir_path):\n",
    "        self._model_path = os.path.join(model_dir_path, 'tony_segmentator', 'model.tar.gz')\n",
    "        self.predictor = Predictor.from_path(self._model_path)\n",
    "        self._separator = 'U-S'\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._build_discourse_units(args[self.TEXT], args[self.TOKENS], \n",
    "                                           self._predict(args[self.TOKENS], args[self.SENTENCES]))\n",
    "        \n",
    "    def _predict(self, tokens, sentences):\n",
    "        \"\"\"\n",
    "        :return: numbers of tokens predicted as EDU left boundaries\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for sentence in sentences:\n",
    "            result += self.predictor.predict(' '.join([token.text for token in tokens[sentence.begin:sentence.end]])\n",
    "                                            )['tags']\n",
    "            \n",
    "        result = np.array(result)\n",
    "        return np.argwhere(result == self._separator)[:, 0]\n",
    "\n",
    "    def _build_discourse_units(self, text, tokens, numbers):\n",
    "        \"\"\"\n",
    "        :param text: original text\n",
    "        :param list tokens: isanlp.annotation.Token\n",
    "        :param numbers: positions of tokens predicted as EDU left boundaries (beginners)\n",
    "        :return: list of DiscourseUnit\n",
    "        \"\"\"\n",
    "        \n",
    "        edus = []\n",
    "    \n",
    "        if numbers.shape[0]:\n",
    "            for i in range(0, len(numbers)-1):\n",
    "                new_edu = DiscourseUnit(i,\n",
    "                                        start=tokens[numbers[i]].begin,\n",
    "                                        end=tokens[numbers[i+1]].begin - 1,\n",
    "                                        text=text[tokens[numbers[i]].begin:tokens[numbers[i+1]].begin],\n",
    "                                        relation='elementary')\n",
    "                edus.append(new_edu)\n",
    "\n",
    "            if numbers.shape[0] == 1:\n",
    "                i = -1\n",
    "            \n",
    "            new_edu = DiscourseUnit(i+1,\n",
    "                            start=tokens[numbers[-1]].begin,\n",
    "                            end=len(text),\n",
    "                            text=text[tokens[numbers[-1]].begin:],\n",
    "                            relation='elementary')\n",
    "            edus.append(new_edu)\n",
    "\n",
    "        return edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentator = AllenNLPSegmentator('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_annotation, read_edus\n",
    "annot = read_annotation('data/news1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = segmentator(annot['text'],\n",
    "                  annot['tokens'], annot['sentences'], \n",
    "                  annot['lemma'], annot['postag'], \n",
    "                  annot['syntax_dep_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edu in res[15:25]:\n",
    "    print(edu.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/tony_segmentator ../isanlp_rst/models/tony_segmentator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
