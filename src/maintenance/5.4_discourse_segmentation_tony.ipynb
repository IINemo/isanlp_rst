{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset for the system introduced in https://www.aclweb.org/anthology/W19-2715/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/IINemo/isanlp.git@discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset for model training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_map = {\n",
    "    'x': '—Ö',\n",
    "    'X': 'X',\n",
    "    'y': '—É',\n",
    "    '‚Äî': '-',\n",
    "    '‚Äú': '¬´',\n",
    "    '‚Äò': '¬´',\n",
    "    '‚Äù': '¬ª',\n",
    "    '‚Äô': '¬ª',\n",
    "    'üòÜ': 'üòÑ',\n",
    "    'üòä': 'üòÑ',\n",
    "    'üòë': 'üòÑ',\n",
    "    'üòî': 'üòÑ',\n",
    "    'üòâ': 'üòÑ',\n",
    "    '‚ùó': 'üòÑ',\n",
    "    'ü§î': 'üòÑ',\n",
    "    'üòÖ': 'üòÑ',\n",
    "    '‚öì': 'üòÑ',\n",
    "    'Œµ': 'Œ±',\n",
    "    'Œ∂': 'Œ±',\n",
    "    'Œ∑': 'Œ±',\n",
    "    'Œº': 'Œ±',\n",
    "    'Œ¥': 'Œ±',\n",
    "    'Œª': 'Œ±',\n",
    "    'ŒΩ': 'Œ±',\n",
    "    'Œ≤': 'Œ±',\n",
    "    'Œ≥': 'Œ±',\n",
    "    'ŒΩ': 'Œ±',\n",
    "    '„Å®': 'Â∞ã',\n",
    "    '„ÅÆ': 'Â∞ã',\n",
    "    'Á•û': 'Â∞ã',\n",
    "    'Èö†': 'Â∞ã',\n",
    "    '„Åó': 'Â∞ã',\n",
    "    '—ë': '–µ',\n",
    "    '–Å': '–ï',\n",
    "    '<': '–º–µ–Ω–µ–µ',\n",
    "    '>': '–±–æ–ª–µ–µ'\n",
    "}\n",
    "# for i in range(127744, 128592):\n",
    "#     symbol_map[chr(i)] = chr(128512)\n",
    "    \n",
    "# for i in range(9312, 10176):\n",
    "#     symbol_map[chr(i)] = chr(128512)\n",
    "    \n",
    "# for i in range(913, 970):\n",
    "#     symbol_map[chr(i)] = 'Œ±'\n",
    "\n",
    "def prepare_token(token):\n",
    "    for key, value in symbol_map.items():\n",
    "        token = token.replace(key, value)\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot2tags(annot, edus):\n",
    "    tags = []\n",
    "    cursor = 0\n",
    "\n",
    "#     for i, edu in enumerate(edus):\n",
    "#         if prepare_token(edu).find(prepare_token(annot['text'][annot['tokens'][0].begin:annot['tokens'][0].end])) != -1:\n",
    "#             cursor = i         \n",
    "    \n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        sentence_tags = []\n",
    "        previous_first_token = 0\n",
    "        previous_edu = ''\n",
    "\n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "\n",
    "            if cursor == len(edus):\n",
    "                is_first_token = False\n",
    "\n",
    "            else:\n",
    "                is_first_token = False\n",
    "                \n",
    "                tmp_edu = prepare_token(edus[cursor])\n",
    "                annot['text'] = annot['text'].replace('–±–∏–ª–∏–µ ', '–û–±–∏–ª–∏–µ ')\n",
    "                original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                original_text = prepare_token(original_text).strip()\n",
    "\n",
    "                if tmp_edu.startswith(original_text):\n",
    "                    if previous_edu:\n",
    "                        if prepare_token(annot['text'][annot['tokens'][previous_first_token].begin:annot['tokens'][\n",
    "                        token].begin].strip()) == previous_edu or original_text.lower() in [\"—Å–Ω–∞—á–∞–ª–∞\", \"–∫–≤–∞—Ç–∞—Ö–µ–≤–∏\", \n",
    "                                                                                           \"—Ü–µ–ª—ã–π\", \"–¥–ª—è\", \n",
    "                                                                                           \"–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π\", \"—Ç–æ–≥–¥–∞\",\n",
    "                                                                                           \"–¥–≤–∞\", \"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º\",\n",
    "                                                                                            \"—Ö–æ—Ç—è\", \"—Ö–æ—Ç—å\",\n",
    "                                                                                            \"–æ–±–∏–ª–∏–µ\", \"–∞–∫—Ç–∏–≤–Ω–æ–µ\",\n",
    "                                                                                            \"–º–µ–Ω–µ–µ\"\n",
    "                                                                                           ] or tmp_edu in [\n",
    "                            \"–∏ –¥–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–æ–ª—å—Ü–∞ (Œ±, Œ±).\",\n",
    "                            \"–£ —ç—Ç–æ–≥–æ –∫–æ–ª—å—Ü–∞ —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π —ç–∫—Å—Ü–µ–Ω—Ç—Ä–∏—Å–∏—Ç–µ—Ç –∏–∑ –≤—Å–µ—Ö,\",\n",
    "                            prepare_token(\"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏—á–µ–∫ –≤ —ç—Ç–æ–º –∫–æ–ª—å—Ü–µ 0,2‚Äî 20 –º–µ—Ç—Ä–æ–≤,\"),\n",
    "                            \"(–ü–µ—Ä–µ–≤–æ–¥ –ù–≥—É–µ–Ω –¢—Ö–∏ –¢—Ö—É–∏ –ß–∞–º))\",\n",
    "                            \"–≤ –ß—É–≤–∞—à–∏–∏ —Å–æ—Å—Ç–æ—è–ª—Å—è –Ω–µ–æ–±—ä—è–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫ —Å–∞—Ö–∞ –∫—É–ª—å—Ç—É—Ä—ã.\",\n",
    "                            \"–∏ –≤—ã–ø—É—Å—Ç–∏–ª–∞ —Å–±–æ—Ä–Ω–∏–∫ —è–∫—É—Ç—Å–∫–æ–π –ø–æ—ç–∑–∏–∏ ¬´–ñ–µ–º—á—É–∂–∏–Ω–∞ –°–∞—Ö–∏¬ª (–°–∞—Ö–∞ –∞—Ö–∞—Ö”ó, 1996),\",\n",
    "                            \"–û–¥–Ω–∞–∫–æ –∏ –ø–µ—Å–Ω—è –µ–º—É –∫–∞–∂–µ—Ç—Å—è –∫–∞–∫–æ–π-—Ç–æ –±–µ—Å—Ü–≤–µ—Ç–Ω–æ–π. –í–∏–¥–∏–º–æ, –ø–æ—Ç–æ–º—É, —á—Ç–æ –æ–Ω–∞ —Ç–≤–æ—Ä–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ—Å–æ–º, –∞ –Ω–µ –¥—É—à–æ–π [–¢–∞–º –∂–µ];\",\n",
    "                            \"–∏ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –∫–∞–∫ –∏ –≤—Å–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç—É—Ä—Ü–∏–∑–º—ã,\",\n",
    "                            prepare_token(\"–ù–æ–≤–æ–±—Ä–∞–Ω—â–∏–Ω–µ - —Å–æ–ª–¥–∞—Ç—á–∏–Ω–µ, –≠–ø–æ–ª–µ—Ç—â–∏–Ω–µ - –±–æ–±—ë—Ä—â–∏–Ω–µ, –í—Å–µ–π –ø–µ—Ö–æ—Ç—â–∏–Ω–µ, –ø–æ–º–æ—Ä—â–∏–Ω–µ, –º–µ–Ω–µ–µ –•–ª–µ–±–æ—Ä–æ–±—â–∏–Ω–µ, –≤–æ–µ–Ω—â–∏–Ω–µ, –ß—Ç–æ —Å –∞—Ä–º–µ–π—Å–∫–æ–π –¥–æ–ª–µ–π –≤–µ–Ω—á–∞–Ω—ã –±–æ–ª–µ–µ \"),\n",
    "                            \"–∫–æ—Ç–æ—Ä—ã–π —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –≤—Ä–µ–¥–Ω—ã–µ —É–ª—å—Ç—Ä–∞—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–µ –ª—É—á–∏ –°–æ–ª–Ω—Ü–∞.\",\n",
    "                            \"–∞ —Ç–∞–∫–∂–µ –∫—Ä–µ–º-—É—Ö–æ–¥ –¥–ª—è –≤–æ–ª–æ—Å ¬´–ü–µ–ø—Ç–∏–¥—ã —à–µ–ª–∫–∞ –∏ –∏—Ä–∞–Ω—Å–∫–∞—è —Ö–Ω–∞¬ª.\",\n",
    "                            \"–ö 2010 –≥–æ–¥—É –æ–∫–æ–ª–æ 100 –æ–∑–æ–Ω–æ—Ä–∞–∑—Ä—É—à–∞—é—â–∏—Ö –≤–µ—â–µ—Å—Ç–≤, –≤–∫–ª—é—á–∞—è –•–§–£, –±—É–¥—É—Ç —Å–Ω—è—Ç—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –ø–æ–≤—Å–µ–º–µ—Å—Ç–Ω–æ.\"\n",
    "                        ]:\n",
    "                            is_first_token = True\n",
    "                            previous_first_token = token\n",
    "                            previous_edu = tmp_edu\n",
    "                            cursor += 1\n",
    "                    else:\n",
    "                        is_first_token = True\n",
    "                        previous_first_token = token\n",
    "                        previous_edu = tmp_edu\n",
    "                        cursor += 1\n",
    "\n",
    "            tag = 'BeginSeg=Yes' if is_first_token else '_'\n",
    "            sentence_tags.append(tag)\n",
    "\n",
    "        tags.append(sentence_tags)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.utils.annotation_conll_converter import AnnotationCONLLConverter\n",
    "\n",
    "converter = AnnotationCONLLConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation import Token, Sentence\n",
    "\n",
    "\n",
    "def split_by_paragraphs(annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag, annot_ud_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def split_on_two(sents, boundary):\n",
    "            list_sum = lambda l: sum([len(sublist) for sublist in l])\n",
    "\n",
    "            i = 1\n",
    "            while list_sum(sents[:i]) < boundary and i < len(sents):\n",
    "                i += 1\n",
    "\n",
    "            intersentence_boundary = min(len(sents[i - 1]), boundary - list_sum(sents[:i - 1]))\n",
    "            return (sents[:i - 1] + [sents[i - 1][:intersentence_boundary]], \n",
    "                    [sents[i - 1][intersentence_boundary:]] + sents[i:])\n",
    "        \n",
    "        def recount_sentences(chunk):\n",
    "            sentences = []\n",
    "            lemma = []\n",
    "            morph = []\n",
    "            postag = []\n",
    "            ud_postag = []\n",
    "            syntax_dep_tree = []\n",
    "            tokens_cursor = 0\n",
    "            local_cursor = 0\n",
    "\n",
    "            for i, sent in enumerate(chunk['syntax_dep_tree']):\n",
    "                if len(sent) > 0:\n",
    "                    sentences.append(Sentence(tokens_cursor, tokens_cursor + len(sent)))\n",
    "                    lemma.append(chunk['lemma'][i])\n",
    "                    morph.append(chunk['morph'][i])\n",
    "                    postag.append(chunk['postag'][i])\n",
    "                    ud_postag.append(chunk['ud_postag'][i])\n",
    "                    syntax_dep_tree.append(chunk['syntax_dep_tree'][i])\n",
    "                    tokens_cursor += len(sent)\n",
    "\n",
    "            chunk['sentences'] = sentences\n",
    "            chunk['lemma'] = lemma\n",
    "            chunk['morph'] = morph\n",
    "            chunk['postag'] = postag\n",
    "            chunk['ud_postag'] = ud_postag\n",
    "            chunk['syntax_dep_tree'] = syntax_dep_tree\n",
    "            \n",
    "            return chunk\n",
    "\n",
    "        chunks = []\n",
    "        prev_right_boundary = -1\n",
    "\n",
    "        for i, token in enumerate(annot_tokens[:-1]):\n",
    "\n",
    "            if '\\n' in annot_text[token.end:annot_tokens[i + 1].begin]:\n",
    "                if prev_right_boundary > -1:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[annot_tokens[prev_right_boundary].end:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[prev_right_boundary + 1:i + 1]\n",
    "                    }\n",
    "                else:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[:i + 1]\n",
    "                    }\n",
    "\n",
    "                lemma, annot_lemma = split_on_two(annot_lemma, i - prev_right_boundary)\n",
    "                morph, annot_morph = split_on_two(annot_morph, i - prev_right_boundary)\n",
    "                postag, annot_postag = split_on_two(annot_postag, i - prev_right_boundary)\n",
    "                ud_postag, annot_ud_postag = split_on_two(annot_ud_postag, i - prev_right_boundary)\n",
    "                syntax_dep_tree, annot_syntax_dep_tree = split_on_two(annot_syntax_dep_tree, i - prev_right_boundary)\n",
    "\n",
    "                chunk.update({\n",
    "                    'lemma': lemma,\n",
    "                    'morph': morph,\n",
    "                    'postag': postag,\n",
    "                    'ud_postag': ud_postag,\n",
    "                    'syntax_dep_tree': syntax_dep_tree,\n",
    "                })\n",
    "                chunks.append(recount_sentences(chunk))\n",
    "\n",
    "                prev_right_boundary = i  # number of last token in the last chunk\n",
    "\n",
    "        chunk = {\n",
    "            'text': annot_text[annot_tokens[prev_right_boundary].end:].strip(),\n",
    "            'tokens': annot_tokens[prev_right_boundary + 1:],\n",
    "            'lemma' : annot_lemma,\n",
    "            'morph': annot_morph,\n",
    "            'postag': annot_postag,\n",
    "            'ud_postag': annot_ud_postag,\n",
    "            'syntax_dep_tree': annot_syntax_dep_tree,\n",
    "        }\n",
    "        \n",
    "        chunks.append(recount_sentences(chunk))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_annotation, read_edus\n",
    "import re\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')\n",
    "TRAIN_FILE = 'rus.rst.rrt_train.conll'\n",
    "DEV_FILE = 'rus.rst.rrt_dev.conll'\n",
    "TEST_FILE = 'rus.rst.rrt_test.conll'\n",
    "MAX_LEN = 230\n",
    "\n",
    "\n",
    "def preprocess(files, train=True, dev=False):\n",
    "    print(f'preprocess {\"train\" if train else \"test\"} set')\n",
    "\n",
    "    output_file = DEV_FILE if dev else TRAIN_FILE if train else TEST_FILE\n",
    "    with open(output_file, 'w') as fo:\n",
    "        for filename in tqdm(files):\n",
    "            filename = filename.replace('.edus', '')\n",
    "            annot = read_annotation(filename)  # split as well  ToDO:\n",
    "            edus = read_edus(filename)\n",
    "            last_edu = 0\n",
    "            # tags = annot2tags(annot, edus)\n",
    "\n",
    "            for i, chunk in enumerate(split_by_paragraphs(  # self,\n",
    "                    annot['text'],\n",
    "                    annot['tokens'],\n",
    "                    annot['sentences'],\n",
    "                    annot['lemma'],\n",
    "                    annot['morph'],\n",
    "                    annot['postag'],\n",
    "                    annot['ud_postag'],\n",
    "                    annot['syntax_dep_tree'])):\n",
    "\n",
    "                sentence = 0\n",
    "                token = 0\n",
    "                chunk['text'] = annot['text']\n",
    "                #edus = \n",
    "                tags = annot2tags(chunk, edus[last_edu:])\n",
    "                \n",
    "                for string in converter(filename.replace('data/', ''), chunk):\n",
    "                    #print(string)\n",
    "                    if string.startswith('# newdoc id ='):\n",
    "                        sentence = 0\n",
    "                        token = 0\n",
    "                        fo.write(string + '\\n')\n",
    "\n",
    "                    elif string == '\\n':\n",
    "                        fo.write(string)\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "\n",
    "                    else:\n",
    "                        if ' ' in string:\n",
    "                            string = re.sub(r' .*\\t', '\\t', string)\n",
    "                        if 'www' in string:\n",
    "                            string = re.sub(r'www[^\\t]*', '_html_', string)\n",
    "                        if 'http' in string:\n",
    "                            string = re.sub(r'http[^ \\t]*', '_html_', string)\n",
    "                            \n",
    "                        string = prepare_token(string)                        \n",
    "                        fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "                        \n",
    "                        if tags[sentence][token] != '_':\n",
    "                            last_edu += 1\n",
    "                        \n",
    "                        token += 1\n",
    "\n",
    "                    if token == MAX_LEN:\n",
    "                        print(filename + ' ::: occured very long sentence; truncate to ' + str(MAX_LEN) + ' tokens.')\n",
    "                        fo.write('\\n')\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "                        break\n",
    "\n",
    "\n",
    "preprocess(train)\n",
    "preprocess(dev, dev=True)\n",
    "preprocess(test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! ls -laht *.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir datasets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_FILE\" \"$DEV_FILE\" \"$TEST_FILE\"\n",
    "\n",
    "export TONY_PATH=\"../tony/\"\n",
    "\n",
    "cp ${1} ${TONY_PATH}/data/rus.rst.rrt/${1}\n",
    "cp ${2} ${TONY_PATH}/data/rus.rst.rrt/${2}\n",
    "cp ${3} ${TONY_PATH}/data/rus.rst.rrt/${3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/tony_segmentator ../isanlp_rst/models/tony_segmentator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
